{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 手写字体训练与检测  --  Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable    #自动求导的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数配置\n",
    "batch_size=64  #单次训练导入数据个数\n",
    "test_batch_size=1000\n",
    "epochs=10  #训练时,每张图片训练次数\n",
    "lr=0.01   #学习率\n",
    "momentum=0.5    #动量\n",
    "no_cuda=False    #是否使用GPU, False表示使用\n",
    "seed=1\n",
    "log_interval=10\n",
    "cuda = not no_cuda and torch.cuda.is_available()  #当no_cuda=False并且cuda可用时使用GPU\n",
    "\n",
    "#设置随机种子, 保证训练时初始化参数一值, 用以作对比\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "#下载 训练以及检测数据\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造网络结构, 可以看到构造了2个隐层\n",
    "#\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)    # 卷积层, 1为输入的channel, 10为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)    # 卷积层, 10为输入的channel, 20为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2_drop = nn.Dropout2d()    # Dropout层 \n",
    "        self.fc1 = nn.Linear(320, 50)  #线性函数,输入大小为320个神经元, 输出为50个神经元\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    #前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对forward的相关解释:\n",
    "(1)  x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "该操作拆开来包含三个基本操作:\n",
    "\n",
    "    1).  self.conv1(x) : 对输入的x做卷积操作,卷积大小为之前定义的 self.conv1\n",
    "\n",
    "    2).  F.max_pool2d() : max pooling操作 参数2表示 filter size\n",
    "\n",
    "    3).  F.relu() : relu函数, 用以非线性化 (备注 relu(x) = max( x , 0) )\n",
    "    \n",
    "(2)  x.view(-1, 320) \n",
    "\n",
    "    类似reshape操作, 将第二维度转化为320, 第一维随之变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练与检测的函数定义\n",
    "#在后面会详细分析\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.390087\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.350225\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.288934\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279396\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272692\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.276944\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.233707\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.222649\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181699\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.159016\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055780\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.065890\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.916480\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.910268\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.704504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.554525\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.621117\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.521305\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.533543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.339470\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.382869\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.338094\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.271376\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.084347\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.204976\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.879644\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.776240\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.967955\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.913667\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.190108\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.847057\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.893181\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.072115\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.982275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.724942\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.857505\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.808646\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.784363\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.735870\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584131\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.723374\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.748310\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.512329\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.768801\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.595193\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.566207\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.751794\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.805440\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.820678\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.651483\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.861828\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.450013\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.675125\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.589900\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.548274\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.504665\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.501509\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.511922\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.680379\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.755212\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.770483\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.784469\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.709941\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.621009\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.819820\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.477789\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.688266\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.616235\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.802615\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.483786\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.534964\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.536785\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.305163\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.494786\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.660869\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.446193\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.344388\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.509734\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.538353\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.363782\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.597787\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.598923\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.436771\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.524937\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.575435\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.374998\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.694149\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.512092\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.516056\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.401807\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.383843\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.312272\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.564603\n",
      "\n",
      "Test set: Average loss: 0.2041, Accuracy: 9418/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.486901\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.544146\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.888303\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.484731\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.326975\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.297956\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.235635\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.396325\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.305222\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.313210\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.332076\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.354760\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.412945\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.374849\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.599363\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.367107\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.528852\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.552118\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.303312\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.384129\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.331025\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.324098\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.526638\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.347089\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.617742\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.676737\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.302390\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.210389\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.494773\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.437703\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.411734\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.295988\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.380997\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.204205\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.429797\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.416162\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.286840\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.341659\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.364185\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.515553\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.480123\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.572951\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.372059\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.325656\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.415332\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.220827\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.585254\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.483924\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.297673\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.341416\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.399177\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.313735\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.311808\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.406030\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.497109\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.329616\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.291911\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.568563\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.335116\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.310216\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.378319\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.397131\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.349563\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.230456\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.322034\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.494531\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.276744\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.312932\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.360437\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.459713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.455755\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.368296\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.428447\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.276449\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.169753\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.380953\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.321000\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.429387\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.431200\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.372420\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.300968\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.317711\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.254833\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.404029\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.230490\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.317188\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.574796\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.342342\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.546834\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.521412\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.312215\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.288447\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.289962\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.556146\n",
      "\n",
      "Test set: Average loss: 0.1270, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.236685\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.392455\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.188066\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.230839\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.410681\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.283347\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.352048\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.334162\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.230600\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.338946\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.351902\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.231788\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.381978\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.248378\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.340572\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.470485\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.309703\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.366788\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.362249\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.186638\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.287045\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.151577\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.355729\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.411932\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.255978\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.306523\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.279838\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.180094\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.240350\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.450948\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.261866\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.396388\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.355917\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.513716\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.416004\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.321718\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.374884\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.342310\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.384549\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.387071\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.144065\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.258094\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.310247\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.273195\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.286649\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.257573\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.289225\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.287682\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.534776\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.324884\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.345444\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.283481\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.281902\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.185973\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.404737\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.140723\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.289611\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.379340\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.235174\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.381586\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.148539\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.251058\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.276207\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.299851\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.339128\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.319319\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.328744\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.265533\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.506505\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.396374\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.551307\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.313056\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.439510\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.339940\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.219809\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.209950\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.285360\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.334508\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.309120\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.162557\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.282938\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.176343\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.230628\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.408929\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.230831\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.149880\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.503326\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.434923\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.270069\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.233795\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.323754\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.140417\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.421436\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.187792\n",
      "\n",
      "Test set: Average loss: 0.0989, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.233671\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.208074\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.257369\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.302572\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.287670\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.328569\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.391298\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.212217\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.366846\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.192780\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.144823\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.427179\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.255843\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.361535\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.317402\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.221273\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.295989\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.287286\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.215783\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.384650\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.193365\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.220765\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.426690\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.516173\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.225634\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.133625\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.392622\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.443564\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.282413\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.232976\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.139788\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.183816\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.100501\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.293785\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.418390\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.297198\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.307018\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.265119\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.289094\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.396080\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.202119\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.361065\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.329962\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.302404\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.374700\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.291659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.250628\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.210430\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.362034\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.308040\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.210972\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.414585\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.259766\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.140869\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.211635\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.285356\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.138712\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.327131\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.352196\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.258434\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.138762\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.202479\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.438078\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.100987\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.092250\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.224267\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.140229\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.216717\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.226066\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.319908\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.576098\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.328703\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.069610\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.225849\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.363515\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.245943\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.145891\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.478262\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.205147\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.169562\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.090261\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.159423\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.431891\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.176603\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.158571\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.099242\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.202835\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.364462\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.272479\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.211968\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.406827\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.440973\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.129932\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.223844\n",
      "\n",
      "Test set: Average loss: 0.0868, Accuracy: 9737/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.275306\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.234381\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.240997\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.192157\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.127385\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.154005\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.181392\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.200791\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.131829\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.335815\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.222793\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.211436\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.234341\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.208621\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.231933\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.197738\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.247108\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.138094\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.224458\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.424481\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.480607\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.257634\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.135101\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.179819\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.197423\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.154386\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.216610\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.291348\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.142891\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.204093\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.286002\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.290060\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.258122\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.147286\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.180062\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.130583\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.216550\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.263487\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.424482\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.189207\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.125508\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.159434\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.229874\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.208245\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.156861\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.211634\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.255346\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.122892\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.291070\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.183378\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.165908\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.123925\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.246958\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.264460\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.311576\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.206567\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.172322\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.355415\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.173746\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.086609\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.268911\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.237583\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.101525\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.141821\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.053535\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.100666\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.400315\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.211739\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.419303\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.125857\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.424365\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.200793\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.215600\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.173107\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.167188\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.142734\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.205211\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.204758\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.172385\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.311693\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.188475\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.269034\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.341294\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.398529\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.113544\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.305832\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.076659\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.182353\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.115446\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.176114\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.103736\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.074462\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.219240\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.223397\n",
      "\n",
      "Test set: Average loss: 0.0787, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.176723\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.176146\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.191567\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.276580\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.190535\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.223824\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.219514\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.382304\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.404811\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.092891\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.163638\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.444936\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.151670\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.109334\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.127616\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.184248\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.239960\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.269087\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.650659\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.493858\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.359294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.154474\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.166033\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.326607\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.358419\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.266059\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.175204\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.107735\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.173254\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.304224\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.284671\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.351058\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.274324\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.299140\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.162694\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.151711\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.488367\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.173438\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.248640\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.161534\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.304421\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.172477\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.161958\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.176514\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.448559\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.699853\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.149711\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.386454\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.160130\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.298405\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.285151\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.142137\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.301945\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.117666\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.147342\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.215785\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.320876\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.258551\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.107271\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.282123\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.161496\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.086751\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.158406\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.191848\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.159248\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.114726\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.301070\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.142145\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.111350\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.197754\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.201688\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.159410\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.198307\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.145888\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.121704\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.531387\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.346753\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.079224\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.291830\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.126071\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.185493\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.289214\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.139702\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.220220\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.203050\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.143873\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.131845\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.191030\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.517676\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.148235\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.124973\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.204551\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.171708\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.111453\n",
      "\n",
      "Test set: Average loss: 0.0692, Accuracy: 9775/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.218132\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.063752\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.168194\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.265532\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.246493\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.141633\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.154517\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.142243\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.191010\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.260132\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.244597\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.217715\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.256459\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.344142\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.252609\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.159876\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.569142\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.244859\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.188019\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.168052\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.131651\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.114707\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.216165\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.228586\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.128982\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.265556\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.123733\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.147425\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.459330\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.202661\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.257804\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.338657\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.353541\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.321474\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.343689\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.148865\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.376880\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.270118\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.374000\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.152999\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.182253\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.219925\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.100959\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.188429\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.106815\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.201748\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.207168\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.224571\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.256374\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.134899\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.143744\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.358644\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.122577\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.153019\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.321985\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.135279\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.242218\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.332222\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.180015\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.162424\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.243230\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.224015\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.126203\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.172230\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.154826\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.133037\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.218338\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.345449\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.305542\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.128703\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.341330\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.179452\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.254897\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.072748\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.174496\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.127595\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.125544\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.175996\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.221248\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.129568\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.149974\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.279928\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.371463\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.317617\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.098092\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.327153\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.132623\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.193577\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.195452\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.242269\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.441660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.174655\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.134845\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.310164\n",
      "\n",
      "Test set: Average loss: 0.0671, Accuracy: 9773/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.438347\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.153409\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.187059\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.296012\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.188008\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.360957\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.340678\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.190956\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.270347\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.384768\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.101446\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.157666\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.243910\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.210179\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.101680\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.147068\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.164354\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.102983\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.198288\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.232101\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.379854\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.139277\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.222524\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.154706\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.198865\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.216766\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.072754\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.111346\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.156581\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.265478\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.173392\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.184563\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.179929\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.194570\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.126123\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.122736\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.353542\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.130552\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.143331\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.387020\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.212682\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.122418\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.120353\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.191051\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.335164\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.340620\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.186335\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.227579\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.138470\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.264200\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.156285\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.098488\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.057859\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.247932\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.324828\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.108889\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.261919\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.389412\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.150819\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.199411\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.201762\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.089241\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.117840\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.184378\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.141104\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.163896\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.284251\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.280506\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.300127\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.168585\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.116018\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.246230\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.243590\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.098984\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.042794\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.113190\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.234504\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.092825\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.264977\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.119368\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.134759\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.152597\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.156721\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.124106\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.274666\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.149220\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.208199\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.300047\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.204368\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.242561\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.143526\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.148485\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.248626\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.400564\n",
      "\n",
      "Test set: Average loss: 0.0633, Accuracy: 9792/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.160313\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.173391\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.174663\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.130921\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.243851\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.254660\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.197410\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.163589\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.091870\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.143367\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.121756\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.158113\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.204437\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.257811\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.133085\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.077856\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.246190\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.181941\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.374633\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.247533\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.239093\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.351359\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.171002\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.179517\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.080417\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.068291\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.105775\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.216720\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.222936\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.212274\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.359765\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.245851\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.140471\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.206286\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.207397\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.219521\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.112291\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.098983\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.324764\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.272997\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.072468\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.115544\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.227853\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.249941\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.108746\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.208553\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.256502\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.251230\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.048024\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.115089\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.055469\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.225317\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.279787\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.445450\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.454720\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.187017\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.107589\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.195380\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.276445\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.199613\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.286949\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.076635\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.243999\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.198457\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.216607\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.209780\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.221229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.199979\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.154747\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.076019\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.360372\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.086524\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.253302\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.146223\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.316338\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.077549\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.217100\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.060326\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.183724\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.271142\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.243035\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.110418\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.145657\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.099538\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.313956\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.185845\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.340120\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.284240\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.205356\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.121585\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.242982\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.055264\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.095505\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.177944\n",
      "\n",
      "Test set: Average loss: 0.0559, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.067230\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.156067\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.099089\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.331816\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.141097\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.104192\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.109565\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.233660\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.680255\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.153661\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.294438\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.280175\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.093364\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.139845\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.186555\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.254772\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.190139\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.216237\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.265886\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.084985\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.264065\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.179586\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.179130\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.375559\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.105023\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.148598\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.152571\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.293430\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.185033\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.249179\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.205018\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.486609\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.119697\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.140298\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.087624\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.166139\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.248271\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.129478\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.085781\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.236942\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.296135\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.186802\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.058775\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.121260\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.060733\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.176426\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.179586\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.137768\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.169356\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.163046\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.126040\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.112147\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.296561\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.433495\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.124479\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.103904\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.453458\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.145596\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.295308\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.248121\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.100128\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.395856\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.206007\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.180397\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.160969\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.245323\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.161645\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.182737\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.165575\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.169871\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.103702\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.105059\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.266139\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.240840\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.298877\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.117973\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.097226\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.168732\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.079284\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.110910\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.192473\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.128051\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.088426\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.133672\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.225890\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.219598\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.141707\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.176438\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.115457\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.119579\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.134408\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.094729\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.303944\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.077667\n",
      "\n",
      "Test set: Average loss: 0.0552, Accuracy: 9811/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练与检测过程\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型的保存 与导入\n",
    "#方式一: 保存model结构以及全部参数\n",
    "torch.save(model, './model.pkl')\n",
    "model_load = torch.load('./model.pkl')\n",
    "\n",
    "#方式二: 仅保存参数\n",
    "torch.save(model.state_dict(), './model_state.pkl')\n",
    "model_load_1 = Net() #初始化模型结构\n",
    "model_load_1.load_state_dict(torch.load('./model_state.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以上即为整个网络的结构以及相应的训练与检测过程\n",
    "\n",
    "## 接下来对网络结构进行进一步详细分析\n",
    "\n",
    "    (1). 网络结构中包含两部分主要数据: data 以及 weight , 训练过程就是对输入的data 找到最合适的weight, 使得输出最能代表data的属性特征.\n",
    "\n",
    "    (2). data由原始数据而来, 一般而言可以分为train(训练集), val(验证集), test(测试集)\n",
    "\n",
    "    (3). 每一层的输出称为 feature map(特征图)\n",
    "\n",
    "    (4). 一般而言, cnn网络的浅层特征图可以包含更多局部信息, 而深层特征图包含更多全局信息.\n",
    "\n",
    "#### 第一, 本网络中 weight 结构的相关分析 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d (p=0.5)\n",
      "  (fc1): Linear (320 -> 50)\n",
      "  (fc2): Linear (50 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#打印所定义的model\n",
    "print( model)\n",
    "# 可以看到定义的基本网络结构包含conv1,conv2,conv2_drop,fc1,fc2这几个基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "1\n",
      "10\n",
      "(5, 5)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "#详细打印所定义的卷积层 conv1\n",
    "print( model.conv1)    #打印卷积核的整体参数\n",
    "print( model.conv1.in_channels)    #打印输入通道数\n",
    "print( model.conv1.out_channels)    #打印输出通道数\n",
    "print( model.conv1.kernel_size)    #打印卷积核kernel_size大小\n",
    "print( model.conv1.padding)    #打印卷积核padding大小\n",
    "\n",
    "# 解释:卷积大小 5x5x1x10 (10个5x5x1大小的卷积核, 其中1为上一层feature map层数)\n",
    "# 对于输入为28x28x1的图片, 经过该卷积, 输出为24x24x10\n",
    "# 并且可见当未对 stride(步长) 以及 padding(补偿) 进行定义时, 默认 stride = 1, padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0718 -0.0599  0.0084  0.1853 -0.0768\n",
      "  0.1293 -0.1749  0.0592  0.0821  0.1828\n",
      "  0.1663  0.1243 -0.0504  0.1141  0.1300\n",
      " -0.1124  0.0702  0.1318  0.1578  0.0617\n",
      " -0.0682  0.1047  0.0271 -0.1189 -0.1897\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0904 -0.0637  0.1429  0.0290 -0.0554\n",
      "  0.0337  0.0381  0.0624 -0.1583 -0.0627\n",
      "  0.1891 -0.0269 -0.0208 -0.0912  0.0931\n",
      "  0.1318  0.0365 -0.0591 -0.0320  0.0291\n",
      "  0.0797 -0.1200  0.1113 -0.1570  0.0432\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.1496 -0.1100  0.1010 -0.0170 -0.0978\n",
      "  0.0173 -0.0529  0.0389  0.1086 -0.1525\n",
      "  0.0892  0.1002  0.0454  0.1093 -0.1228\n",
      " -0.0084  0.0033 -0.0241 -0.0512 -0.1507\n",
      " -0.0800  0.0008 -0.1261 -0.1517  0.0656\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1841  0.1774 -0.1910 -0.1493  0.1080\n",
      " -0.1123 -0.0688 -0.1626 -0.0138  0.0164\n",
      " -0.0535 -0.1626  0.0468 -0.1920  0.1368\n",
      "  0.1302 -0.1057 -0.0190 -0.0736 -0.1097\n",
      " -0.0977 -0.0791  0.0472  0.0167  0.1136\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1334 -0.1217  0.1484  0.0244 -0.0805\n",
      " -0.1413 -0.1151 -0.1964 -0.1658  0.1137\n",
      " -0.1936 -0.1794  0.0982 -0.1419 -0.1451\n",
      " -0.1724 -0.0811 -0.1184 -0.1391  0.0542\n",
      "  0.0923 -0.0045  0.0354  0.1587 -0.1332\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.0933  0.1575 -0.0044 -0.0888  0.0488\n",
      "  0.1815 -0.1477  0.1556 -0.1853  0.1212\n",
      " -0.1131 -0.1757 -0.1836  0.0401  0.1334\n",
      " -0.1620  0.1551 -0.1530 -0.1430  0.0127\n",
      " -0.1313  0.1250 -0.1038  0.0597  0.1700\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.1718 -0.1985  0.0081 -0.1621 -0.0222\n",
      " -0.0622  0.1464 -0.0393  0.1361  0.0205\n",
      "  0.1034 -0.1369  0.0744  0.0157 -0.1044\n",
      " -0.1055  0.0956  0.0508 -0.0491  0.0022\n",
      "  0.0554 -0.0316 -0.0575  0.0952  0.0783\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1180 -0.0655 -0.0610 -0.0951  0.1857\n",
      "  0.0408  0.0199  0.1835  0.0559 -0.0893\n",
      "  0.0237  0.0028  0.0758  0.1155 -0.1758\n",
      "  0.1775  0.1404 -0.0148 -0.1047  0.0456\n",
      "  0.0171 -0.1974 -0.1359 -0.1116 -0.1345\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1836  0.0263  0.1904  0.0029 -0.0769\n",
      " -0.0371 -0.0407 -0.0456 -0.1744  0.0870\n",
      "  0.1378 -0.1964  0.0099 -0.0372  0.1531\n",
      "  0.0804 -0.0349  0.0985  0.0731 -0.0715\n",
      "  0.0693 -0.0319 -0.1852  0.1787  0.0618\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "  0.0207 -0.0407  0.0559  0.0685  0.1569\n",
      " -0.1204  0.0470  0.0402  0.1923  0.0864\n",
      "  0.1597 -0.1317 -0.0542 -0.0993 -0.1249\n",
      "  0.0943  0.0632 -0.1431 -0.1321 -0.1617\n",
      " -0.1606  0.1239  0.0068  0.0769 -0.0167\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(1).打印随机初始化的卷积核参数 (不存在seed)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print( model.conv1.weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0332  0.1989  0.0881  0.1730 -0.2000\n",
      " -0.1488 -0.0791  0.1996 -0.1413 -0.1056\n",
      " -0.1631 -0.0414 -0.1255 -0.0448 -0.0618\n",
      "  0.0679 -0.0413  0.1742  0.0155  0.1385\n",
      " -0.0323 -0.0747  0.0741  0.0098 -0.1182\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0226  0.1512 -0.1082 -0.1890  0.0138\n",
      "  0.0682  0.1656 -0.0331 -0.0171  0.0235\n",
      " -0.0277 -0.1438  0.1757 -0.1208  0.1114\n",
      "  0.1203  0.0864  0.1873  0.1211 -0.0746\n",
      " -0.1629  0.0769  0.0073  0.1506  0.1460\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1578  0.1317 -0.1660  0.1318 -0.1844\n",
      " -0.0908 -0.1321 -0.1763  0.1513  0.0682\n",
      " -0.1607  0.0372 -0.0316  0.0687  0.1832\n",
      " -0.0353  0.0133 -0.1210  0.0768 -0.0841\n",
      " -0.0738 -0.1432  0.0746  0.1133  0.1339\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.0350 -0.1927 -0.1863  0.1001  0.0496\n",
      "  0.1955  0.0643  0.0993 -0.0806 -0.0878\n",
      " -0.0215  0.1157 -0.1112 -0.1587 -0.1707\n",
      " -0.0208 -0.0123  0.1634 -0.1615 -0.0826\n",
      "  0.1613 -0.0849 -0.1522 -0.1480  0.0099\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1923 -0.1666  0.0715  0.1667 -0.1153\n",
      "  0.1642 -0.0938 -0.0804 -0.0034  0.0338\n",
      " -0.1787  0.0264  0.0296  0.0456 -0.1413\n",
      "  0.1826  0.0357 -0.0956  0.0799 -0.1076\n",
      " -0.1591  0.0134 -0.0344  0.1800  0.0778\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0028 -0.0343  0.0162 -0.1800  0.1062\n",
      "  0.0144 -0.1819  0.0655 -0.1440  0.0060\n",
      "  0.1170  0.1778 -0.1881  0.0346  0.1533\n",
      "  0.1614  0.0163 -0.1450 -0.0208 -0.1443\n",
      "  0.1569  0.1230 -0.0490 -0.0409  0.0154\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.1339  0.0609  0.1710 -0.0555 -0.0609\n",
      "  0.0284  0.1003  0.0551  0.0904 -0.1495\n",
      "  0.1533  0.0761  0.0495  0.0591  0.1004\n",
      " -0.0584 -0.0604  0.1053 -0.0920 -0.0574\n",
      "  0.1584  0.1011 -0.0288  0.1525  0.1859\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1953  0.0654 -0.0008  0.0487 -0.1705\n",
      " -0.1541  0.1148  0.1798 -0.1744 -0.0200\n",
      " -0.0579  0.0314  0.1767 -0.0367 -0.0481\n",
      " -0.1052  0.1052  0.1614  0.1086  0.0295\n",
      " -0.0795 -0.1989  0.1091  0.0469 -0.1388\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0693  0.0315  0.0108 -0.1964  0.1544\n",
      "  0.0836 -0.0571 -0.0117  0.1634  0.1058\n",
      "  0.0493 -0.0130 -0.1937 -0.0924  0.1718\n",
      "  0.1327  0.0764  0.0205  0.1989 -0.1720\n",
      " -0.1311 -0.0110 -0.1451  0.0971  0.1730\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1232  0.0787 -0.0143 -0.1736 -0.1079\n",
      "  0.1022  0.0033  0.1016 -0.1166  0.1692\n",
      " -0.1803  0.0846  0.0075 -0.1503 -0.1311\n",
      " -0.1920 -0.0415 -0.1895 -0.1571 -0.1887\n",
      "  0.0039 -0.1015 -0.1591  0.1440 -0.0845\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0153  0.3259  0.1931  0.1950 -0.2066\n",
      " -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
      " -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
      "  0.0233 -0.0447  0.1835  0.1321  0.3404\n",
      "  0.1407  0.1476  0.2579  0.1959  0.0585\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0104 -0.0224 -0.3664 -0.4861 -0.2800\n",
      "  0.1141  0.1102 -0.2812 -0.4165 -0.3379\n",
      "  0.1372  0.1199  0.2881 -0.0999  0.1634\n",
      "  0.2905  0.3823  0.5954  0.5653  0.1324\n",
      " -0.1438  0.0064  0.0802  0.3379  0.2844\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.0798  0.0704 -0.1030  0.2154 -0.0975\n",
      " -0.1746 -0.2341 -0.2216  0.1732  0.1978\n",
      " -0.3685 -0.1535 -0.1572  0.1322  0.3971\n",
      " -0.2559 -0.1418 -0.1172  0.3395  0.2059\n",
      " -0.1713 -0.0901  0.2505  0.3522  0.2085\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1241 -0.1835 -0.3326 -0.1037 -0.1134\n",
      "  0.3162  0.0332 -0.0893 -0.2823 -0.2161\n",
      "  0.1476  0.1888 -0.1981 -0.2667 -0.2407\n",
      "  0.1627  0.0904  0.0908 -0.2753 -0.1488\n",
      "  0.2771  0.0394 -0.1264 -0.2166 -0.0730\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.3005 -0.2953 -0.1426 -0.0045 -0.1248\n",
      " -0.0297 -0.2309 -0.2401 -0.1252  0.0536\n",
      " -0.3134 -0.0809 -0.0239  0.0702 -0.0181\n",
      "  0.0806 -0.0049 -0.0313  0.1956  0.0620\n",
      " -0.1641  0.0897  0.1914  0.4325  0.2904\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1330  0.2172  0.1960 -0.0969  0.1130\n",
      "  0.1974  0.0932  0.1406 -0.2397 -0.0977\n",
      "  0.3167  0.3363 -0.2704 -0.1850 -0.0186\n",
      "  0.3908  0.1115 -0.2426 -0.1940 -0.2603\n",
      "  0.2042  0.2092 -0.1117 -0.2013 -0.1616\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0198  0.2373  0.2729 -0.0540 -0.1854\n",
      "  0.0802  0.3993  0.1696 -0.0028 -0.2808\n",
      "  0.2505  0.3697 -0.0181 -0.1314 -0.0736\n",
      "  0.0541  0.1614  0.2596 -0.0544 -0.0701\n",
      "  0.1702  0.2460  0.2558  0.3425  0.3234\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.0982  0.1252 -0.0135 -0.0752 -0.3418\n",
      " -0.1307  0.2256  0.3210 -0.2659 -0.3056\n",
      " -0.2033  0.0767  0.5127  0.0826 -0.2298\n",
      " -0.3450 -0.0224  0.3877  0.3312  0.0064\n",
      " -0.2941 -0.4151  0.0306  0.1472 -0.0846\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0570 -0.0232 -0.0239 -0.1755  0.2165\n",
      "  0.0881 -0.1158 -0.0787  0.1568  0.2332\n",
      "  0.0131 -0.0756 -0.2181  0.0534  0.3254\n",
      "  0.0991  0.0501  0.0569  0.3412 -0.0361\n",
      " -0.1060  0.0797 -0.0573  0.1666  0.1814\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1200  0.0947 -0.0307 -0.2284 -0.1777\n",
      "  0.0758 -0.0426  0.0249 -0.2037  0.0998\n",
      " -0.2321  0.0203 -0.0582 -0.2084 -0.1820\n",
      " -0.2436 -0.0961 -0.2408 -0.2075 -0.2478\n",
      " -0.0113 -0.1164 -0.1620  0.1383 -0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(2).打印随机初始化的卷积核参数 (使用固定seed)\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print( model.conv1.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.390087\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.350225\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.288934\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279396\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272692\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.276944\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.233707\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.222649\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181699\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.159016\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055780\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.065890\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.916481\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.910268\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.704504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.554525\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.621117\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.521305\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.533543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.339470\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.382869\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.338094\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.271376\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.084347\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.204976\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.879644\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.776240\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.967955\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.913667\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.190108\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.847057\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.893181\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.072115\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.982275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.724942\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.857505\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.808646\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.784363\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.735870\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584132\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.723374\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.748310\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.512329\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.768801\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.595193\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.566207\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.751794\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.805441\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.820678\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.651483\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.861828\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.450013\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.675125\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.589900\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.548274\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.504815\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.501443\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.513226\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.680256\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.761249\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.773745\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.784808\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.702843\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.620407\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.827360\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.473729\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.690846\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.614329\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.801135\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.482657\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.535080\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.534342\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.295305\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.498265\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.652372\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.444436\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.345962\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.510644\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.546157\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.368057\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.590018\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.590431\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.433518\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.524811\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.571309\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.370530\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.692114\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.516627\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.524651\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.400514\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.376999\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.313202\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.555166\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0466  0.2031  0.0893  0.1611 -0.2132\n",
      " -0.1936 -0.1076  0.1918 -0.1395 -0.1201\n",
      " -0.2006 -0.0598 -0.1203 -0.0266 -0.0498\n",
      "  0.0581 -0.0258  0.2143  0.0675  0.1800\n",
      " -0.0053 -0.0306  0.1248  0.0615 -0.0771\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0364  0.1486 -0.1940 -0.3186 -0.1161\n",
      "  0.0935  0.1672 -0.1010 -0.1421 -0.0576\n",
      "  0.0400 -0.0273  0.2717 -0.0324  0.2114\n",
      "  0.2760  0.3282  0.4855  0.4201  0.1387\n",
      " -0.0682  0.2083  0.1857  0.3354  0.2976\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1588  0.1365 -0.1396  0.1756 -0.1081\n",
      " -0.1313 -0.1584 -0.1809  0.1946  0.1662\n",
      " -0.2231 -0.0186 -0.0632  0.1343  0.3125\n",
      " -0.0924 -0.0317 -0.1161  0.1984  0.0776\n",
      " -0.0982 -0.1332  0.1467  0.2583  0.2716\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.0506 -0.1400 -0.1847  0.0781  0.0193\n",
      "  0.2604  0.0830  0.0677 -0.1320 -0.1303\n",
      "  0.0647  0.1492 -0.1345 -0.1917 -0.1927\n",
      "  0.0859  0.0290  0.1340 -0.1921 -0.1035\n",
      "  0.2538 -0.0577 -0.1743 -0.1766 -0.0167\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.2334 -0.2086  0.0280  0.1653 -0.0758\n",
      "  0.0983 -0.1439 -0.0961  0.0194  0.0723\n",
      " -0.2369 -0.0221  0.0337  0.0955 -0.0845\n",
      "  0.1606  0.0354 -0.0436  0.1656 -0.0402\n",
      " -0.1364  0.0644  0.0645  0.2998  0.1651\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1139  0.1199  0.1174 -0.1610  0.0591\n",
      "  0.1518 -0.0390  0.0978 -0.2051 -0.0853\n",
      "  0.2658  0.2762 -0.2334 -0.0760  0.0485\n",
      "  0.3203  0.0911 -0.1987 -0.1066 -0.2093\n",
      "  0.2858  0.1994 -0.0675 -0.0893 -0.0253\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0590  0.2004  0.2798 -0.0502 -0.1513\n",
      "  0.1376  0.2859  0.1823  0.0711 -0.2584\n",
      "  0.3385  0.3405  0.2042  0.0581  0.0280\n",
      "  0.1112  0.1552  0.2583 -0.0349 -0.0425\n",
      "  0.2332  0.2018  0.0644  0.2440  0.2972\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1920  0.1026  0.0852  0.1157 -0.1874\n",
      " -0.1738  0.1403  0.2622 -0.1226 -0.0631\n",
      " -0.0785  0.0763  0.2957  0.0519 -0.0504\n",
      " -0.1161  0.1627  0.2953  0.2247  0.0771\n",
      " -0.0997 -0.1714  0.1870  0.1206 -0.1041\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0796 -0.0018 -0.0129 -0.1723  0.2378\n",
      "  0.0698 -0.0696 -0.0056  0.2377  0.2448\n",
      "  0.0299 -0.0211 -0.1736  0.0291  0.3437\n",
      "  0.1022  0.0523  0.0361  0.3140 -0.0277\n",
      " -0.1726 -0.0423 -0.1340  0.1667  0.2404\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1355  0.0666 -0.0303 -0.1926 -0.1293\n",
      "  0.0835 -0.0185  0.0756 -0.1443  0.1396\n",
      " -0.1965  0.0672 -0.0126 -0.1697 -0.1512\n",
      " -0.2076 -0.0568 -0.2049 -0.1721 -0.2050\n",
      " -0.0027 -0.1051 -0.1597  0.1451 -0.0872\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(3).经过一轮训练之后的卷积参数\n",
    "for epoch in range(1, 1 + 1):\n",
    "    train(epoch)\n",
    "    \n",
    "print( model.conv1.weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积weight分析总结:\n",
    "   (1). seed可以使得每次初始化的值固定,这样在做实验的时候可以对比,大家的初始化参数都是一样的;\n",
    "\n",
    "   (2). 在实际项目中一般不需要加seed;\n",
    "   \n",
    "   (3). 这里也可以再次看到conv的参数个数为 10x1x5x5 (10为输出的卷积个数, 1为上层feature map 层数, 5x5为卷积核大小);\n",
    "    \n",
    "   (4). 可以发现每次train之后,更新的参数是conv的参数, 这里我们可以这样理解: train的过程就是为了找到合适的卷积核参数, 使得对于输入的data, 可以有效的提取特征, 进而根据所提取的特征作进一步的工作, 比如分类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout2d (p=0.5)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#  dropout分析\n",
    "#  可见默认值为 p = 0.5 , 这里p表示随机失活神经元概率, 即0.5的神经元在单次传播过程中不进行梯度更新.\n",
    "\n",
    "print(model.conv2_drop)\n",
    "print(model.conv2_drop.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear (320 -> 50)\n",
      "320\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 线性函数,可以等价于全连接\n",
    "# 参数包括 weight, bias 即权重和偏置(权重系数以及常数项系数)\n",
    "\n",
    "print(model.fc1)\n",
    "print(model.fc1.in_features)    #输入的神经元个数\n",
    "print(model.fc1.out_features)    #输出的神经元个数\n",
    "\n",
    "#若想打印参数信息, 如下:\n",
    "# print(model.fc1.weight)\n",
    "# print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二, 本网络中 data 结构的相关分析 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchvision.datasets.mnist.MNIST object at 0x7fa8866cf5f8>\n",
      "60000\n",
      "(\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.0424\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1995  2.6051\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  2.3633\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5940\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1315\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  1.7523  2.3633\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  0.2758  1.7650  2.4524  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  1.3068  2.7960  2.7960  2.7960  2.2742\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.3860 -0.1951 -0.1951 -0.1951  1.1795  1.3068\n",
      "  0.0340  0.7722  1.5359  1.7396  2.7960  2.7960  2.7960  2.7960  2.7960\n",
      "  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7706\n",
      "  2.7960  2.7960  2.7960  2.7960  2.7960  2.0960  1.8923  2.7197  2.6433\n",
      "  1.5614  0.9377  2.7960  2.7960  2.1851 -0.2842 -0.4242  0.1231  1.5359\n",
      " -0.2460 -0.4115  1.5359  2.7960  0.7213 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242  1.3450  2.7960  1.9942 -0.3988 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.2842  1.9942  2.7960  0.4668 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242  0.0213  2.6433  2.4396  1.6123  0.9504 -0.4115\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  0.6068  2.6306  2.7960  2.7960  1.0904\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1486  1.9432  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.2206  0.7595  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.7451\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1613  1.2305  1.9051  2.7960\n",
      " -0.4242 -0.4242 -0.4242  0.0722  1.4596  2.4906  2.7960  2.7960  2.7960\n",
      " -0.4242 -0.1187  1.0268  2.3887  2.7960  2.7960  2.7960  2.7960  2.1342\n",
      "  0.4159  2.2869  2.7960  2.7960  2.7960  2.7960  2.0960  0.6068 -0.3988\n",
      "  2.7960  2.7960  2.7960  2.7960  2.0578  0.5940 -0.3097 -0.4242 -0.4242\n",
      "  2.7960  2.7960  2.6815  1.2686 -0.2842 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.2941  1.2559 -0.2206 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.8032 -0.0933  1.6887  2.8215  2.7197  1.1923 -0.4242 -0.4242 -0.4242\n",
      "  2.4396  1.7650  2.7960  2.6560  2.0578  0.3904 -0.4242 -0.4242 -0.4242\n",
      "  0.7595  0.6195  0.6195  0.2886  0.0722 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.1060 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.4850 -0.0806 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  1.9560 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.7451  0.3904 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.2105 -0.3988 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7578  1.8923 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.5686 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.FloatTensor of size 1x28x28]\n",
      ", 5)\n",
      "<bound method _type of \n",
      "(  0  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  1  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  2  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      " ...  \n",
      "\n",
      "(59997,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59998,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59999,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "[torch.ByteTensor of size 60000x28x28]\n",
      ">\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(  0  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  1  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  2  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      " ...  \n",
      "\n",
      "(59997,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59998,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59999,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "[torch.ByteTensor of size 60000x28x28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对原始下载的train data 进行分析\n",
    "\n",
    "print(train_loader.dataset)    #输入网络的train data (经过了数据转换, 归一化之后的)\n",
    "print(len(train_loader.dataset))    # train data 图片张数\n",
    "print(train_loader.dataset[0])    # 该输出的为 转化后的第一张图片的数字结构\n",
    "print(train_loader.dataset.train_data[0])   #该输出的为 变回原始输入的第一张图片的数字结构\n",
    "print(train_loader.dataset.train_data[0].type) #该输出的为 变回原始输入的第一张图片的数据类型, 数据类型为torch.ByteTensor 张量\n",
    "print(train_loader.dataset.train_data[0].shape)  #图像大小为1x28x28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 可以看到对于输入到model中进行训练的数据有如下特点:\n",
    "\n",
    "(1).输入数据维度为(60000,1,28,28) 即60000张图片, 每张图片为单通道灰度图, 长和宽均为28\n",
    "\n",
    "(2).输入数据的数据类型为 归一化的float tensor (torch.FloatTensor )\n",
    "\n",
    "(3).可以通过 transforms 对数据类型进行转换\n",
    "\n",
    "(4).可以通过train_loader.dataset.train_data直接得到原始图像归一化前的张量数据结构\n",
    "\n",
    "#### 2. 我们知道这里数据类型是 Tensor 张量, 如果需要以图片形式输出可以有如下几种方式:\n",
    "    \n",
    "(1).使用 \" .numpy()\" 将 Tensor类型  转换为 numpy, 然后使用 opencv 或 matplotlib 作图;\n",
    "    \n",
    "(2).使用transforms.ToPILImage() 将 Tensor类型 转化为 PIL类型输出;\n",
    "    \n",
    "(3).使用visdom工具, 直接对 Tensor 做可视化 (尝试失败,不想研究,感兴趣可以自己研究下).\n",
    "\n",
    "(4).其他,例如使用TensorFlow的可视化工具\n",
    "\n",
    "#### 下面选取了第一种方式进行作图, 其他方式感兴趣可以自己尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHVCAYAAAAtoIVHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4198a9ea20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始图像可视化输出 : 使用 opencv 或 matplotlib 作图\n",
    "\n",
    "import numpy as np    #导入numpy\n",
    "img_tensor = train_loader.dataset.train_data[0] #导入tensor 数据\n",
    "img_np = img_tensor.numpy()  #将tensor 数据 转化为numpy\n",
    "\n",
    "# #下面几行代码使用opencv画图, 不建议在jupyter notebook内使用, 因为cv2.waitKey() 会导致后面程序无法正常运行  \n",
    "# import cv2    #导入opencv\n",
    "# cv2.imshow(\"image\", img_np)\n",
    "# cv2.waitKey()  \n",
    "\n",
    "#使用matplotlib 作图, 在jupyter notebook中推荐\n",
    "import  matplotlib.pyplot as plt    #导入matplotlib画图工具\n",
    "#plt图像嵌入jupyter内\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "# 图1: 输出原图\n",
    "ax1 = fig.add_subplot(221) \n",
    "ax1.imshow(img_np)  \n",
    "\n",
    "# 图2: 输出热力图\n",
    "ax2 = fig.add_subplot(222) \n",
    "ax2.imshow(img_np,  cmap=\"hot\" )  \n",
    "\n",
    "# 图3: 输出灰度图\n",
    "ax3 = fig.add_subplot(223) \n",
    "ax3.imshow(img_np, cmap=plt.cm.gray)  \n",
    "\n",
    "# 图4: 输出反向灰度图\n",
    "ax4 = fig.add_subplot(224) \n",
    "ax4.imshow(img_np, cmap=plt.cm.gray_r)  \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHVCAYAAAAU6/ZZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxTAMAACAZDEMAwAAIFn/H1XsRkiWN5f4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa86f4afba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#输出 前6张图\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "num =231\n",
    "for i in range(6):\n",
    "    img_tensor = train_loader.dataset.train_data[i] #导入tensor 数据\n",
    "    img_np = img_tensor.numpy()  #将tensor 数据 转化为numpy\n",
    "    ax1 = fig.add_subplot(num) \n",
    "    ax1.imshow(img_np)  \n",
    "    num += 1\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面, 我们展示一下网络的详细训练过程\n",
    "\n",
    "#### 在此, 我将展示这个网络每一步前向传播中feature map发生的变化, 以及反向传播的weight发生的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化 model\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "#选择优化方式, 这里使用的是SGD随机梯度下降, 当然也可以选择别的\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "#导入 train数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个batch的图片总个数:\n",
      "64\n",
      "这个batch图片的类别信息:\n",
      "[5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "#第一个batch的一次前向传播以及反向传播\n",
    "\n",
    "#以比较愚蠢的方式导入一个batch的数据, 这里一个batch包含64张图\n",
    "model.train()\n",
    "for batch_idx, (data_i, target_i) in enumerate(train_loader):\n",
    "    if batch_idx !=0:\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "print (\"一个batch的图片总个数:\" )\n",
    "print (len(data))\n",
    "print (\"这个batch图片的类别信息:\" )\n",
    "print ([j for j in target])\n",
    "\n",
    "#转换数据类型\n",
    "data, target = Variable(data), Variable(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体流程\n",
    "在此, 我将展示这个前向传播以及反向传播过程中, 参数是如何变化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向传播前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1407 -0.0253  0.1999 -0.1994\n",
      " -0.1119  0.1906 -0.1263  0.0701 -0.1803\n",
      " -0.0821  0.1815  0.0779  0.1674 -0.1716\n",
      "  0.0052 -0.0165  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0292 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1960 -0.1326  0.0846 -0.1297  0.0142\n",
      "  0.1144  0.0488 -0.1724  0.1641  0.0055\n",
      " -0.0211  0.1503 -0.0945  0.1493 -0.0684\n",
      "  0.0383 -0.0214  0.0342  0.0387  0.0328\n",
      " -0.0773  0.0035 -0.0590  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1999  0.1026 -0.1491 -0.0924 -0.1258\n",
      "  0.0420  0.0497  0.1619 -0.0237  0.1122\n",
      "  0.1907  0.1579  0.0430 -0.1519  0.1576\n",
      " -0.0566  0.1748 -0.1697  0.1135 -0.1544\n",
      "  0.1580  0.1302  0.1174 -0.1951  0.0236\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1319 -0.1651  0.1064  0.1096 -0.0925\n",
      " -0.0695  0.1518 -0.1159 -0.0858  0.0017\n",
      " -0.1722 -0.1757 -0.0232  0.0240  0.1866\n",
      "  0.0097  0.0489 -0.1157  0.0142 -0.1830\n",
      " -0.0899  0.1399 -0.1802 -0.0496  0.1484\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0826 -0.0188  0.0553 -0.0241  0.1129\n",
      "  0.1193 -0.0050  0.0981  0.1629 -0.0506\n",
      "  0.1275  0.0013  0.1190  0.1001  0.1036\n",
      " -0.1814  0.1130  0.0299  0.1662  0.1661\n",
      " -0.1892  0.0051 -0.1623 -0.0477 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0287  0.1197 -0.1414  0.0240 -0.1098\n",
      " -0.1157 -0.0984 -0.0533  0.0326  0.0231\n",
      " -0.0876 -0.1740  0.1189  0.1045 -0.1423\n",
      "  0.0480 -0.0041  0.1461  0.0988  0.1371\n",
      "  0.0738  0.0284 -0.1009 -0.1743  0.1637\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0487 -0.1323  0.0369 -0.0769 -0.0445\n",
      " -0.1175  0.1258 -0.1453  0.0420 -0.1572\n",
      "  0.0530  0.1615  0.1287  0.0696 -0.1123\n",
      "  0.1034 -0.0652 -0.1831  0.0643 -0.0481\n",
      "  0.0143  0.0125  0.0592 -0.0781 -0.0212\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0751 -0.0441 -0.1314 -0.0647\n",
      "  0.0334 -0.1760 -0.1744  0.0369  0.0008\n",
      "  0.0643  0.0650 -0.1496 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1934\n",
      "  0.0604  0.0861 -0.0189 -0.1015 -0.0565\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0887 -0.0322  0.0334\n",
      " -0.1326  0.1553  0.1177  0.1299 -0.1733\n",
      " -0.0784  0.0653  0.0403 -0.1282  0.0069\n",
      "  0.0510 -0.1013  0.0019 -0.0489  0.0513\n",
      " -0.1697 -0.1396 -0.1768  0.0360 -0.1063\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1708 -0.1845  0.1475  0.1374 -0.0094\n",
      "  0.1567  0.0862 -0.0326  0.0202  0.0633\n",
      " -0.0656  0.0035 -0.1308  0.0776 -0.0355\n",
      "  0.0447 -0.0382 -0.0373 -0.0405  0.0012\n",
      "  0.0511  0.0078 -0.0660 -0.0332  0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "前向传播前, conv1的梯度大小: \n",
      "None\n",
      "输出前向传播的输出结果:\n",
      "Variable containing:\n",
      "-2.1917 -2.3389 -2.4776 -2.5055 -2.5838 -2.1226 -2.3332 -2.4761 -2.2067 -1.9665\n",
      "-2.3421 -2.4224 -2.2239 -2.4818 -2.2421 -2.1416 -2.1980 -2.3784 -2.6176 -2.0957\n",
      "-2.0686 -2.3997 -2.2660 -2.3733 -2.3183 -2.0729 -2.4807 -2.3592 -2.6409 -2.1870\n",
      "-2.2944 -2.3667 -2.4883 -2.5643 -2.3807 -2.1352 -2.2432 -2.4597 -2.1665 -2.0517\n",
      "-2.1448 -2.3865 -2.2335 -2.5471 -2.5662 -2.1384 -2.2525 -2.2114 -2.7266 -2.0348\n",
      "-2.2982 -2.2594 -2.3840 -2.3467 -2.5374 -2.4862 -2.0554 -2.5187 -2.1425 -2.1284\n",
      "-2.4886 -2.5447 -2.1866 -2.5661 -2.2188 -2.0752 -2.5031 -2.5515 -2.2258 -1.9153\n",
      "-1.9393 -2.5985 -2.1147 -2.8482 -2.4422 -2.2964 -2.1619 -2.4603 -2.3845 -2.0960\n",
      "-2.1799 -2.3747 -2.3158 -2.4328 -2.4555 -2.2211 -2.2499 -2.3455 -2.3372 -2.1603\n",
      "-2.2281 -2.5368 -2.4331 -2.4517 -2.4350 -1.9741 -2.0728 -2.5939 -2.2360 -2.2509\n",
      "-1.8192 -2.7740 -2.3935 -2.6287 -2.5342 -2.2678 -2.1573 -2.5078 -2.3442 -1.9945\n",
      "-2.4192 -2.5439 -2.1110 -2.4508 -2.4340 -2.1790 -2.3352 -2.6602 -2.1291 -1.9763\n",
      "-2.1247 -2.4333 -2.4106 -2.5872 -2.3709 -2.1526 -2.4335 -2.6375 -2.2811 -1.8529\n",
      "-2.3751 -2.7142 -2.0875 -2.5173 -2.3513 -2.0784 -2.4173 -2.0669 -2.6543 -2.0431\n",
      "-2.1912 -2.4592 -2.4106 -2.4180 -2.3067 -2.1690 -2.1832 -2.4349 -2.3272 -2.1882\n",
      "-2.2212 -2.2410 -2.3946 -2.3726 -2.4396 -2.2294 -2.1965 -2.6396 -2.3027 -2.0926\n",
      "-1.9898 -2.2431 -2.1093 -2.6554 -2.6940 -2.2502 -2.2139 -2.4399 -2.4631 -2.1987\n",
      "-2.3077 -2.4465 -2.3485 -2.5105 -2.5310 -2.4227 -2.3416 -2.7747 -1.7772 -1.9572\n",
      "-2.0951 -2.2901 -2.5304 -2.3550 -2.2505 -2.1212 -2.1675 -2.7168 -2.5046 -2.1744\n",
      "-2.3021 -2.3970 -2.5456 -2.5876 -2.3763 -2.1785 -2.4074 -2.2853 -2.2199 -1.9050\n",
      "-2.2898 -2.4483 -2.2757 -2.4523 -2.3434 -2.0998 -2.1776 -2.2350 -2.7781 -2.0995\n",
      "-2.2040 -2.2526 -2.3080 -2.5174 -2.3956 -2.1536 -2.1230 -2.2663 -2.6692 -2.2584\n",
      "-2.2036 -2.5150 -2.1456 -2.5483 -2.4155 -2.0466 -2.3890 -2.2637 -2.6616 -2.0441\n",
      "-2.2354 -2.2853 -2.2650 -2.6003 -2.3806 -2.1687 -2.4592 -2.5543 -2.4610 -1.8463\n",
      "-2.5697 -2.5652 -1.9627 -2.3670 -2.2222 -2.4777 -2.2041 -2.2341 -2.6983 -2.0008\n",
      "-2.1056 -2.7707 -2.0606 -2.7980 -2.2783 -2.0909 -2.6696 -2.3528 -2.5673 -1.8317\n",
      "-2.3632 -2.4257 -2.5368 -2.4694 -2.4007 -2.2030 -2.3762 -2.4359 -2.0958 -1.9026\n",
      "-1.9568 -2.3545 -2.4787 -2.5966 -2.2041 -2.0351 -2.5239 -2.2615 -3.1984 -1.9719\n",
      "-2.2489 -2.1556 -2.4867 -2.4040 -2.3254 -2.2293 -2.1297 -2.3336 -2.5161 -2.2713\n",
      "-2.2829 -2.4468 -2.3339 -2.5784 -2.4063 -2.2632 -2.3624 -2.4603 -2.1860 -1.8806\n",
      "-2.4679 -2.2778 -2.7220 -2.5969 -2.2259 -2.0969 -2.3808 -2.4264 -1.9929 -2.0831\n",
      "-2.4238 -2.5591 -2.2387 -2.5329 -2.1949 -2.1496 -2.2046 -2.4003 -2.4711 -2.0052\n",
      "-2.1285 -2.6520 -2.0582 -2.5639 -2.4473 -1.8961 -2.4036 -2.4409 -2.6807 -2.0946\n",
      "-2.2828 -2.5400 -2.4459 -2.2604 -2.2999 -2.0228 -2.5025 -2.4390 -2.4166 -1.9858\n",
      "-2.5354 -2.4369 -2.6835 -2.4380 -2.1282 -2.3610 -2.3397 -2.5180 -2.0610 -1.8326\n",
      "-2.3134 -2.6355 -2.2767 -2.5351 -2.2555 -2.2306 -2.4475 -2.2983 -2.2260 -1.9637\n",
      "-2.3555 -2.4203 -2.2633 -2.5761 -2.1811 -2.2187 -2.0691 -2.6659 -2.1896 -2.2376\n",
      "-2.3995 -2.3211 -2.2956 -2.4347 -2.5406 -2.5550 -2.0133 -2.4852 -1.9873 -2.1864\n",
      "-2.3185 -2.5356 -2.4706 -2.6629 -2.2481 -2.2803 -2.0107 -2.6065 -2.0153 -2.1265\n",
      "-1.9324 -2.5658 -2.1262 -2.6632 -2.5522 -2.4343 -2.1997 -2.1198 -2.7826 -2.0326\n",
      "-2.1326 -2.2145 -2.4450 -2.7521 -2.4630 -2.3214 -1.9683 -2.4741 -2.2274 -2.2378\n",
      "-2.3462 -2.3429 -2.2402 -2.4967 -2.5272 -2.1842 -2.1444 -2.5370 -2.2098 -2.1120\n",
      "-2.1757 -2.5165 -2.4680 -2.6204 -2.4094 -2.3591 -2.3112 -2.6958 -1.8663 -1.9484\n",
      "-2.1342 -2.8745 -2.0702 -2.8340 -1.9909 -2.2673 -2.4702 -2.3902 -2.7212 -1.8442\n",
      "-2.3864 -2.5237 -2.1162 -2.5365 -2.1206 -2.0354 -2.1538 -2.3193 -2.7570 -2.3045\n",
      "-2.2868 -2.2972 -2.4244 -2.2220 -2.3672 -2.0699 -2.3322 -2.6889 -2.3263 -2.1344\n",
      "-2.1557 -2.7498 -2.2694 -2.6719 -2.1903 -2.1308 -2.7246 -2.4672 -2.4089 -1.7327\n",
      "-2.3071 -2.2596 -2.4543 -2.3650 -2.4317 -2.1026 -2.1948 -2.4624 -2.3949 -2.1340\n",
      "-2.3137 -2.3152 -2.4037 -2.4310 -2.4515 -2.1451 -2.1701 -2.3205 -2.3438 -2.1850\n",
      "-2.2577 -2.3574 -2.1525 -2.7303 -2.4588 -2.3310 -1.9631 -2.8991 -2.0003 -2.2460\n",
      "-2.2999 -2.3248 -2.2850 -2.5005 -2.4065 -2.1419 -2.2773 -2.3552 -2.3842 -2.1124\n",
      "-2.1142 -2.3619 -2.8975 -2.6281 -2.2181 -1.9516 -2.2129 -2.6395 -2.2779 -2.0914\n",
      "-2.1293 -2.5448 -2.0544 -2.6568 -2.4130 -2.2041 -2.2242 -2.1898 -2.7798 -2.1021\n",
      "-2.2683 -2.3625 -2.4920 -2.5399 -2.4385 -2.2241 -2.2912 -2.1167 -2.4855 -1.9633\n",
      "-2.0891 -2.3568 -2.2857 -2.5621 -2.2980 -2.3271 -2.3080 -2.5953 -2.2897 -2.0463\n",
      "-2.2074 -2.6438 -2.5295 -2.5457 -2.2072 -2.1596 -2.2656 -2.8731 -1.9984 -1.9690\n",
      "-2.2371 -2.5495 -2.2701 -2.7344 -2.3885 -2.3036 -2.7096 -1.9611 -2.3510 -1.8796\n",
      "-2.3664 -2.3153 -2.1863 -2.4156 -2.2737 -2.2603 -2.1829 -2.4413 -2.4409 -2.1920\n",
      "-2.4292 -2.1724 -2.3422 -2.4067 -2.3432 -2.2659 -2.1561 -2.3471 -2.3404 -2.2605\n",
      "-2.3696 -2.2260 -2.4691 -2.5400 -2.1486 -2.3161 -2.3544 -2.5392 -2.3237 -1.9113\n",
      "-2.1325 -2.3291 -2.2246 -2.5680 -2.4224 -2.2070 -2.2386 -2.4278 -2.4760 -2.1067\n",
      "-2.1109 -2.2991 -2.2738 -2.6320 -2.6050 -2.1885 -2.2745 -2.4374 -2.3920 -1.9951\n",
      "-2.0481 -2.6920 -2.1901 -2.5944 -2.2659 -2.0874 -2.4800 -2.3809 -2.5863 -1.9847\n",
      "-1.8215 -2.7782 -2.2195 -2.6894 -2.4385 -2.3070 -2.2767 -2.2908 -2.7802 -1.9187\n",
      "[torch.cuda.FloatTensor of size 64x10 (GPU 0)]\n",
      "\n",
      "输出本次前向传波的loss值:\n",
      "Variable containing:\n",
      " 2.3166\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "前向传播后反向传播前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1407 -0.0253  0.1999 -0.1994\n",
      " -0.1119  0.1906 -0.1263  0.0701 -0.1803\n",
      " -0.0821  0.1815  0.0779  0.1674 -0.1716\n",
      "  0.0052 -0.0165  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0292 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1960 -0.1326  0.0846 -0.1297  0.0142\n",
      "  0.1144  0.0488 -0.1724  0.1641  0.0055\n",
      " -0.0211  0.1503 -0.0945  0.1493 -0.0684\n",
      "  0.0383 -0.0214  0.0342  0.0387  0.0328\n",
      " -0.0773  0.0035 -0.0590  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1999  0.1026 -0.1491 -0.0924 -0.1258\n",
      "  0.0420  0.0497  0.1619 -0.0237  0.1122\n",
      "  0.1907  0.1579  0.0430 -0.1519  0.1576\n",
      " -0.0566  0.1748 -0.1697  0.1135 -0.1544\n",
      "  0.1580  0.1302  0.1174 -0.1951  0.0236\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1319 -0.1651  0.1064  0.1096 -0.0925\n",
      " -0.0695  0.1518 -0.1159 -0.0858  0.0017\n",
      " -0.1722 -0.1757 -0.0232  0.0240  0.1866\n",
      "  0.0097  0.0489 -0.1157  0.0142 -0.1830\n",
      " -0.0899  0.1399 -0.1802 -0.0496  0.1484\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0826 -0.0188  0.0553 -0.0241  0.1129\n",
      "  0.1193 -0.0050  0.0981  0.1629 -0.0506\n",
      "  0.1275  0.0013  0.1190  0.1001  0.1036\n",
      " -0.1814  0.1130  0.0299  0.1662  0.1661\n",
      " -0.1892  0.0051 -0.1623 -0.0477 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0287  0.1197 -0.1414  0.0240 -0.1098\n",
      " -0.1157 -0.0984 -0.0533  0.0326  0.0231\n",
      " -0.0876 -0.1740  0.1189  0.1045 -0.1423\n",
      "  0.0480 -0.0041  0.1461  0.0988  0.1371\n",
      "  0.0738  0.0284 -0.1009 -0.1743  0.1637\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0487 -0.1323  0.0369 -0.0769 -0.0445\n",
      " -0.1175  0.1258 -0.1453  0.0420 -0.1572\n",
      "  0.0530  0.1615  0.1287  0.0696 -0.1123\n",
      "  0.1034 -0.0652 -0.1831  0.0643 -0.0481\n",
      "  0.0143  0.0125  0.0592 -0.0781 -0.0212\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0751 -0.0441 -0.1314 -0.0647\n",
      "  0.0334 -0.1760 -0.1744  0.0369  0.0008\n",
      "  0.0643  0.0650 -0.1496 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1934\n",
      "  0.0604  0.0861 -0.0189 -0.1015 -0.0565\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0887 -0.0322  0.0334\n",
      " -0.1326  0.1553  0.1177  0.1299 -0.1733\n",
      " -0.0784  0.0653  0.0403 -0.1282  0.0069\n",
      "  0.0510 -0.1013  0.0019 -0.0489  0.0513\n",
      " -0.1697 -0.1396 -0.1768  0.0360 -0.1063\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1708 -0.1845  0.1475  0.1374 -0.0094\n",
      "  0.1567  0.0862 -0.0326  0.0202  0.0633\n",
      " -0.0656  0.0035 -0.1308  0.0776 -0.0355\n",
      "  0.0447 -0.0382 -0.0373 -0.0405  0.0012\n",
      "  0.0511  0.0078 -0.0660 -0.0332  0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "前向传播后反向传播前, conv1的梯度大小: \n",
      "None\n",
      "反向传播后参数更新前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1407 -0.0253  0.1999 -0.1994\n",
      " -0.1119  0.1906 -0.1263  0.0701 -0.1803\n",
      " -0.0821  0.1815  0.0779  0.1674 -0.1716\n",
      "  0.0052 -0.0165  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0292 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1960 -0.1326  0.0846 -0.1297  0.0142\n",
      "  0.1144  0.0488 -0.1724  0.1641  0.0055\n",
      " -0.0211  0.1503 -0.0945  0.1493 -0.0684\n",
      "  0.0383 -0.0214  0.0342  0.0387  0.0328\n",
      " -0.0773  0.0035 -0.0590  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1999  0.1026 -0.1491 -0.0924 -0.1258\n",
      "  0.0420  0.0497  0.1619 -0.0237  0.1122\n",
      "  0.1907  0.1579  0.0430 -0.1519  0.1576\n",
      " -0.0566  0.1748 -0.1697  0.1135 -0.1544\n",
      "  0.1580  0.1302  0.1174 -0.1951  0.0236\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1319 -0.1651  0.1064  0.1096 -0.0925\n",
      " -0.0695  0.1518 -0.1159 -0.0858  0.0017\n",
      " -0.1722 -0.1757 -0.0232  0.0240  0.1866\n",
      "  0.0097  0.0489 -0.1157  0.0142 -0.1830\n",
      " -0.0899  0.1399 -0.1802 -0.0496  0.1484\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0826 -0.0188  0.0553 -0.0241  0.1129\n",
      "  0.1193 -0.0050  0.0981  0.1629 -0.0506\n",
      "  0.1275  0.0013  0.1190  0.1001  0.1036\n",
      " -0.1814  0.1130  0.0299  0.1662  0.1661\n",
      " -0.1892  0.0051 -0.1623 -0.0477 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0287  0.1197 -0.1414  0.0240 -0.1098\n",
      " -0.1157 -0.0984 -0.0533  0.0326  0.0231\n",
      " -0.0876 -0.1740  0.1189  0.1045 -0.1423\n",
      "  0.0480 -0.0041  0.1461  0.0988  0.1371\n",
      "  0.0738  0.0284 -0.1009 -0.1743  0.1637\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0487 -0.1323  0.0369 -0.0769 -0.0445\n",
      " -0.1175  0.1258 -0.1453  0.0420 -0.1572\n",
      "  0.0530  0.1615  0.1287  0.0696 -0.1123\n",
      "  0.1034 -0.0652 -0.1831  0.0643 -0.0481\n",
      "  0.0143  0.0125  0.0592 -0.0781 -0.0212\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0751 -0.0441 -0.1314 -0.0647\n",
      "  0.0334 -0.1760 -0.1744  0.0369  0.0008\n",
      "  0.0643  0.0650 -0.1496 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1934\n",
      "  0.0604  0.0861 -0.0189 -0.1015 -0.0565\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0887 -0.0322  0.0334\n",
      " -0.1326  0.1553  0.1177  0.1299 -0.1733\n",
      " -0.0784  0.0653  0.0403 -0.1282  0.0069\n",
      "  0.0510 -0.1013  0.0019 -0.0489  0.0513\n",
      " -0.1697 -0.1396 -0.1768  0.0360 -0.1063\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1708 -0.1845  0.1475  0.1374 -0.0094\n",
      "  0.1567  0.0862 -0.0326  0.0202  0.0633\n",
      " -0.0656  0.0035 -0.1308  0.0776 -0.0355\n",
      "  0.0447 -0.0382 -0.0373 -0.0405  0.0012\n",
      "  0.0511  0.0078 -0.0660 -0.0332  0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "反向传播后参数更新前, conv1的梯度大小: \n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.3463  0.6184 -0.2758 -0.8305 -0.4801\n",
      "  -0.8215 -0.6240 -0.6429 -1.1730  0.1307\n",
      "  -1.3305 -1.0457 -0.6398 -0.8149 -0.6028\n",
      "  -1.3718 -0.5000  0.4222  0.2160 -0.4317\n",
      "  -0.8053 -0.2420  0.3499 -0.0915 -0.3746\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.8458  1.0981  1.7423  1.1609  0.2784\n",
      "   0.6371  1.1301  1.4692  1.5803  0.4052\n",
      "   1.8034  1.6476  1.7121  2.0627  0.2767\n",
      "   1.3466  1.8047  1.9710  1.0228 -0.3845\n",
      "   0.5407  1.0828  0.2695 -0.6510 -0.6353\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   1.0312  0.2995 -0.8286 -0.1971  0.6839\n",
      "   1.9231  1.3045 -0.6895 -0.3176  0.6284\n",
      "   2.4924  1.2012 -0.3450 -0.0562  0.1884\n",
      "   1.3948  1.0324 -0.0201 -0.1842 -0.0379\n",
      "   0.8227  0.7073 -0.2214 -0.6452 -0.6269\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.4975 -0.3463 -0.9919 -0.1335  0.9910\n",
      "  -0.4467 -1.0035 -0.9867  0.1089  0.9369\n",
      "  -0.4779 -0.7210 -0.5734  0.6263  1.6000\n",
      "  -0.2135 -0.4196 -0.1360  0.7248  1.2390\n",
      "   0.0434  0.1029  0.3569  1.6689  2.1162\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.5420 -0.5063  0.2818  1.1600  0.7046\n",
      "   0.8319  0.4551  0.6602  1.4149  1.1129\n",
      "   1.5212  0.9873  1.4069  1.3991  1.1776\n",
      "   0.6301  0.4116  0.1092  0.7031  0.3712\n",
      "   0.0221 -0.7653 -0.5858 -0.4976 -0.6248\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.6691 -0.2460 -0.7290 -0.5006 -0.3713\n",
      "   0.3614 -0.4556 -1.1538 -0.7508 -0.1883\n",
      "   0.6926 -0.2026 -0.5392  0.6832  0.8796\n",
      "   0.8453  0.4241  0.8854  1.5294  1.4465\n",
      "   0.1458  0.2488  0.2774  1.4359  1.3525\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.9540 -2.2914 -1.2312  0.3066  0.3058\n",
      "  -1.3925 -1.8126 -1.0787 -0.1990  0.4649\n",
      "  -1.7038 -1.4672 -0.3606  0.1169  0.8722\n",
      "  -1.2839 -0.9938  0.0822  0.6386  0.5338\n",
      "  -1.1617 -1.3410 -0.5339  0.6857  0.8773\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.0117 -0.8731 -0.8799 -0.6177 -0.6013\n",
      "  -0.5717 -0.9042 -0.2218 -0.5013 -0.1205\n",
      "  -0.3034  0.0373  0.4982  0.1243  0.2656\n",
      "   0.0287 -0.1075  0.3334  0.2179  1.2106\n",
      "  -0.1266  0.3910  0.5489  0.2352  1.0603\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.1507  0.2236  0.4486 -0.1891 -1.2527\n",
      "   0.3117  0.6156  0.4889 -0.3017 -0.7800\n",
      "   1.1671  1.3772  0.8571  0.1191  0.3352\n",
      "   1.1114  1.4498  0.6852  0.2267  0.4673\n",
      "   0.9398  0.8102  0.0995  0.4834  1.0221\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -1.3299 -0.0858 -0.8089 -1.5902 -2.0066\n",
      "  -0.4566 -0.1631 -1.2764 -2.0821 -1.8971\n",
      "  -0.1947 -0.3759 -0.8444 -1.4726 -1.7862\n",
      "   0.5720  0.9287  0.2534 -0.4292 -0.4988\n",
      "   1.3671  1.2267  0.3378  0.4742  0.8270\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "参数更新后, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1406 -0.0253  0.1999 -0.1993\n",
      " -0.1118  0.1906 -0.1263  0.0702 -0.1803\n",
      " -0.0820  0.1817  0.0780  0.1675 -0.1715\n",
      "  0.0054 -0.0164  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0293 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1961 -0.1327  0.0845 -0.1298  0.0142\n",
      "  0.1144  0.0487 -0.1725  0.1640  0.0054\n",
      " -0.0213  0.1502 -0.0947  0.1491 -0.0684\n",
      "  0.0382 -0.0216  0.0340  0.0386  0.0328\n",
      " -0.0773  0.0034 -0.0591  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1998  0.1026 -0.1491 -0.0924 -0.1259\n",
      "  0.0418  0.0495  0.1620 -0.0236  0.1121\n",
      "  0.1905  0.1578  0.0430 -0.1519  0.1576\n",
      " -0.0568  0.1746 -0.1697  0.1135 -0.1544\n",
      "  0.1579  0.1301  0.1174 -0.1951  0.0237\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1318 -0.1651  0.1065  0.1096 -0.0926\n",
      " -0.0695  0.1519 -0.1158 -0.0858  0.0017\n",
      " -0.1722 -0.1756 -0.0231  0.0240  0.1865\n",
      "  0.0097  0.0489 -0.1157  0.0141 -0.1831\n",
      " -0.0899  0.1399 -0.1803 -0.0497  0.1482\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0827 -0.0188  0.0553 -0.0242  0.1128\n",
      "  0.1192 -0.0050  0.0981  0.1627 -0.0507\n",
      "  0.1273  0.0012  0.1188  0.0999  0.1035\n",
      " -0.1814  0.1129  0.0298  0.1661  0.1660\n",
      " -0.1892  0.0052 -0.1622 -0.0476 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0288  0.1197 -0.1414  0.0240 -0.1097\n",
      " -0.1158 -0.0983 -0.0532  0.0327  0.0231\n",
      " -0.0877 -0.1740  0.1190  0.1044 -0.1424\n",
      "  0.0479 -0.0042  0.1460  0.0987  0.1369\n",
      "  0.0738  0.0284 -0.1010 -0.1744  0.1636\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0488 -0.1321  0.0370 -0.0769 -0.0446\n",
      " -0.1173  0.1259 -0.1452  0.0421 -0.1573\n",
      "  0.0532  0.1616  0.1288  0.0696 -0.1124\n",
      "  0.1035 -0.0651 -0.1831  0.0642 -0.0482\n",
      "  0.0144  0.0127  0.0592 -0.0782 -0.0213\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0752 -0.0440 -0.1314 -0.0647\n",
      "  0.0334 -0.1759 -0.1743  0.0370  0.0008\n",
      "  0.0643  0.0650 -0.1497 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1932\n",
      "  0.0604  0.0860 -0.0190 -0.1015 -0.0566\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0886 -0.0322  0.0335\n",
      " -0.1326  0.1552  0.1177  0.1299 -0.1732\n",
      " -0.0786  0.0652  0.0402 -0.1283  0.0068\n",
      "  0.0509 -0.1014  0.0018 -0.0489  0.0513\n",
      " -0.1698 -0.1397 -0.1768  0.0360 -0.1064\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1706 -0.1845  0.1475  0.1376 -0.0092\n",
      "  0.1567  0.0862 -0.0324  0.0204  0.0635\n",
      " -0.0656  0.0035 -0.1307  0.0778 -0.0353\n",
      "  0.0447 -0.0383 -0.0373 -0.0405  0.0013\n",
      "  0.0509  0.0077 -0.0660 -0.0332  0.1091\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 预先记录前向传播前的参数大小(以conv1的weiht为例)\n",
    "print(\"前向传播前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"前向传播前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#初始化梯度\n",
    "optimizer.zero_grad()  # zero the gradient buffers，必须要置零\n",
    "\n",
    "#前向传播\n",
    "output = model(data)   #前向传播的输出结果, 是一个 64x10的结果, 其中64表示batch_size,10为类别数\n",
    "print(\"输出前向传播的输出结果:\")\n",
    "print(output)\n",
    "\n",
    "#计算loss\n",
    "loss = F.nll_loss(output, target)    # 计算本次前向传波的loss值\n",
    "print(\"输出本次前向传波的loss值:\")\n",
    "print(loss)\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"前向传播后反向传播前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"前向传播后反向传播前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#反向传播\n",
    "loss.backward()    # loss的反向传播\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"反向传播后参数更新前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"反向传播后参数更新前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#参数更新\n",
    "optimizer.step()\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"参数更新后, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程详细分析\n",
    "(1). 前向传播的目的是为了通过网络计算出最终的预测结果, \n",
    "再通过损失函数计算出预测值与真实值之间的偏差,即loss, \n",
    "将loss值极小化作为优化目标, 极小化预测值与真实值之间的差别.\n",
    "\n",
    "(2). 优化的方式就是通过计算梯度, 来更新参数包括所有的weight, bias. \n",
    "进而使得对于输入的data都可以准确的预测其结果.\n",
    "\n",
    "(3).于是反向传播的过程也就是根据loss计算相关参数的grad, 计算方式采用的是连式法则.\n",
    "\n",
    "(4).在计算完各层参数的grad之后,对参数进行更新,\n",
    "这个更新后的参数,可以使得通过网络得到的预测结果与真实结果之间的差异变小,达到优化的目的.\n",
    "\n",
    "(5). 而在pytorch代码层面上, 可以发现weight的更新时发生在optimizer.step() (即参数更新指令)之后的, \n",
    "而梯度生成时发生在loss.backward() (反向传播)之后的, \n",
    "并且梯度是单次有效的,每次前向传播,反向传播后的梯度都是根据本次loss计算出来的,\n",
    "因此在进行前向传播之前,要将梯度初始化.\n",
    "\n",
    "#### 当然, 感兴趣的话还可以打印每次卷积前后data发生的变化,这对于理解卷积过程也是有所帮助的, 这部分内容我将放在后面检测过程中的单步结果中讲."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面, 我们展示一下网络的详细检测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 1 ====. Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      "-2.3931e+01 -2.0862e+01 -1.5924e+01 -1.4905e+01 -2.4521e+01 -2.7253e+01\n",
      "\n",
      "Columns 6 to 9 \n",
      "-4.1199e+01 -1.9073e-06 -1.9175e+01 -1.4312e+01\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n",
      "==== 2 ==== Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      "-2.3931e+01 -2.0862e+01 -1.5924e+01 -1.4905e+01 -2.4521e+01 -2.7253e+01\n",
      "\n",
      "Columns 6 to 9 \n",
      "-4.1199e+01 -1.9073e-06 -1.9175e+01 -1.4312e+01\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n",
      "==== 3 ==== 检测结果为: \t \n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "==== 4 ==== 真实结果为: \t Variable containing:\n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#以比较蠢的方式选取第一张图片数据\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "#如下两种方式得到,softmax结果:\n",
    "output = model(data)\n",
    "print(\"==== 1 ====.\", output) \n",
    "\n",
    "output = model.forward(data)\n",
    "print(\"==== 2 ====\", output) \n",
    "\n",
    "#对比检测结果与真实结果: (softmax最大的值对应的index对应的label即为检测结果)\n",
    "pred = output.data.max(1, keepdim=True)[1] \n",
    "print(\"==== 3 ==== 检测结果为: \\t {}\".format(pred))\n",
    "print(\"==== 4 ==== 真实结果为: \\t {}\".format(target))\n",
    "\n",
    "#print(output.data.max(1, keepdim=True))\n",
    "# 可见检测结果是正确的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测过程详细分析\n",
    "\n",
    "(1). 检测就是一个前向传播的过程, 将通过卷积,全连最终接得到feature map, 通过softmax进行分类.\n",
    "\n",
    "(2). pytorch代码上, 可以发现model() 和 model,forward() 都可以得到结果.\n",
    "\n",
    "(3). 对于softmax的原理, 可以看机器学习相关资料, 这里不再展开.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Variable containing:\n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n",
    "\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "print(target)  #打印真实label,可见这张图是7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输入图片的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.6450  1.9305  1.5996\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.4015  2.8088  2.8088\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.4286  1.0268  0.4922\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.4978  0.3395  0.0340 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  2.8088  2.6433  2.0960  2.0960  2.0960  2.0960  2.0960  2.0960\n",
      "  1.0268  1.6505  2.4651  2.8088  2.4396  2.8088  2.8088  2.8088  2.7578\n",
      " -0.4242 -0.4242 -0.2078  0.4159 -0.2460  0.4286  0.4286  0.4286  0.3268\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1442\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.2177\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.3268  2.7451\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.2686  2.8088\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.3097  2.1851  2.7324\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.1795  2.8088  1.8923\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5304  2.7706  2.6306  0.3013\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.1824  2.3887  2.8088  1.6887 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.3860  2.1596  2.8088  2.3633  0.0213 -0.4242\n",
      " -0.4242 -0.4242 -0.4242  0.0595  2.8088  2.8088  0.5559 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.0296  2.4269  2.8088  1.0395 -0.4115 -0.4242 -0.4242\n",
      " -0.4242 -0.4242  1.2686  2.8088  2.8088  0.2377 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  0.3522  2.6560  2.8088  2.8088  0.2377 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.1159  2.8088  2.8088  2.3633  0.0849 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.1159  2.8088  2.2105 -0.1951 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.0960  2.0960  1.7396  0.2377 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.4906  2.8088  2.8088  1.3577 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.1569  2.5797  2.8088  0.9250 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.6322  2.7960  2.2360 -0.1951 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.5415  2.8215  0.6322 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  2.6051  0.1358 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  0.3649 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.9560 -0.3606 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.3140 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.cuda.FloatTensor of size 1x1x28x28 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "conv1卷积核参数:\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0153  0.3259  0.1931  0.1950 -0.2066\n",
      " -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
      " -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
      "  0.0233 -0.0447  0.1835  0.1321  0.3404\n",
      "  0.1407  0.1476  0.2579  0.1959  0.0585\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0104 -0.0224 -0.3664 -0.4861 -0.2800\n",
      "  0.1141  0.1102 -0.2812 -0.4165 -0.3379\n",
      "  0.1372  0.1199  0.2881 -0.0999  0.1634\n",
      "  0.2905  0.3823  0.5954  0.5653  0.1324\n",
      " -0.1438  0.0064  0.0802  0.3379  0.2844\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.0798  0.0704 -0.1030  0.2154 -0.0975\n",
      " -0.1746 -0.2341 -0.2216  0.1732  0.1978\n",
      " -0.3685 -0.1535 -0.1572  0.1322  0.3971\n",
      " -0.2559 -0.1418 -0.1172  0.3395  0.2059\n",
      " -0.1713 -0.0901  0.2505  0.3522  0.2085\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1241 -0.1835 -0.3326 -0.1037 -0.1134\n",
      "  0.3162  0.0332 -0.0893 -0.2823 -0.2161\n",
      "  0.1476  0.1888 -0.1981 -0.2667 -0.2407\n",
      "  0.1627  0.0904  0.0908 -0.2753 -0.1488\n",
      "  0.2771  0.0394 -0.1264 -0.2166 -0.0730\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.3005 -0.2953 -0.1426 -0.0045 -0.1248\n",
      " -0.0297 -0.2309 -0.2401 -0.1252  0.0536\n",
      " -0.3134 -0.0809 -0.0239  0.0702 -0.0181\n",
      "  0.0806 -0.0049 -0.0313  0.1956  0.0620\n",
      " -0.1641  0.0897  0.1914  0.4325  0.2904\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1330  0.2172  0.1960 -0.0969  0.1130\n",
      "  0.1974  0.0932  0.1406 -0.2397 -0.0977\n",
      "  0.3167  0.3363 -0.2704 -0.1850 -0.0186\n",
      "  0.3908  0.1115 -0.2426 -0.1940 -0.2603\n",
      "  0.2042  0.2092 -0.1117 -0.2013 -0.1616\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0198  0.2373  0.2729 -0.0540 -0.1854\n",
      "  0.0802  0.3993  0.1696 -0.0028 -0.2808\n",
      "  0.2505  0.3697 -0.0181 -0.1314 -0.0736\n",
      "  0.0541  0.1614  0.2596 -0.0544 -0.0701\n",
      "  0.1702  0.2460  0.2558  0.3425  0.3234\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.0982  0.1252 -0.0135 -0.0752 -0.3418\n",
      " -0.1307  0.2256  0.3210 -0.2659 -0.3056\n",
      " -0.2033  0.0767  0.5127  0.0826 -0.2298\n",
      " -0.3450 -0.0224  0.3877  0.3312  0.0064\n",
      " -0.2941 -0.4151  0.0306  0.1472 -0.0846\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0570 -0.0232 -0.0239 -0.1755  0.2165\n",
      "  0.0881 -0.1158 -0.0787  0.1568  0.2332\n",
      "  0.0131 -0.0756 -0.2181  0.0534  0.3254\n",
      "  0.0991  0.0501  0.0569  0.3412 -0.0361\n",
      " -0.1060  0.0797 -0.0573  0.1666  0.1814\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1200  0.0947 -0.0307 -0.2284 -0.1777\n",
      "  0.0758 -0.0426  0.0249 -0.2037  0.0998\n",
      " -0.2321  0.0203 -0.0582 -0.2084 -0.1820\n",
      " -0.2436 -0.0961 -0.2408 -0.2075 -0.2478\n",
      " -0.0113 -0.1164 -0.1620  0.1383 -0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "conv1偏置项参数:\n",
      "Parameter containing:\n",
      " 0.0825\n",
      " 0.2392\n",
      "-0.1182\n",
      " 0.1155\n",
      " 0.0808\n",
      "-0.2142\n",
      "-0.2408\n",
      "-0.1918\n",
      "-0.1114\n",
      "-0.0601\n",
      "[torch.cuda.FloatTensor of size 10 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "经过conv1后的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "           ...             ⋱             ...          \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "           ...             ⋱             ...          \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "           ...             ⋱             ...          \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "           ...             ⋱             ...          \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x24x24 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层分析\n",
    "data_ori = data\n",
    "print(\"原始输入图片的数字结构:\")\n",
    "print(data_ori)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"conv1卷积核参数:\")\n",
    "print(model.conv1.weight)\n",
    "print(\"conv1偏置项参数:\")\n",
    "print(model.conv1.bias)\n",
    "\n",
    "print(\"========================================\")\n",
    "data_1 = model.conv1(data_ori)\n",
    "print(\"经过conv1后的数字结构:\")\n",
    "print(data_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积过程详细分析:\n",
    "（1）这里原始输入data_ori的维度是（1x1x28x28）\n",
    "\n",
    "（2）卷积核conv1.weight的维度是（10x1x5x5）\n",
    "\n",
    "（3）偏置项conv1.bias的维度是 10\n",
    "\n",
    "（4）可见输出的feature map 的维度是（1x10x24x24）\n",
    "\n",
    "#### 输出的feature map 大小计算公式 \n",
    "    输入：（N_0, C_0, H_0, W_0）\n",
    "    输出：（N_1, C_1, H_1, W_1）\n",
    "    有： W_1 = (W_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         H_1 = (H_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         C_1 = 卷积核个数 （备注：卷积核大小为（C_1, C_0, kernel_size, kernel_size））\n",
    "         N_1 = N_0 = batch_size\n",
    "    其中, \n",
    "        pad为padding的大小（其就是在原始输入外侧加 pad圈 0, 那么原始为h×w的大小，加padding之后就变为了(h+2*pad)x(w+2*pad)的大小）\n",
    "        kernel_size为卷积核的大小\n",
    "        stride为卷积步长,这里等于默认值1.\n",
    "   ##### 需要注意的是N表示batch_size, 即当前批次共有batch_size张图, 这个是不变的, 即有N_1 = N_0\n",
    "    \n",
    "#### 本例中输出的feature map 详细计算过程\n",
    "    可以看到输出的feature map中第一个输出值: data_1(1,1,1,1) = -0.0017\n",
    "    它是由：data_ori中左上角, 大小为(N_0 x C_0 x 5 x 5), 即\n",
    "    data_ori(:,:,0:5,0:5)=\n",
    "     [-0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242]\n",
    "    与 第一层的 conv1.weight\n",
    "    conv1.weight(0 ,：,.,.) = \n",
    "      [0.0153  0.3259  0.1931  0.1950 -0.2066\n",
    "      -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
    "      -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
    "      0.0233 -0.0447  0.1835  0.1321  0.3404\n",
    "      0.1407  0.1476  0.2579  0.1959  0.0585]\n",
    "    做点乘, 然后加上\n",
    "    conv1.bias(0) = 0.0825\n",
    "    可以使用计算器计算下结果：得到 -0.00174612\n",
    "    可见与他计算的data_1(1,1,1,1) = -0.0017是相当的\n",
    "\n",
    "#### 总结输出的feature map 详细计算公式\n",
    "    已知输入data_ori(N_0, C_0, H_0, W_0),  conv1.weight(C_1, C_0, kernel_size, kernel_size),  conv1.bias(C_1)\n",
    "    求输出的data_1(n,c,h,w)其值为\n",
    "    data_ori(n, :, h×stride:h×stride+kernel_size, w×stride:w×stride+kernel_size)\n",
    "    与\n",
    "    conv.weight(c ,: , : ,:)做点乘\n",
    "    加上\n",
    "    conv.bias(c)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "经过max pooling后的feature map数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017  0.3455  1.2724  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017  1.6837  3.3778  ...   2.4993  0.7371 -0.0017\n",
      "           ...             ⋱             ...          \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630  0.7679  1.1940  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630  2.7411  6.7602  ...   3.5579  0.1652 -0.2630\n",
      "           ...             ⋱             ...          \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608  0.6066  1.3461  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608  3.0434  3.3505  ...  -0.1443 -0.3742 -0.2608\n",
      "           ...             ⋱             ...          \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.2614  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.5649  3.0527  ...  -1.5655 -0.1412  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756  0.0296  0.1225  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756  1.5005  1.4817  ...   0.2098 -0.5287 -0.5756\n",
      "           ...             ⋱             ...          \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.4119 -0.6259  ...   0.5980  0.9780  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "经过conv1后的数字结构:\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017  0.0608  0.3455  0.8538  1.2724  1.4396  1.3006  0.8535\n",
      " -0.0017 -0.0017  0.5275  1.6837  2.7457  3.3778  3.3344  3.1512  2.6747\n",
      " -0.0017 -0.0017  0.8974  1.2590  1.6062  1.2892  1.3017  1.6038  2.2797\n",
      " -0.0017 -0.0017 -0.3010 -1.2731 -2.2763 -2.5375 -3.1587 -2.5038 -1.8821\n",
      " -0.0017 -0.0017 -1.0842 -2.3369 -1.9042 -1.8973 -2.1895 -2.8344 -2.9603\n",
      " -0.0017 -0.0017 -0.8184 -0.7709 -0.0775  0.9084  0.4683  0.0965 -0.1638\n",
      " -0.0017 -0.0017 -0.1779 -0.1352  0.2566  0.4353  0.5155  0.3479  0.4869\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0464 -0.1331\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.0005\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.0395\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.0213  0.4029\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.2316  1.4803\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.0437  0.8652  1.7949\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.3526  1.3541  1.1868\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.5308  0.7673 -0.0971\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.1483 -0.7748 -1.8839\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      "  0.4996  0.1734  0.0627 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      "  2.4576  2.1946  2.1039  2.0162  2.0162  2.0162  2.0162  1.9954  1.8377\n",
      "  3.0039  3.6255  3.9424  4.1317  4.1313  4.1576  4.1107  3.9753  3.3676\n",
      " -1.2762 -0.6847 -0.3738  0.0987  0.0568  0.0144  0.1171  0.5596  0.9202\n",
      " -3.9484 -4.3085 -4.6537 -4.5226 -4.5697 -4.7228 -3.5051 -2.3684 -1.3582\n",
      " -0.4938 -0.9599 -1.5885 -1.8035 -1.6925 -1.1929  0.1243  0.6024  0.2624\n",
      "  1.0183  1.2752  1.0073  1.0110  1.3938  2.7425  2.7830  1.4057  1.2838\n",
      "  0.1670  0.0896  0.2999  0.2679  1.3167  2.2086  1.0132 -0.3329  0.3618\n",
      " -0.0017 -0.0017 -0.0017  0.3529  1.4477  1.0786 -0.8503 -0.6226  0.0705\n",
      " -0.0017 -0.0017  0.0050  0.6705  1.0553 -0.2341 -1.1998 -0.2379  0.3001\n",
      " -0.0017 -0.0017  0.1310  1.0215  0.5068 -0.8174 -0.4138  0.4674  0.0689\n",
      " -0.0017  0.0541  0.9060  1.4459  0.3201 -0.4437  0.6590  0.5221 -0.3402\n",
      "  0.0124  0.5351  1.8143  1.3910 -0.4377 -0.1002  0.6833 -0.0517 -0.3649\n",
      "  0.2392  1.5924  1.9914  0.3194 -0.9065 -0.0053  0.1393 -0.4005 -0.3027\n",
      "  1.1412  1.7883  0.9720 -1.1891 -0.6066 -0.0167 -0.2895 -0.3347 -0.0950\n",
      "  1.6663  1.2136 -0.9374 -1.2041 -0.5392 -0.0012 -0.3768 -0.2032  0.0093\n",
      "  1.6012 -0.1374 -0.9147 -0.5659  0.2526 -0.1816 -0.0516 -0.0485 -0.0017\n",
      "  0.9821 -0.4056  0.2994  1.0234  0.4291  0.0995  0.0090  0.0051 -0.0017\n",
      "  0.1692  0.2703  1.0794  0.6725  0.1500  0.0298  0.0110 -0.0017 -0.0017\n",
      " -0.2058  0.1201  0.3927 -0.5828 -0.9598 -0.2440 -0.0016 -0.0017 -0.0017\n",
      " -2.4738 -1.6648 -1.1554 -1.0119 -1.1567 -0.2346 -0.0017 -0.0017 -0.0017\n",
      "\n",
      "Columns 18 to 23 \n",
      "  -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      "  1.4128  0.8431  0.4004  0.0914 -0.0017 -0.0017\n",
      "  2.4993  1.4736  0.7371  0.2644 -0.0017 -0.0017\n",
      "  0.5247  0.3231  0.0010  0.0664 -0.0017 -0.0017\n",
      " -1.2413 -1.4040 -1.1422 -0.4949 -0.0017 -0.0017\n",
      "  0.1531 -0.9371 -1.3626 -0.6350 -0.0017 -0.0017\n",
      "  1.1979 -0.0818 -0.6946 -0.2703 -0.0017 -0.0017\n",
      "  0.5796 -0.3212 -0.2587 -0.0218 -0.0017 -0.0017\n",
      " -0.2484 -0.5561 -0.2119  0.0017 -0.0017 -0.0017\n",
      " -0.3312 -0.4037 -0.0849 -0.0017 -0.0017 -0.0017\n",
      " -0.1033  0.0714  0.0068 -0.0017 -0.0017 -0.0017\n",
      " -0.3068 -0.0010 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0756 -0.0008 -0.0017 -0.0017 -0.0017 -0.0017\n",
      "  0.0095 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      "[torch.cuda.FloatTensor of size 1x24x24 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "经过max pooling后的feature map第一层输出的数字结构:\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017 -0.0017\n",
      " -0.0017  0.3455  1.2724  1.4396  0.8535  0.1734 -0.0017 -0.0017 -0.0017\n",
      " -0.0017  1.6837  3.3778  3.3344  3.0039  3.9424  4.1317  4.1576  3.9753\n",
      " -0.0017 -0.3010 -1.8973 -2.1895 -1.2762 -0.3738  0.0987  0.1171  0.9202\n",
      " -0.0017 -0.1352  0.9084  0.5155  1.0183  1.2752  1.3938  2.7830  1.4057\n",
      " -0.0017 -0.0017 -0.0017 -0.0017  0.1670  0.2999  1.4477  2.2086  0.3618\n",
      " -0.0017 -0.0017 -0.0017 -0.0017 -0.0017  0.1310  1.0553 -0.2341  0.4674\n",
      " -0.0017 -0.0017 -0.0017 -0.0017  0.0124  1.8143  1.4459  0.6833  0.5221\n",
      " -0.0017 -0.0017 -0.0017 -0.0017  1.1412  1.9914  0.3194  0.1393 -0.0950\n",
      " -0.0017 -0.0017 -0.0017  0.2316  1.6663  1.2136  0.2526 -0.0012  0.0093\n",
      " -0.0017 -0.0017 -0.0017  1.3541  1.7949  1.0794  1.0234  0.0995  0.0051\n",
      " -0.0017 -0.0017 -0.0017  0.7673 -0.0971  0.3927 -0.5828 -0.0016 -0.0017\n",
      "\n",
      "Columns 9 to 11 \n",
      "  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017\n",
      "  2.4993  0.7371 -0.0017\n",
      "  0.5247  0.0664 -0.0017\n",
      "  1.1979 -0.2703 -0.0017\n",
      "  0.5796  0.0017 -0.0017\n",
      "  0.0714  0.0068 -0.0017\n",
      " -0.0008 -0.0017 -0.0017\n",
      "  0.0095 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017\n",
      "[torch.cuda.FloatTensor of size 1x12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pooling层分析\n",
    "print(\"========================================\")\n",
    "data_2 = F.max_pool2d(data_1, 2)\n",
    "print(\"经过max pooling后的feature map数字结构:\")\n",
    "print(data_2)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"输入的feature map的第一层输入的数字结构:\")\n",
    "print(data_1[:,0,:,:])\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"经过max pooling后的输出的feature map第一层输出的数字结构:\")\n",
    "print(data_2[:,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max pooling详细分析\n",
    "（1）这里输入的feature map为 data_1, 其维度是（1x10x24x24）\n",
    "\n",
    "（2）max pooling的 kernel_size 为2\n",
    "\n",
    "（3）max pooling的 stride = 2, padding = 0(默认值)\n",
    "\n",
    "（3）max pooling不存在参数\n",
    "\n",
    "（4）可见输出的feature map 的维度是（1x10x12x12）\n",
    "\n",
    "#### max pooling输出feature map 大小计算公式:\n",
    "    输入：（N_0, C_0, H_0, W_0）\n",
    "    输出：（N_1, C_1, H_1, W_1）\n",
    "    有: W_1 = (W_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         H_1 = (H_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         N_1 = N_0\n",
    "         C_1 = C_0\n",
    "         \n",
    "#### 本例中输出的feature map 详细计算过程(以第一层输入输出为例)\n",
    "    可以看到输出的feature map中第一个输出值: data_2(1,1,1,1) = -0.0017\n",
    "    它是由：data_ori中左上角, 大小为(N_0 x C_0 x 2 x 2), 即\n",
    "    data_ori(:,:,0:2,0:2)=\n",
    "     [-0.0017 -0.0017 \n",
    "     -0.0017 -0.0017]\n",
    "    中取最大的数即 -0.0017 \n",
    "\n",
    "#### 总结输出的feature map 详细计算公式\n",
    "    已知输入data_1(N_0, C_0, H_0, W_0),  max_pooling(kernel_size, kernel_size)\n",
    "    求输出的data_2(n,c,h,w)其值为\n",
    "    data_ori(n, c, h×stride:h×stride+kernel_size, w×stride:w×stride+kernel_size)\n",
    "    中最大的值\n",
    "    \n",
    "#### 除此max pooling外, 还有average pooling, 将取最大改为求平均即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.3455  1.2724  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  1.6837  3.3778  ...   2.4993  0.7371  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.7679  1.1940  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  2.7411  6.7602  ...   3.5579  0.1652  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.6066  1.3461  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  3.0434  3.3505  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.2614  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.5649  3.0527  ...   0.0000  0.0000  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0296  0.1225  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  1.5005  1.4817  ...   0.2098  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.4119  0.0000  ...   0.5980  0.9780  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# relu分析\n",
    "data_3 = F.relu(data_2)\n",
    "print(data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可见relu的作用就是将所有的负值全变为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.3455  1.2724  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  1.6837  3.3778  ...   2.4993  0.7371  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.7679  1.1940  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  2.7411  6.7602  ...   3.5579  0.1652  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.6066  1.3461  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  3.0434  3.3505  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.2614  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.5649  3.0527  ...   0.0000  0.0000  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0296  0.1225  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  1.5005  1.4817  ...   0.2098  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.4119  0.0000  ...   0.5980  0.9780  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.3455  1.2724  1.4396  0.8535  0.1734  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  1.6837  3.3778  3.3344  3.0039  3.9424  4.1317  4.1576  3.9753  2.4993\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0987  0.1171  0.9202  0.5247\n",
      " 0.0000  0.0000  0.9084  0.5155  1.0183  1.2752  1.3938  2.7830  1.4057  1.1979\n",
      " 0.0000  0.0000  0.0000  0.0000  0.1670  0.2999  1.4477  2.2086  0.3618  0.5796\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.1310  1.0553  0.0000  0.4674  0.0714\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0124  1.8143  1.4459  0.6833  0.5221  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  1.1412  1.9914  0.3194  0.1393  0.0000  0.0095\n",
      " 0.0000  0.0000  0.0000  0.2316  1.6663  1.2136  0.2526  0.0000  0.0093  0.0000\n",
      " 0.0000  0.0000  0.0000  1.3541  1.7949  1.0794  1.0234  0.0995  0.0051  0.0000\n",
      " 0.0000  0.0000  0.0000  0.7673  0.0000  0.3927  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 10 to 11 \n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.7371  0.0000\n",
      " 0.0664  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0017  0.0000\n",
      " 0.0068  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 12x12 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  0.3455  1.2724  1.4396  0.8535  0.1734  0.0000  0.0000  0.0000  0.0000\n",
      " 0.0000  1.6837  3.3778  3.3344  3.0039  3.9424  4.1317  4.1576  3.9753  2.4993\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0987  0.1171  0.9202  0.5247\n",
      " 0.0000  0.0000  0.9084  0.5155  1.0183  1.2752  1.3938  2.7830  1.4057  1.1979\n",
      " 0.0000  0.0000  0.0000  0.0000  0.1670  0.2999  1.4477  2.2086  0.3618  0.5796\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.1310  1.0553  0.0000  0.4674  0.0714\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0124  1.8143  1.4459  0.6833  0.5221  0.0000\n",
      " 0.0000  0.0000  0.0000  0.0000  1.1412  1.9914  0.3194  0.1393  0.0000  0.0095\n",
      " 0.0000  0.0000  0.0000  0.2316  1.6663  1.2136  0.2526  0.0000  0.0093  0.0000\n",
      " 0.0000  0.0000  0.0000  1.3541  1.7949  1.0794  1.0234  0.0995  0.0051  0.0000\n",
      " 0.0000  0.0000  0.0000  0.7673  0.0000  0.3927  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 10 to 11 \n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.7371  0.0000\n",
      " 0.0664  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0017  0.0000\n",
      " 0.0068  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      " 0.0000  0.0000\n",
      "[torch.cuda.FloatTensor of size 12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropout分析\n",
    "data_4 = model.conv2_drop(data_3)\n",
    "print(data_4)\n",
    "#输出dropout 前后 data变化\n",
    "print(data_3[0][0])\n",
    "print(data_4[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以看到dropout对 feature map的值是不发生作用的, 发生作用的地方是在做反向传播的时候, 随机选取50%的参数不做更新, 这里的参数不是data, 而是weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  1.4278  4.6537\n",
      "  1.4789  3.7263  1.5381  5.6258\n",
      "  3.6111  4.5027  2.9709  4.1608\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.9423  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.8625\n",
      "  0.3288  0.2927  3.0163  4.6285\n",
      "  0.0000  1.5496  4.2584  3.4905\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  2.0538  0.0000  3.2203  4.3586\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0596  3.8516  3.8381  0.0000\n",
      "  1.6316  5.0432  1.8663  0.0000\n",
      "\n",
      "(0 ,3 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  1.3039  1.6838  6.2781  3.9044\n",
      "  0.0000  2.1764  4.8490  0.0000\n",
      "  0.0000  3.9669  4.8148  0.0000\n",
      "\n",
      "(0 ,4 ,.,.) = \n",
      "  4.5605  3.4471  3.1032  3.6521\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.2399  0.0000  1.5308  0.1347\n",
      "  0.4592  0.3001  2.0400  0.0000\n",
      "\n",
      "(0 ,5 ,.,.) = \n",
      "  7.5186  7.6845  5.5027  0.9139\n",
      "  0.7199  0.0420  0.0000  0.3314\n",
      "  0.0000  0.0000  0.0000  0.3550\n",
      "  0.5168  0.0000  0.0000  2.0920\n",
      "\n",
      "(0 ,6 ,.,.) = \n",
      "  1.9850  4.2534  4.5308  0.8775\n",
      "  1.5024  2.3591  0.4591  0.0263\n",
      "  0.0445  0.0000  1.1943  0.4853\n",
      "  0.0000  0.0000  1.0827  0.1767\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.5577\n",
      "  0.0000  0.0000  2.9400  5.9152\n",
      "  0.0000  0.0000  4.3578  3.9151\n",
      "  0.0000  3.8651  4.4425  0.0000\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.1141\n",
      "  0.0000  0.0000  0.0000  4.8505\n",
      "  0.0000  0.0000  3.5778  6.1633\n",
      "  0.0000  0.3245  6.4293  4.8102\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  3.8774  3.8048  3.0886  0.1005\n",
      "  4.4834  4.0057  2.0880  0.0894\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,10,.,.) = \n",
      "  7.1521  8.5299  6.4809  4.3107\n",
      "  1.8966  4.1193  3.6547  0.9806\n",
      "  0.1185  0.0000  0.0000  0.3489\n",
      "  0.0000  0.0000  0.0000  1.0587\n",
      "\n",
      "(0 ,11,.,.) = \n",
      "  5.2279  4.8831  2.8296  0.0000\n",
      "  2.3045  0.3476  0.0000  0.0000\n",
      "  0.0000  0.0000  0.8301  1.7054\n",
      "  0.0000  0.0000  1.7602  0.8139\n",
      "\n",
      "(0 ,12,.,.) = \n",
      "  0.0000  0.0000  2.8747  4.9522\n",
      "  0.0000  0.0000  3.8448  5.4075\n",
      "  0.0000  0.0000  1.6091  0.0000\n",
      "  0.0000  1.8615  0.0000  0.0000\n",
      "\n",
      "(0 ,13,.,.) = \n",
      "  0.5822  3.0807  5.6038  5.5548\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.2652  1.7636  0.0000  0.0738\n",
      "  1.9917  1.2470  0.7079  0.0000\n",
      "\n",
      "(0 ,14,.,.) = \n",
      "  2.5587  3.9183  4.4978  4.2096\n",
      "  0.3842  0.0000  0.9650  0.0000\n",
      "  0.0033  3.1279  0.7542  0.0000\n",
      "  4.0328  3.1480  0.0000  0.0000\n",
      "\n",
      "(0 ,15,.,.) = \n",
      "  3.8946  4.7747  6.0395  0.0000\n",
      "  3.1097  6.2064  7.3545  0.6185\n",
      "  1.0198  1.0677  0.0000  0.0000\n",
      "  1.7164  1.6107  0.0000  0.0000\n",
      "\n",
      "(0 ,16,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0215  6.4836  6.8000  0.2988\n",
      "  2.4543  8.3626  1.0008  1.5816\n",
      "\n",
      "(0 ,17,.,.) = \n",
      "  3.2479  2.0450  2.5464  1.5312\n",
      "  0.0000  0.0000  0.0000  0.7133\n",
      "  0.0000  0.0000  0.0000  0.4370\n",
      "  0.0000  0.0000  0.0000  0.1338\n",
      "\n",
      "(0 ,18,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  6.0200  6.2574  4.9912  0.0000\n",
      "  0.0000  0.3572  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,19,.,.) = \n",
      "  2.9372  3.5126  1.8300  1.1791\n",
      "  2.1404  0.3379  0.0000  0.3085\n",
      "  0.0000  0.0000  0.0000  2.4746\n",
      "  0.0000  0.0000  0.9793  2.1479\n",
      "[torch.cuda.FloatTensor of size 1x20x4x4 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.4278  4.6537  1.4789  3.7263\n",
      "\n",
      "Columns 10 to 19 \n",
      " 1.5381  5.6258  3.6111  4.5027  2.9709  4.1608  0.9423  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.0000  0.0000  0.0000  0.8625  0.3288  0.2927  3.0163  4.6285  0.0000  1.5496\n",
      "\n",
      "Columns 30 to 39 \n",
      " 4.2584  3.4905  2.0538  0.0000  3.2203  4.3586  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.0596  3.8516  3.8381  0.0000  1.6316  5.0432  1.8663  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.0000  0.0000  1.3039  1.6838  6.2781  3.9044  0.0000  2.1764  4.8490  0.0000\n",
      "\n",
      "Columns 60 to 69 \n",
      " 0.0000  3.9669  4.8148  0.0000  4.5605  3.4471  3.1032  3.6521  0.0000  0.0000\n",
      "\n",
      "Columns 70 to 79 \n",
      " 0.0000  0.0000  0.2399  0.0000  1.5308  0.1347  0.4592  0.3001  2.0400  0.0000\n",
      "\n",
      "Columns 80 to 89 \n",
      " 7.5186  7.6845  5.5027  0.9139  0.7199  0.0420  0.0000  0.3314  0.0000  0.0000\n",
      "\n",
      "Columns 90 to 99 \n",
      " 0.0000  0.3550  0.5168  0.0000  0.0000  2.0920  1.9850  4.2534  4.5308  0.8775\n",
      "\n",
      "Columns 100 to 109 \n",
      " 1.5024  2.3591  0.4591  0.0263  0.0445  0.0000  1.1943  0.4853  0.0000  0.0000\n",
      "\n",
      "Columns 110 to 119 \n",
      " 1.0827  0.1767  0.0000  0.0000  0.0000  0.5577  0.0000  0.0000  2.9400  5.9152\n",
      "\n",
      "Columns 120 to 129 \n",
      " 0.0000  0.0000  4.3578  3.9151  0.0000  3.8651  4.4425  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 130 to 139 \n",
      " 0.0000  0.1141  0.0000  0.0000  0.0000  4.8505  0.0000  0.0000  3.5778  6.1633\n",
      "\n",
      "Columns 140 to 149 \n",
      " 0.0000  0.3245  6.4293  4.8102  3.8774  3.8048  3.0886  0.1005  4.4834  4.0057\n",
      "\n",
      "Columns 150 to 159 \n",
      " 2.0880  0.0894  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 160 to 169 \n",
      " 7.1521  8.5299  6.4809  4.3107  1.8966  4.1193  3.6547  0.9806  0.1185  0.0000\n",
      "\n",
      "Columns 170 to 179 \n",
      " 0.0000  0.3489  0.0000  0.0000  0.0000  1.0587  5.2279  4.8831  2.8296  0.0000\n",
      "\n",
      "Columns 180 to 189 \n",
      " 2.3045  0.3476  0.0000  0.0000  0.0000  0.0000  0.8301  1.7054  0.0000  0.0000\n",
      "\n",
      "Columns 190 to 199 \n",
      " 1.7602  0.8139  0.0000  0.0000  2.8747  4.9522  0.0000  0.0000  3.8448  5.4075\n",
      "\n",
      "Columns 200 to 209 \n",
      " 0.0000  0.0000  1.6091  0.0000  0.0000  1.8615  0.0000  0.0000  0.5822  3.0807\n",
      "\n",
      "Columns 210 to 219 \n",
      " 5.6038  5.5548  0.0000  0.0000  0.0000  0.0000  0.2652  1.7636  0.0000  0.0738\n",
      "\n",
      "Columns 220 to 229 \n",
      " 1.9917  1.2470  0.7079  0.0000  2.5587  3.9183  4.4978  4.2096  0.3842  0.0000\n",
      "\n",
      "Columns 230 to 239 \n",
      " 0.9650  0.0000  0.0033  3.1279  0.7542  0.0000  4.0328  3.1480  0.0000  0.0000\n",
      "\n",
      "Columns 240 to 249 \n",
      " 3.8946  4.7747  6.0395  0.0000  3.1097  6.2064  7.3545  0.6185  1.0198  1.0677\n",
      "\n",
      "Columns 250 to 259 \n",
      " 0.0000  0.0000  1.7164  1.6107  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 260 to 269 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0215  6.4836  6.8000  0.2988  2.4543  8.3626\n",
      "\n",
      "Columns 270 to 279 \n",
      " 1.0008  1.5816  3.2479  2.0450  2.5464  1.5312  0.0000  0.0000  0.0000  0.7133\n",
      "\n",
      "Columns 280 to 289 \n",
      " 0.0000  0.0000  0.0000  0.4370  0.0000  0.0000  0.0000  0.1338  0.0000  0.0000\n",
      "\n",
      "Columns 290 to 299 \n",
      " 0.0000  0.0000  6.0200  6.2574  4.9912  0.0000  0.0000  0.3572  0.0000  0.0000\n",
      "\n",
      "Columns 300 to 309 \n",
      " 0.0000  0.0000  0.0000  0.0000  2.9372  3.5126  1.8300  1.1791  2.1404  0.3379\n",
      "\n",
      "Columns 310 to 319 \n",
      " 0.0000  0.3085  0.0000  0.0000  0.0000  2.4746  0.0000  0.0000  0.9793  2.1479\n",
      "[torch.cuda.FloatTensor of size 1x320 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view函数的解释, 作用相当于reshape\n",
    "data_5 = F.relu(F.max_pool2d(model.conv2_drop(model.conv2(data_3)), 2))\n",
    "data_6 = data_5.view(-1, 320)\n",
    "print(data_5)\n",
    "print(data_6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view函数的详细解释(reshap)\n",
    "    (1)可以看到 data_5的 大小为(1x20x4x4)\n",
    "    (2)可以看到 data_6的 大小为(1x320)\n",
    "    \n",
    "    对于data_5.view(-1, 320) 这里-1代表的意思是如果一共有参数n个,第二维度大小为320,第一维度的大小就是n/320,\n",
    "    而这里参数个数一共有20*4*4=320个, 于是-1实际代表的是1\n",
    "    作用就是,将输入由四维大小(1,20,4,4)转化为二维大小(1,320)\n",
    "    \n",
    "#### 为什么要做这个操作?\n",
    "    因为在这后面要做的是全连接,即构造线性函数,每一个参数与之前的feature map的值都要相乘并加上对应的偏置项,\n",
    "    于是在做全连接之前,先把feature 拉成线性的, 根据计算知道这里feature map一共有320个值\n",
    "    这就是为什么view(-1, 320) 为什么取320而不取其他数,因为是算出来的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-4.0338e-02 -6.5420e-02  2.9302e-02  ...   1.5774e-02 -1.1305e-02 -3.3663e-02\n",
      "-4.1198e-02  8.7542e-02  5.5837e-02  ...   3.9004e-02  5.4171e-02  2.7169e-02\n",
      " 5.0260e-03  1.5068e-02  3.4877e-02  ...  -2.1720e-02 -5.0910e-03  3.5259e-02\n",
      "                ...                   ⋱                   ...                \n",
      " 3.2933e-03 -1.4169e-01 -1.0570e-01  ...   4.1575e-02 -5.8910e-03  4.8875e-03\n",
      " 4.1089e-02  4.7138e-02  1.2651e-01  ...  -2.6279e-02  4.5768e-02 -4.5279e-02\n",
      "-1.0499e-01 -8.1729e-02  7.7756e-02  ...  -3.7410e-02  1.1007e-02 -4.5733e-02\n",
      "[torch.cuda.FloatTensor of size 50x320 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -2.2186\n",
      "  1.1325\n",
      " -3.7462\n",
      "  0.0095\n",
      "  5.1125\n",
      "  5.2625\n",
      "  6.7460\n",
      "  4.4131\n",
      " -2.3670\n",
      "  3.0571\n",
      " -3.1146\n",
      "  1.1816\n",
      " -0.6933\n",
      "  0.0099\n",
      "  3.0595\n",
      "  4.1246\n",
      "  3.0742\n",
      "  2.0759\n",
      "  0.6459\n",
      "  0.8945\n",
      "  7.3118\n",
      "  2.3470\n",
      " -2.6763\n",
      "  0.3897\n",
      "  7.6166\n",
      "  5.1986\n",
      "  6.7618\n",
      "  7.9958\n",
      "  6.9429\n",
      "  4.4286\n",
      " -0.8830\n",
      "  9.8462\n",
      "  2.8981\n",
      "  6.2058\n",
      "  4.4228\n",
      " -2.2390\n",
      "  5.3023\n",
      "  5.3017\n",
      "  0.8226\n",
      "  6.5077\n",
      "  4.3101\n",
      " -2.0635\n",
      " -1.8775\n",
      "  1.6398\n",
      "  3.0341\n",
      "  4.3542\n",
      "  0.2515\n",
      "  7.4655\n",
      "  7.8568\n",
      "  3.6777\n",
      "[torch.cuda.FloatTensor of size 50 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      " -1.0449  -8.2869   5.3824   9.5446   6.5458  -8.3712  -7.7773   6.0719\n",
      "\n",
      "Columns 8 to 15 \n",
      "  8.0330   6.6544  -4.5391  -9.5820  -5.0040  -5.6120  -9.5278  -7.0249\n",
      "\n",
      "Columns 16 to 23 \n",
      " -1.6771  -6.2779   6.6371  -2.3270  -8.0951  -5.6178 -10.1184  -1.4593\n",
      "\n",
      "Columns 24 to 31 \n",
      " -4.4467   8.2166  -4.6894 -14.1489  -5.7013  -4.1587   8.1968   6.7633\n",
      "\n",
      "Columns 32 to 39 \n",
      "  0.4900  -8.5459   5.7732  -9.6709   4.9946  -3.3426   6.4647  -6.3564\n",
      "\n",
      "Columns 40 to 47 \n",
      "  5.0565 -17.4092   1.4055 -12.0347 -11.3845  -1.1785  -2.9493  -3.9071\n",
      "\n",
      "Columns 48 to 49 \n",
      " -9.5983   6.3450\n",
      "[torch.cuda.FloatTensor of size 1x50 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "\t fc详细计算过程, 以第一个输出为例:\n",
      "========================================\n",
      "data_6的值:\n",
      "Variable containing:\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.4278\n",
      " 4.6537\n",
      " 1.4789\n",
      " 3.7263\n",
      " 1.5381\n",
      " 5.6258\n",
      " 3.6111\n",
      " 4.5027\n",
      " 2.9709\n",
      " 4.1608\n",
      " 0.9423\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.8625\n",
      " 0.3288\n",
      " 0.2927\n",
      " 3.0163\n",
      " 4.6285\n",
      " 0.0000\n",
      " 1.5496\n",
      " 4.2584\n",
      " 3.4905\n",
      " 2.0538\n",
      " 0.0000\n",
      " 3.2203\n",
      " 4.3586\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0596\n",
      " 3.8516\n",
      " 3.8381\n",
      " 0.0000\n",
      " 1.6316\n",
      " 5.0432\n",
      " 1.8663\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.3039\n",
      " 1.6838\n",
      " 6.2781\n",
      " 3.9044\n",
      " 0.0000\n",
      " 2.1764\n",
      " 4.8490\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.9669\n",
      " 4.8148\n",
      " 0.0000\n",
      " 4.5605\n",
      " 3.4471\n",
      " 3.1032\n",
      " 3.6521\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2399\n",
      " 0.0000\n",
      " 1.5308\n",
      " 0.1347\n",
      " 0.4592\n",
      " 0.3001\n",
      " 2.0400\n",
      " 0.0000\n",
      " 7.5186\n",
      " 7.6845\n",
      " 5.5027\n",
      " 0.9139\n",
      " 0.7199\n",
      " 0.0420\n",
      " 0.0000\n",
      " 0.3314\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.3550\n",
      " 0.5168\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.0920\n",
      " 1.9850\n",
      " 4.2534\n",
      " 4.5308\n",
      " 0.8775\n",
      " 1.5024\n",
      " 2.3591\n",
      " 0.4591\n",
      " 0.0263\n",
      " 0.0445\n",
      " 0.0000\n",
      " 1.1943\n",
      " 0.4853\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.0827\n",
      " 0.1767\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.5577\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.9400\n",
      " 5.9152\n",
      " 0.0000\n",
      " 0.0000\n",
      " 4.3578\n",
      " 3.9151\n",
      " 0.0000\n",
      " 3.8651\n",
      " 4.4425\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.1141\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 4.8505\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.5778\n",
      " 6.1633\n",
      " 0.0000\n",
      " 0.3245\n",
      " 6.4293\n",
      " 4.8102\n",
      " 3.8774\n",
      " 3.8048\n",
      " 3.0886\n",
      " 0.1005\n",
      " 4.4834\n",
      " 4.0057\n",
      " 2.0880\n",
      " 0.0894\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 7.1521\n",
      " 8.5299\n",
      " 6.4809\n",
      " 4.3107\n",
      " 1.8966\n",
      " 4.1193\n",
      " 3.6547\n",
      " 0.9806\n",
      " 0.1185\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.3489\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.0587\n",
      " 5.2279\n",
      " 4.8831\n",
      " 2.8296\n",
      " 0.0000\n",
      " 2.3045\n",
      " 0.3476\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.8301\n",
      " 1.7054\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.7602\n",
      " 0.8139\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.8747\n",
      " 4.9522\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.8448\n",
      " 5.4075\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.6091\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.8615\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.5822\n",
      " 3.0807\n",
      " 5.6038\n",
      " 5.5548\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2652\n",
      " 1.7636\n",
      " 0.0000\n",
      " 0.0738\n",
      " 1.9917\n",
      " 1.2470\n",
      " 0.7079\n",
      " 0.0000\n",
      " 2.5587\n",
      " 3.9183\n",
      " 4.4978\n",
      " 4.2096\n",
      " 0.3842\n",
      " 0.0000\n",
      " 0.9650\n",
      " 0.0000\n",
      " 0.0033\n",
      " 3.1279\n",
      " 0.7542\n",
      " 0.0000\n",
      " 4.0328\n",
      " 3.1480\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.8946\n",
      " 4.7747\n",
      " 6.0395\n",
      " 0.0000\n",
      " 3.1097\n",
      " 6.2064\n",
      " 7.3545\n",
      " 0.6185\n",
      " 1.0198\n",
      " 1.0677\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.7164\n",
      " 1.6107\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0215\n",
      " 6.4836\n",
      " 6.8000\n",
      " 0.2988\n",
      " 2.4543\n",
      " 8.3626\n",
      " 1.0008\n",
      " 1.5816\n",
      " 3.2479\n",
      " 2.0450\n",
      " 2.5464\n",
      " 1.5312\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.7133\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.4370\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.1338\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 6.0200\n",
      " 6.2574\n",
      " 4.9912\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.3572\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.9372\n",
      " 3.5126\n",
      " 1.8300\n",
      " 1.1791\n",
      " 2.1404\n",
      " 0.3379\n",
      " 0.0000\n",
      " 0.3085\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.4746\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.9793\n",
      " 2.1479\n",
      "[torch.cuda.FloatTensor of size 320 (GPU 0)]\n",
      "\n",
      "第一层weight值:\n",
      "Variable containing:\n",
      "-0.0403\n",
      "-0.0654\n",
      " 0.0293\n",
      "-0.0220\n",
      " 0.0594\n",
      "-0.0769\n",
      "-0.0388\n",
      " 0.0575\n",
      "-0.0590\n",
      "-0.0742\n",
      " 0.0576\n",
      " 0.0405\n",
      "-0.0811\n",
      "-0.0595\n",
      " 0.0487\n",
      " 0.0388\n",
      "-0.0020\n",
      " 0.0186\n",
      "-0.0727\n",
      "-0.0404\n",
      " 0.0734\n",
      " 0.0295\n",
      "-0.0232\n",
      "-0.0463\n",
      "-0.0103\n",
      " 0.1836\n",
      " 0.0884\n",
      "-0.0654\n",
      "-0.1494\n",
      "-0.1318\n",
      "-0.0302\n",
      " 0.0359\n",
      "-0.0370\n",
      " 0.0249\n",
      " 0.0544\n",
      "-0.0563\n",
      "-0.0828\n",
      " 0.0789\n",
      "-0.0133\n",
      "-0.0178\n",
      "-0.1151\n",
      "-0.1059\n",
      "-0.0464\n",
      " 0.0653\n",
      " 0.1190\n",
      " 0.0678\n",
      " 0.0282\n",
      " 0.0063\n",
      " 0.0014\n",
      "-0.0043\n",
      " 0.0008\n",
      " 0.0252\n",
      "-0.0331\n",
      " 0.0047\n",
      "-0.0460\n",
      "-0.0286\n",
      "-0.0053\n",
      " 0.0686\n",
      " 0.0203\n",
      "-0.0401\n",
      "-0.0053\n",
      " 0.0862\n",
      " 0.0141\n",
      "-0.0499\n",
      "-0.0008\n",
      "-0.0023\n",
      " 0.0331\n",
      " 0.0056\n",
      "-0.0403\n",
      "-0.0068\n",
      " 0.0073\n",
      " 0.0413\n",
      "-0.0731\n",
      "-0.0300\n",
      "-0.0907\n",
      " 0.0533\n",
      " 0.0998\n",
      " 0.0871\n",
      "-0.0400\n",
      "-0.0040\n",
      " 0.0187\n",
      "-0.0137\n",
      "-0.0456\n",
      " 0.0862\n",
      " 0.0597\n",
      " 0.0666\n",
      " 0.0096\n",
      "-0.0328\n",
      " 0.1117\n",
      "-0.0182\n",
      "-0.1137\n",
      "-0.0823\n",
      " 0.0590\n",
      "-0.0353\n",
      " 0.0060\n",
      "-0.0665\n",
      " 0.0327\n",
      " 0.0496\n",
      " 0.0050\n",
      " 0.0223\n",
      "-0.0142\n",
      " 0.0314\n",
      " 0.0027\n",
      "-0.0589\n",
      " 0.0148\n",
      " 0.1117\n",
      " 0.0011\n",
      "-0.1575\n",
      " 0.0148\n",
      " 0.0798\n",
      "-0.0003\n",
      "-0.1123\n",
      "-0.0164\n",
      " 0.0322\n",
      "-0.0132\n",
      "-0.0133\n",
      " 0.0354\n",
      " 0.0725\n",
      " 0.0130\n",
      "-0.0588\n",
      "-0.1110\n",
      "-0.0826\n",
      "-0.0121\n",
      "-0.0241\n",
      "-0.1480\n",
      " 0.0284\n",
      " 0.0646\n",
      " 0.1048\n",
      "-0.0040\n",
      " 0.0026\n",
      " 0.0173\n",
      "-0.0691\n",
      " 0.0540\n",
      " 0.0243\n",
      " 0.0723\n",
      "-0.0214\n",
      "-0.1842\n",
      "-0.1369\n",
      "-0.0065\n",
      "-0.0553\n",
      "-0.0799\n",
      "-0.0083\n",
      " 0.1216\n",
      " 0.0179\n",
      "-0.0376\n",
      "-0.0355\n",
      " 0.0091\n",
      "-0.0369\n",
      "-0.0881\n",
      " 0.0459\n",
      " 0.0444\n",
      " 0.0205\n",
      "-0.0082\n",
      " 0.1520\n",
      " 0.0884\n",
      "-0.0602\n",
      "-0.0333\n",
      " 0.0256\n",
      "-0.0013\n",
      "-0.0865\n",
      " 0.0002\n",
      " 0.0222\n",
      " 0.0266\n",
      " 0.0410\n",
      "-0.0064\n",
      " 0.0034\n",
      "-0.0414\n",
      " 0.0255\n",
      " 0.0083\n",
      " 0.0418\n",
      "-0.0018\n",
      "-0.0259\n",
      " 0.0121\n",
      " 0.0557\n",
      "-0.0365\n",
      "-0.0763\n",
      " 0.0163\n",
      " 0.0387\n",
      "-0.0025\n",
      " 0.0068\n",
      " 0.0190\n",
      " 0.0246\n",
      " 0.0422\n",
      "-0.0582\n",
      " 0.0132\n",
      " 0.0544\n",
      "-0.0667\n",
      "-0.1277\n",
      "-0.0233\n",
      "-0.0578\n",
      "-0.0006\n",
      "-0.0209\n",
      "-0.0944\n",
      "-0.0135\n",
      "-0.0191\n",
      "-0.0366\n",
      "-0.0076\n",
      "-0.0480\n",
      "-0.0213\n",
      " 0.0031\n",
      "-0.0275\n",
      " 0.0041\n",
      " 0.0435\n",
      " 0.0378\n",
      " 0.0082\n",
      " 0.0266\n",
      " 0.0389\n",
      "-0.0123\n",
      "-0.0687\n",
      "-0.0538\n",
      " 0.0305\n",
      "-0.0045\n",
      " 0.0039\n",
      " 0.0219\n",
      " 0.0255\n",
      "-0.0334\n",
      "-0.0419\n",
      " 0.0596\n",
      "-0.0370\n",
      "-0.0661\n",
      " 0.0677\n",
      " 0.0543\n",
      " 0.0696\n",
      " 0.0254\n",
      "-0.0648\n",
      " 0.0100\n",
      "-0.0420\n",
      "-0.0146\n",
      " 0.0335\n",
      "-0.0042\n",
      " 0.0125\n",
      "-0.0022\n",
      "-0.0825\n",
      "-0.0700\n",
      "-0.1016\n",
      " 0.0313\n",
      " 0.1155\n",
      " 0.1193\n",
      " 0.0603\n",
      " 0.0227\n",
      " 0.0003\n",
      " 0.0185\n",
      "-0.0408\n",
      " 0.0333\n",
      "-0.0695\n",
      " 0.0163\n",
      "-0.0468\n",
      " 0.0367\n",
      "-0.1245\n",
      " 0.0627\n",
      " 0.0558\n",
      " 0.0079\n",
      " 0.0329\n",
      " 0.0515\n",
      " 0.0329\n",
      "-0.0321\n",
      " 0.0300\n",
      "-0.0167\n",
      "-0.0852\n",
      "-0.0166\n",
      " 0.0241\n",
      " 0.0769\n",
      "-0.0471\n",
      " 0.0243\n",
      "-0.0974\n",
      "-0.0209\n",
      "-0.0809\n",
      "-0.0170\n",
      "-0.0415\n",
      " 0.0695\n",
      "-0.0251\n",
      " 0.0636\n",
      "-0.0759\n",
      " 0.0027\n",
      " 0.0212\n",
      "-0.0401\n",
      "-0.0213\n",
      " 0.0867\n",
      " 0.0888\n",
      "-0.0027\n",
      " 0.0816\n",
      " 0.0351\n",
      "-0.0664\n",
      " 0.0218\n",
      "-0.0292\n",
      "-0.0971\n",
      "-0.0617\n",
      " 0.0107\n",
      " 0.0657\n",
      " 0.0194\n",
      " 0.0308\n",
      "-0.0237\n",
      "-0.0333\n",
      "-0.0013\n",
      "-0.0354\n",
      " 0.1099\n",
      "-0.0237\n",
      " 0.0085\n",
      " 0.0317\n",
      "-0.0551\n",
      " 0.0079\n",
      " 0.0700\n",
      " 0.0189\n",
      "-0.0197\n",
      "-0.0320\n",
      " 0.0199\n",
      "-0.0433\n",
      " 0.0893\n",
      " 0.0248\n",
      " 0.1014\n",
      " 0.0398\n",
      " 0.0140\n",
      " 0.0202\n",
      " 0.0335\n",
      "-0.0327\n",
      "-0.0378\n",
      " 0.0384\n",
      " 0.0158\n",
      "-0.0113\n",
      "-0.0337\n",
      "[torch.cuda.FloatTensor of size 320 (GPU 0)]\n",
      "\n",
      "第一个bias值:\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -2.2186\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "手动计算结果:\n",
      "320\n",
      "Variable containing:\n",
      "-1.0449\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "-1.0449\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fc 全连接分析\n",
    "data_7 = model.fc1(data_6)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)\n",
    "print(data_7)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"\\t fc详细计算过程, 以第一个输出为例:\")\n",
    "print(\"========================================\")\n",
    "print(\"data_6的值:\")\n",
    "print(data_6[0])\n",
    "\n",
    "print(\"第一层weight值:\")\n",
    "w_1 = model.fc1.weight[0]\n",
    "print(w_1)\n",
    "\n",
    "print(\"第一个bias值:\")\n",
    "b_1 = model.fc1.bias[0]\n",
    "print(b_1)\n",
    "\n",
    "print(\"手动计算结果:\")\n",
    "res=0\n",
    "for ind_i in range(len(w_1)):\n",
    "    res += data_6[0][ind_i] * w_1[ind_i]\n",
    "res +=b_1\n",
    "print(res)\n",
    "print(\"自动计算结果对比:\")\n",
    "print(data_7[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fc详细分析:\n",
    "    (1) 由定义有: fc1 = nn.Linear(320, 50) \n",
    "            其中 320表示输入大小,50为输出大小\n",
    "    (2) 输入feature map大小为 (1x320)\n",
    "    (3) 输出feature map大小为 (1x50)\n",
    "    (4) weight大小(50x320)\n",
    "    (5) bias大小(50)\n",
    " \n",
    "#### 本例中fc计算演示:\n",
    "    data_7[i][j] = data_6[i, :] * weight[j, :] + bias[j]\n",
    "    \n",
    "## 至此, 关键部分全部分析完, 感兴趣可以再研究下softmax\n",
    "#### 最后, 我们看下如何将Variable 转为numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "           ...             ⋱             ...          \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "           ...             ⋱             ...          \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "           ...             ⋱             ...          \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "           ...             ⋱             ...          \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x24x24 (GPU 0)]\n",
      "\n",
      "[[[[-0.00174922 -0.00174922 -0.00174922 ..., -0.00174922 -0.00174922\n",
      "    -0.00174922]\n",
      "   [-0.00174922 -0.00174922 -0.00174922 ..., -0.00174922 -0.00174922\n",
      "    -0.00174922]\n",
      "   [-0.00174922 -0.00174922 -0.00174922 ..., -0.00174922 -0.00174922\n",
      "    -0.00174922]\n",
      "   ..., \n",
      "   [-0.00174922 -0.00174922 -0.00174922 ..., -0.00174922 -0.00174922\n",
      "    -0.00174922]\n",
      "   [-0.00174922 -0.00174922 -0.00174922 ..., -0.00174922 -0.00174922\n",
      "    -0.00174922]\n",
      "   [-0.00174922 -0.00174922 -0.00174922 ..., -0.00174922 -0.00174922\n",
      "    -0.00174922]]\n",
      "\n",
      "  [[-0.26299047 -0.26299047 -0.26299047 ..., -0.26299047 -0.26299047\n",
      "    -0.26299047]\n",
      "   [-0.26299047 -0.26299047 -0.26299047 ..., -0.26299047 -0.26299047\n",
      "    -0.26299047]\n",
      "   [-0.26299047 -0.26299047 -0.26299047 ..., -0.26299047 -0.26299047\n",
      "    -0.26299047]\n",
      "   ..., \n",
      "   [-0.26299047 -0.26299047 -0.26299047 ..., -0.26299047 -0.26299047\n",
      "    -0.26299047]\n",
      "   [-0.26299047 -0.26299047 -0.26299047 ..., -0.26299047 -0.26299047\n",
      "    -0.26299047]\n",
      "   [-0.26299047 -0.26299047 -0.26299047 ..., -0.26299047 -0.26299047\n",
      "    -0.26299047]]\n",
      "\n",
      "  [[-0.26081282 -0.26081282 -0.26081282 ..., -0.26081282 -0.26081282\n",
      "    -0.26081282]\n",
      "   [-0.26081282 -0.26081282 -0.26081282 ..., -0.26081282 -0.26081282\n",
      "    -0.26081282]\n",
      "   [-0.26081282 -0.26081282 -0.26081282 ..., -0.26081282 -0.26081282\n",
      "    -0.26081282]\n",
      "   ..., \n",
      "   [-0.26081282 -0.26081282 -0.26081282 ..., -0.26081282 -0.26081282\n",
      "    -0.26081282]\n",
      "   [-0.26081282 -0.26081282 -0.26081282 ..., -0.26081282 -0.26081282\n",
      "    -0.26081282]\n",
      "   [-0.26081282 -0.26081282 -0.26081282 ..., -0.26081282 -0.26081282\n",
      "    -0.26081282]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.05345865  0.05345865  0.05345865 ...,  0.05345865  0.05345865\n",
      "     0.05345865]\n",
      "   [ 0.05345865  0.05345865  0.05345865 ...,  0.05345865  0.05345865\n",
      "     0.05345865]\n",
      "   [ 0.05345865  0.05345865  0.05345865 ...,  0.05345865  0.05345865\n",
      "     0.05345865]\n",
      "   ..., \n",
      "   [ 0.05345865  0.05345865  0.05345865 ...,  0.05345865  0.05345865\n",
      "     0.05345865]\n",
      "   [ 0.05345865  0.05345865  0.05345865 ...,  0.05345865  0.05345865\n",
      "     0.05345865]\n",
      "   [ 0.05345865  0.05345865  0.05345865 ...,  0.05345865  0.05345865\n",
      "     0.05345865]]\n",
      "\n",
      "  [[-0.57564765 -0.57564765 -0.57564765 ..., -0.57564765 -0.57564765\n",
      "    -0.57564765]\n",
      "   [-0.57564765 -0.57564765 -0.57564765 ..., -0.57564765 -0.57564765\n",
      "    -0.57564765]\n",
      "   [-0.57564765 -0.57564765 -0.57564765 ..., -0.57564765 -0.57564765\n",
      "    -0.57564765]\n",
      "   ..., \n",
      "   [-0.57564765 -0.57564765 -0.57564765 ..., -0.57564765 -0.57564765\n",
      "    -0.57564765]\n",
      "   [-0.57564765 -0.57564765 -0.57564765 ..., -0.57564765 -0.57564765\n",
      "    -0.57564765]\n",
      "   [-0.57564765 -0.57564765 -0.57564765 ..., -0.57564765 -0.57564765\n",
      "    -0.57564765]]\n",
      "\n",
      "  [[ 0.98547399  0.98547399  0.98547399 ...,  0.98547399  0.98547399\n",
      "     0.98547399]\n",
      "   [ 0.98547399  0.98547399  0.98547399 ...,  0.98547399  0.98547399\n",
      "     0.98547399]\n",
      "   [ 0.98547399  0.98547399  0.98547399 ...,  0.98547399  0.98547399\n",
      "     0.98547399]\n",
      "   ..., \n",
      "   [ 0.98547399  0.98547399  0.98547399 ...,  0.98547399  0.98547399\n",
      "     0.98547399]\n",
      "   [ 0.98547399  0.98547399  0.98547399 ...,  0.98547399  0.98547399\n",
      "     0.98547399]\n",
      "   [ 0.98547399  0.98547399  0.98547399 ...,  0.98547399  0.98547399\n",
      "     0.98547399]]]]\n"
     ]
    }
   ],
   "source": [
    "print(data_1)\n",
    "out = data_1.data.cpu().numpy()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 恭喜!  结束了!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
