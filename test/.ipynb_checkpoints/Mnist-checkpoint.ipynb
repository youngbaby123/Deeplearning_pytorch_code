{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 手写字体训练与检测  --  Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable    #自动求导的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#超参数配置\n",
    "batch_size=64  #单次训练导入数据个数\n",
    "test_batch_size=1000\n",
    "epochs=10  #训练时,每张图片训练次数\n",
    "lr=0.01   #学习率\n",
    "momentum=0.5    #动量\n",
    "no_cuda=False    #是否使用GPU, False表示使用\n",
    "seed=1\n",
    "log_interval=10\n",
    "cuda = not no_cuda and torch.cuda.is_available()  #当no_cuda=False并且cuda可用时使用GPU\n",
    "\n",
    "#设置随机种子, 保证训练时初始化参数一值, 用以作对比\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "#下载 训练以及检测数据\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造网络结构, 可以看到构造了2个隐层\n",
    "#\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)    # 卷积层, 1为输入的channel, 10为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)    # 卷积层, 10为输入的channel, 20为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2_drop = nn.Dropout2d()    # Dropout层 \n",
    "        self.fc1 = nn.Linear(320, 50)  #线性函数,输入大小为320个神经元, 输出为50个神经元\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    #前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对forward的相关解释:\n",
    "(1)  x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "该操作拆开来包含三个基本操作:\n",
    "\n",
    "    1).  self.conv1(x) : 对输入的x做卷积操作,卷积大小为之前定义的 self.conv1\n",
    "\n",
    "    2).  F.max_pool2d() : max pooling操作 参数2表示 filter size\n",
    "\n",
    "    3).  F.relu() : relu函数, 用以非线性化 (备注 relu(x) = max( x , 0) )\n",
    "    \n",
    "(2)  x.view(-1, 320) \n",
    "\n",
    "    类似reshape操作, 将第二维度转化为320, 第一维随之变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练与检测的函数定义\n",
    "#在后面会详细分析\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.390087\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.350225\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.288934\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279396\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272692\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.276944\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.233707\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.222649\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181699\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.159016\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055779\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.065890\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.916480\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.910268\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.704504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.554525\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.621117\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.521305\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.533543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.339470\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.382869\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.338094\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.271376\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.084347\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.204976\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.879644\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.776240\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.967955\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.913667\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.190108\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.847057\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.893181\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.072115\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.982275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.725554\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.858674\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.810362\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.785607\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.735104\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584801\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.722361\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.745441\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.510521\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.767266\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.596363\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.570127\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.746539\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.802764\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.824961\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.653730\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.854700\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.437190\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.668879\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.589972\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.549752\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.499882\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.505056\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.515400\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.687045\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.769007\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.778845\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.788898\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.708807\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.610942\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.841369\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.470926\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.680037\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.618858\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.804772\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.494690\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.519919\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.538888\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.302490\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.494315\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.658625\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.442112\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.339776\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.509223\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.555668\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.360001\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.591205\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.591832\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.434845\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.513702\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.573484\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.374919\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.679278\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.514459\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.529196\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.412403\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.378912\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.316964\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.551598\n",
      "\n",
      "Test set: Average loss: 0.2031, Accuracy: 9401/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.480435\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.545743\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.887286\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.470167\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.329940\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.286654\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.234806\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.395047\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.298028\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.306732\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.337039\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.354378\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.394430\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.383586\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.587576\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.373231\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.535637\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.551338\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.315839\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.379615\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.318732\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.326641\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.546262\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.364304\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.612468\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.677984\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.306877\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.199852\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.502253\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.445021\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.406648\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.304673\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.373903\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.207534\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.445714\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.423703\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.295741\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.354861\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.367628\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.516056\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.469065\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.581838\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.377490\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.331874\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.399048\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.225080\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.588976\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.468710\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.282009\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.331235\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.396348\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.311375\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.316645\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.411767\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.504182\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.334745\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.293470\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.567331\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.329995\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.316156\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.377170\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.400327\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.357344\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.227250\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.313585\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.493161\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.288737\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.315879\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.362384\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.452395\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.469302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.393639\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.428729\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.275679\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.163790\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.365021\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.307671\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.426413\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.438311\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.376026\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.314323\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.320142\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.250480\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.400407\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.233706\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.315638\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.582933\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.343731\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.551217\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.515233\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.310657\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.281727\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.276409\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.556928\n",
      "\n",
      "Test set: Average loss: 0.1259, Accuracy: 9617/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.232098\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.407711\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.201903\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.230475\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.407134\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.277433\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.356236\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.327878\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.246427\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.353631\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.339663\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.231599\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.377521\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.245801\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.330036\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.468125\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.306800\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.383827\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.369223\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.189842\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.292049\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.154948\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.364966\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.386353\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.264370\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.301134\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.277636\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.185307\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.236510\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.453270\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.271522\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.396370\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.355261\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.513420\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.397085\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.319202\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.383495\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.340683\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.364426\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.366507\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.140530\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.259189\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.309180\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.251094\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.276938\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.261863\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.277366\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.293944\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.532462\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.322372\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.358663\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.313745\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.289096\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.188189\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.415025\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.127417\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.272962\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.370175\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.241562\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.366039\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.152631\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.238648\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.273348\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.293112\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.342284\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.307116\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.314846\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.245099\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.490692\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.381266\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.561362\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.304418\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.434134\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.326420\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.221008\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.202569\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.275236\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.303433\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.315283\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.157813\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.302353\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.168807\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.239335\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.428900\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.223299\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.162388\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.494536\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.448432\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.273798\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.240223\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.325727\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.142611\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.422514\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.190277\n",
      "\n",
      "Test set: Average loss: 0.0989, Accuracy: 9695/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.227598\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.225651\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.246407\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.308084\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.301032\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.361182\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.412812\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.216867\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.363977\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.206416\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.153364\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.420926\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.237961\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.340501\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.310107\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.215822\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.296759\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.281774\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.205477\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.365336\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.196481\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.223847\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.437360\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.504907\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.211094\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.139514\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.389125\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.468900\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.288837\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.218675\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.137462\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.187042\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.106103\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.303672\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.422360\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.292023\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.301153\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.280837\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.295196\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.409152\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.205780\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.369639\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.332480\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.292429\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.345469\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.299449\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.260880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.201317\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.385359\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.309881\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.212249\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.414946\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.257169\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.141080\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.219810\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.298084\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.143865\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.334172\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.362228\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.243972\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.132711\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.216682\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.419131\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.098073\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.080341\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.211008\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.151778\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.212914\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.232975\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.291832\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.591195\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.318484\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.076889\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.203410\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.356145\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.233754\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.144860\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.493324\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.195819\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.203432\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.090081\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.157556\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.455997\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.169298\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.156193\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.096736\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.192112\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.354582\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.285781\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.214679\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.390052\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.427485\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.136860\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.225616\n",
      "\n",
      "Test set: Average loss: 0.0876, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.262146\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.225903\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.239931\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.207990\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.115826\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.147189\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.181789\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.219573\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.138036\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.344480\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.235494\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.193018\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.233846\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.220410\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.226018\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.188852\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.268993\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.132675\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.222262\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.430075\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.473663\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.249303\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.121394\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.186734\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.191668\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.151747\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.200033\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.266965\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.130398\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.203022\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.274358\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.283545\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.243018\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.152499\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.176007\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.136309\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.220985\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.261006\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.415903\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.191003\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.125959\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.148403\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.217827\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.196171\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.164318\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.223679\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.279338\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.121260\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.295411\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.203262\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.182180\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.107500\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.243403\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.291361\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.312016\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.230759\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.169270\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.355976\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.166247\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.078326\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.277202\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.252745\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.105064\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.147679\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.047249\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.091506\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.399602\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.214027\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.439966\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.130844\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.461122\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.202791\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.227432\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.142659\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.166708\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.149847\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.205563\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.189417\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.176780\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.314063\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.178785\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.267964\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.318526\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.395356\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.120598\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.316037\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.078754\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.173795\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.108205\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.171408\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.101023\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.077531\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.231693\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.233874\n",
      "\n",
      "Test set: Average loss: 0.0781, Accuracy: 9749/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.167173\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.194180\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.174997\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.276751\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.213153\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.202084\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.226414\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.404350\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.381323\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.088794\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.175452\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.419886\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.140098\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.112053\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.128447\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.195268\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.235832\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.317555\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.623810\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.476527\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.357719\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.175293\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.171386\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.303306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.396643\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.277898\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.182859\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.115429\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.170990\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.305452\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.267121\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.322747\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.259888\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.295761\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.161701\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.161130\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.486215\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.176035\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.250458\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.175380\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.279800\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.188528\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.177521\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.185497\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.450367\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.727334\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.140555\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.387275\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.151299\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.311174\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.260707\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.139400\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.285460\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.115793\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.126715\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.213399\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.326559\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.217830\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.098946\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.266563\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.173596\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.095250\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.181891\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.200266\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.170326\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.130583\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.290264\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.141737\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.112495\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.193475\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.194990\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.156440\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.215527\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.153716\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.125705\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.517264\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.327953\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.077739\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.307745\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.127420\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.186149\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.273555\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.140073\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.230581\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.209937\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.148219\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.129535\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.183823\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.503663\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.167006\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.113503\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.218927\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.181527\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.110912\n",
      "\n",
      "Test set: Average loss: 0.0684, Accuracy: 9783/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.237948\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.065516\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.151794\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.270509\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.238885\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.128979\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.171192\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.141432\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.178146\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.265683\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.249582\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.240739\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.260156\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.331764\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.249658\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.149155\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.606458\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.258630\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.195065\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.171206\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.174732\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.106068\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.240219\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.219305\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.111232\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.260856\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.131776\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.147818\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.458756\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.200629\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.264951\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.373442\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.360091\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.302249\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.334725\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.142921\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.382215\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.281230\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.412657\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.144402\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.205011\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.216435\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.105028\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.206682\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.105519\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.179726\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.187730\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.226383\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.254412\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.140021\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.157189\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.346718\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.158587\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.150208\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.326552\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.143729\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.249773\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.324388\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.191079\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.172239\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.242550\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.219195\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.133524\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.161039\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.151411\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.123939\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.218582\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.325995\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.326896\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.149939\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.316204\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.179801\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.283881\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.081509\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.179442\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.130574\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.151744\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.173107\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.212449\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.119366\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.164477\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.273734\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.383748\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.340274\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.099996\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.325254\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.125408\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.212151\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.190167\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.243264\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.455328\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.164344\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.117176\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.297536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0679, Accuracy: 9773/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.444795\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.150508\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.175913\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.313038\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.199153\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.358195\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.330897\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.195107\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.251931\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.373651\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.103019\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.168833\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.229469\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.211137\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.117833\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.158808\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.170642\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.104099\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.205781\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.219751\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.399115\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.134931\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.217905\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.152934\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.204208\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.205235\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.075810\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.103716\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.157749\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.277868\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.165150\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.190395\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.174827\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.207145\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.129052\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.149606\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.346498\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.112010\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.132671\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.400758\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.205216\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.111156\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.104449\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.173368\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.308151\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.357115\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.167510\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.263277\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.135352\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.286318\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.151989\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.082321\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.060467\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.261583\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.296321\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.107815\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.249275\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.431000\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.142408\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.222006\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.176037\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.099982\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.099218\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.183854\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.150040\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.165098\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.286413\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.300385\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.282088\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.168051\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.118517\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.241152\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.241925\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.108479\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.047769\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.119558\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.254802\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.096926\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.273320\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.115342\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.144472\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.158468\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.176827\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.129512\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.286607\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.153993\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.218884\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.296551\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.209439\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.245451\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.151227\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.163403\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.246506\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.398583\n",
      "\n",
      "Test set: Average loss: 0.0629, Accuracy: 9796/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.158510\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.174759\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.172111\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.116465\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.241447\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.250493\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.198466\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.138087\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.097055\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.141961\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.129517\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.172395\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.217385\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.250692\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.123821\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.085511\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.263858\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.190279\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.406771\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.289881\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.237743\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.357679\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.173192\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.176966\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.071305\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.065666\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.145470\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.220818\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.225820\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.206518\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.341546\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.255784\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.118318\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.227874\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.193292\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.202984\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.093761\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.093040\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.337579\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.259578\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.070857\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.124335\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.229779\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.230127\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.107839\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.237171\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.247471\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.242128\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.053770\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.102941\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.054267\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.221209\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.287276\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.408163\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.412743\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.200986\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.107302\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.183877\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.268480\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.177203\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.294945\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.075268\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.233142\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.218225\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.239779\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.211740\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.203863\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.239101\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.138611\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.072594\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.343111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.084807\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.261486\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.165098\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.273619\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.073845\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.217410\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.055961\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.193206\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.280905\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.252971\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.098676\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.152378\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.080527\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.298402\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.203409\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.348434\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.311035\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.201358\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.132085\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.260366\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.053961\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.101685\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.175547\n",
      "\n",
      "Test set: Average loss: 0.0557, Accuracy: 9815/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.072708\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.160749\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.088466\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.299789\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.128857\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.106759\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.087068\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.255215\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.699816\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.141812\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.282016\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.270907\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.084571\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.145746\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.199700\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.266501\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.179667\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.231719\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.284858\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.107603\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.281102\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.167687\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.180528\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.361223\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.093334\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.153460\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.164555\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.307351\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.204428\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.265083\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.221073\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.434625\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.101654\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.142976\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.077983\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.166623\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.264705\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.137214\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.072109\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.291953\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.301281\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.187157\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.065815\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.137897\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.080799\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.161603\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.176731\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.145031\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.160311\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.158757\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.146727\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.088563\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.309806\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.441620\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.111119\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.087869\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.451382\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.150731\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.323101\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.235307\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.100626\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.382412\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.192464\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.159767\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.153395\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.253875\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.168794\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.195979\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.135069\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.182576\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.099560\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.104513\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.262352\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.232541\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.292554\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.111972\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.101813\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.168572\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.074373\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.106207\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.212728\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.129369\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.080297\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.148846\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.228558\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.180168\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.143417\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.200643\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.116699\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.130083\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.174853\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.110317\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.269810\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.079338\n",
      "\n",
      "Test set: Average loss: 0.0539, Accuracy: 9822/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练与检测过程\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxh/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# 模型的保存 与导入\n",
    "#方式一: 保存model结构以及全部参数\n",
    "torch.save(model, './model.pkl')\n",
    "model_load = torch.load('./model.pkl')\n",
    "\n",
    "#方式二: 仅保存参数\n",
    "torch.save(model.state_dict(), './model_state.pkl')\n",
    "model_load_1 = Net() #初始化模型结构\n",
    "model_load_1.load_state_dict(torch.load('./model_state.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以上即为整个网络的结构以及相应的训练与检测过程\n",
    "\n",
    "## 接下来对网络结构进行进一步详细分析\n",
    "\n",
    "    (1). 网络结构中包含两部分主要数据: data 以及 weight , 训练过程就是对输入的data 找到最合适的weight, 使得输出最能代表data的属性特征.\n",
    "\n",
    "    (2). data由原始数据而来, 一般而言可以分为train(训练集), val(验证集), test(测试集)\n",
    "\n",
    "    (3). 每一层的输出称为 feature map(特征图)\n",
    "\n",
    "    (4). 一般而言, cnn网络的浅层特征图可以包含更多局部信息, 而深层特征图包含更多全局信息.\n",
    "\n",
    "#### 第一, 本网络中 weight 结构的相关分析 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d (p=0.5)\n",
      "  (fc1): Linear (320 -> 50)\n",
      "  (fc2): Linear (50 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#打印所定义的model\n",
    "print( model)\n",
    "# 可以看到定义的基本网络结构包含conv1,conv2,conv2_drop,fc1,fc2这几个基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "1\n",
      "10\n",
      "(5, 5)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "#详细打印所定义的卷积层 conv1\n",
    "print( model.conv1)    #打印卷积核的整体参数\n",
    "print( model.conv1.in_channels)    #打印输入通道数\n",
    "print( model.conv1.out_channels)    #打印输出通道数\n",
    "print( model.conv1.kernel_size)    #打印卷积核kernel_size大小\n",
    "print( model.conv1.padding)    #打印卷积核padding大小\n",
    "\n",
    "# 解释:卷积大小 5x5x1x10 (10个5x5x1大小的卷积核, 其中1为上一层feature map层数)\n",
    "# 对于输入为28x28x1的图片, 经过该卷积, 输出为24x24x10\n",
    "# 并且可见当未对 stride(步长) 以及 padding(补偿) 进行定义时, 默认 stride = 1, padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0247 -0.0834 -0.0338  0.1239 -0.1936\n",
      " -0.1864 -0.0272 -0.0943 -0.1515  0.0306\n",
      "  0.1318 -0.0155 -0.1244  0.1550 -0.1595\n",
      " -0.1910 -0.1177  0.0955 -0.0661  0.1181\n",
      " -0.1098  0.1333  0.0280 -0.1379  0.0454\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1550 -0.0375  0.0342 -0.0751  0.1672\n",
      "  0.1394 -0.0956  0.1466 -0.1229  0.0695\n",
      " -0.1958 -0.1481 -0.0373 -0.1409 -0.0648\n",
      "  0.1743  0.1693 -0.1071  0.0298  0.0181\n",
      "  0.0560  0.1408 -0.0296 -0.1449  0.0993\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.1714 -0.1813  0.1681 -0.0786  0.0293\n",
      "  0.0163  0.0687  0.1660  0.0717  0.0531\n",
      "  0.0623  0.1037 -0.0500 -0.0773 -0.0287\n",
      " -0.0461  0.0256 -0.1186  0.1313  0.1488\n",
      "  0.1431  0.0652  0.1899 -0.1749 -0.0859\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1668 -0.0433  0.1281  0.1108 -0.1142\n",
      " -0.0550 -0.1938 -0.1270 -0.1351  0.0853\n",
      " -0.0072  0.0983  0.1947 -0.0068 -0.1382\n",
      "  0.1640  0.0487 -0.1730 -0.1342  0.0337\n",
      " -0.1676  0.0438  0.0596 -0.1276  0.0540\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1607 -0.1439  0.0727 -0.1495 -0.0805\n",
      "  0.0800  0.0655  0.0301  0.1921 -0.0631\n",
      "  0.1360  0.0467 -0.1956  0.1215  0.1110\n",
      "  0.0858  0.0425 -0.1535 -0.0573 -0.1875\n",
      " -0.1871 -0.1294 -0.0494  0.1901 -0.1304\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.0375 -0.0546  0.1578 -0.1384  0.0669\n",
      " -0.1131  0.0518  0.1932 -0.1917  0.1094\n",
      "  0.1285  0.1466 -0.0202 -0.1857  0.0699\n",
      " -0.0957  0.0607  0.1522 -0.0016  0.0461\n",
      "  0.0549  0.1973 -0.0892  0.1753 -0.0603\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0802 -0.0400 -0.1748 -0.1310 -0.0657\n",
      "  0.1307 -0.0504 -0.0832  0.0407 -0.1640\n",
      " -0.1413 -0.1868  0.1313  0.0115 -0.1388\n",
      " -0.1561  0.0923  0.1773 -0.1513 -0.1006\n",
      "  0.0238 -0.0721  0.1515 -0.0031 -0.0457\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.0706  0.1545 -0.0126 -0.1644 -0.1432\n",
      " -0.0696 -0.0898  0.0529  0.1107  0.1209\n",
      "  0.0926  0.1726  0.0350 -0.0240  0.0750\n",
      " -0.1904  0.0452 -0.1511 -0.0605  0.1450\n",
      "  0.0319  0.0284 -0.0781 -0.0817 -0.1566\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.0305  0.1288 -0.1667  0.1295 -0.1985\n",
      " -0.1212 -0.1577  0.0568 -0.1654  0.1715\n",
      "  0.1410  0.1224 -0.0847  0.1367 -0.1810\n",
      " -0.0094 -0.0521  0.0155 -0.0628  0.0044\n",
      "  0.0766  0.0614 -0.0659  0.0751 -0.0312\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1311  0.0131 -0.1882  0.1518 -0.0390\n",
      " -0.1049  0.1609 -0.1630  0.0849  0.0047\n",
      " -0.1063  0.1889 -0.0628  0.1387 -0.0369\n",
      "  0.1666 -0.1204  0.1274  0.0232 -0.1417\n",
      "  0.0497  0.0238  0.1763  0.0261  0.1760\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(1).打印随机初始化的卷积核参数 (不存在seed)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print( model.conv1.weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0332  0.1989  0.0881  0.1730 -0.2000\n",
      " -0.1488 -0.0791  0.1996 -0.1413 -0.1056\n",
      " -0.1631 -0.0414 -0.1255 -0.0448 -0.0618\n",
      "  0.0679 -0.0413  0.1742  0.0155  0.1385\n",
      " -0.0323 -0.0747  0.0741  0.0098 -0.1182\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0226  0.1512 -0.1082 -0.1890  0.0138\n",
      "  0.0682  0.1656 -0.0331 -0.0171  0.0235\n",
      " -0.0277 -0.1438  0.1757 -0.1208  0.1114\n",
      "  0.1203  0.0864  0.1873  0.1211 -0.0746\n",
      " -0.1629  0.0769  0.0073  0.1506  0.1460\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1578  0.1317 -0.1660  0.1318 -0.1844\n",
      " -0.0908 -0.1321 -0.1763  0.1513  0.0682\n",
      " -0.1607  0.0372 -0.0316  0.0687  0.1832\n",
      " -0.0353  0.0133 -0.1210  0.0768 -0.0841\n",
      " -0.0738 -0.1432  0.0746  0.1133  0.1339\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.0350 -0.1927 -0.1863  0.1001  0.0496\n",
      "  0.1955  0.0643  0.0993 -0.0806 -0.0878\n",
      " -0.0215  0.1157 -0.1112 -0.1587 -0.1707\n",
      " -0.0208 -0.0123  0.1634 -0.1615 -0.0826\n",
      "  0.1613 -0.0849 -0.1522 -0.1480  0.0099\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1923 -0.1666  0.0715  0.1667 -0.1153\n",
      "  0.1642 -0.0938 -0.0804 -0.0034  0.0338\n",
      " -0.1787  0.0264  0.0296  0.0456 -0.1413\n",
      "  0.1826  0.0357 -0.0956  0.0799 -0.1076\n",
      " -0.1591  0.0134 -0.0344  0.1800  0.0778\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0028 -0.0343  0.0162 -0.1800  0.1062\n",
      "  0.0144 -0.1819  0.0655 -0.1440  0.0060\n",
      "  0.1170  0.1778 -0.1881  0.0346  0.1533\n",
      "  0.1614  0.0163 -0.1450 -0.0208 -0.1443\n",
      "  0.1569  0.1230 -0.0490 -0.0409  0.0154\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.1339  0.0609  0.1710 -0.0555 -0.0609\n",
      "  0.0284  0.1003  0.0551  0.0904 -0.1495\n",
      "  0.1533  0.0761  0.0495  0.0591  0.1004\n",
      " -0.0584 -0.0604  0.1053 -0.0920 -0.0574\n",
      "  0.1584  0.1011 -0.0288  0.1525  0.1859\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1953  0.0654 -0.0008  0.0487 -0.1705\n",
      " -0.1541  0.1148  0.1798 -0.1744 -0.0200\n",
      " -0.0579  0.0314  0.1767 -0.0367 -0.0481\n",
      " -0.1052  0.1052  0.1614  0.1086  0.0295\n",
      " -0.0795 -0.1989  0.1091  0.0469 -0.1388\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0693  0.0315  0.0108 -0.1964  0.1544\n",
      "  0.0836 -0.0571 -0.0117  0.1634  0.1058\n",
      "  0.0493 -0.0130 -0.1937 -0.0924  0.1718\n",
      "  0.1327  0.0764  0.0205  0.1989 -0.1720\n",
      " -0.1311 -0.0110 -0.1451  0.0971  0.1730\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1232  0.0787 -0.0143 -0.1736 -0.1079\n",
      "  0.1022  0.0033  0.1016 -0.1166  0.1692\n",
      " -0.1803  0.0846  0.0075 -0.1503 -0.1311\n",
      " -0.1920 -0.0415 -0.1895 -0.1571 -0.1887\n",
      "  0.0039 -0.1015 -0.1591  0.1440 -0.0845\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(2).打印随机初始化的卷积核参数 (使用固定seed)\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print( model.conv1.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.390087\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.350225\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.288934\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279396\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272692\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.276944\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.233707\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.222649\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181699\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.159016\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055779\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.065890\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.916481\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.910268\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.704504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.554525\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.621117\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.521305\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.533543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.339470\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.382869\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.338094\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.271376\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.084347\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.204976\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.879644\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.776240\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.967955\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.913667\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.190108\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.847057\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.893181\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.072115\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.982275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.725554\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.858674\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.810363\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.785607\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.735104\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584801\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.722361\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.745441\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.510521\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.767266\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.596460\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.570140\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.749264\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.802890\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.826509\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.653605\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.858620\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.439111\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.667258\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.585826\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.549554\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.514638\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.507171\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.520978\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.682735\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.756923\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.776693\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.791183\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.712861\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.622693\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.837698\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.477014\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.697098\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.623697\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.800171\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.486367\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.522767\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.533716\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.307948\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.494099\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.649698\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.436011\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.337375\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.509784\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.555043\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.375213\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.593493\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.593096\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.441896\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.518173\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.581477\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.376156\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.687730\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.518164\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.519619\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.403492\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.392675\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.320085\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.550517\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0461  0.2039  0.0901  0.1621 -0.2127\n",
      " -0.1929 -0.1073  0.1919 -0.1386 -0.1191\n",
      " -0.2005 -0.0605 -0.1206 -0.0259 -0.0499\n",
      "  0.0586 -0.0262  0.2148  0.0684  0.1799\n",
      " -0.0042 -0.0303  0.1248  0.0607 -0.0779\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0351  0.1483 -0.1950 -0.3184 -0.1167\n",
      "  0.0919  0.1663 -0.1008 -0.1414 -0.0569\n",
      "  0.0396 -0.0267  0.2722 -0.0316  0.2126\n",
      "  0.2765  0.3265  0.4838  0.4178  0.1382\n",
      " -0.0672  0.2069  0.1837  0.3341  0.2970\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1573  0.1360 -0.1398  0.1752 -0.1074\n",
      " -0.1326 -0.1585 -0.1814  0.1935  0.1660\n",
      " -0.2240 -0.0195 -0.0641  0.1327  0.3135\n",
      " -0.0928 -0.0325 -0.1172  0.1973  0.0776\n",
      " -0.0984 -0.1342  0.1456  0.2572  0.2714\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.0503 -0.1404 -0.1845  0.0781  0.0189\n",
      "  0.2603  0.0827  0.0681 -0.1319 -0.1310\n",
      "  0.0640  0.1489 -0.1337 -0.1916 -0.1932\n",
      "  0.0852  0.0289  0.1344 -0.1926 -0.1036\n",
      "  0.2532 -0.0570 -0.1739 -0.1773 -0.0159\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.2327 -0.2082  0.0286  0.1643 -0.0790\n",
      "  0.0978 -0.1446 -0.0960  0.0183  0.0689\n",
      " -0.2386 -0.0238  0.0327  0.0942 -0.0868\n",
      "  0.1587  0.0337 -0.0455  0.1650 -0.0408\n",
      " -0.1368  0.0639  0.0643  0.3004  0.1658\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1131  0.1196  0.1173 -0.1620  0.0585\n",
      "  0.1519 -0.0387  0.0964 -0.2063 -0.0859\n",
      "  0.2667  0.2761 -0.2347 -0.0769  0.0491\n",
      "  0.3213  0.0913 -0.2000 -0.1069 -0.2085\n",
      "  0.2854  0.1995 -0.0681 -0.0901 -0.0259\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0613  0.1995  0.2804 -0.0504 -0.1510\n",
      "  0.1365  0.2850  0.1824  0.0709 -0.2587\n",
      "  0.3390  0.3406  0.2043  0.0592  0.0291\n",
      "  0.1109  0.1546  0.2583 -0.0353 -0.0425\n",
      "  0.2327  0.2020  0.0640  0.2439  0.2962\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1924  0.1016  0.0843  0.1164 -0.1858\n",
      " -0.1729  0.1412  0.2639 -0.1212 -0.0625\n",
      " -0.0767  0.0780  0.2971  0.0526 -0.0502\n",
      " -0.1150  0.1621  0.2945  0.2240  0.0772\n",
      " -0.0998 -0.1718  0.1866  0.1212 -0.1044\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0806 -0.0025 -0.0133 -0.1720  0.2379\n",
      "  0.0688 -0.0703 -0.0054  0.2383  0.2447\n",
      "  0.0307 -0.0192 -0.1713  0.0298  0.3441\n",
      "  0.1045  0.0551  0.0375  0.3139 -0.0282\n",
      " -0.1703 -0.0403 -0.1336  0.1665  0.2401\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1353  0.0667 -0.0308 -0.1928 -0.1300\n",
      "  0.0838 -0.0185  0.0753 -0.1444  0.1391\n",
      " -0.1965  0.0673 -0.0126 -0.1697 -0.1511\n",
      " -0.2074 -0.0567 -0.2049 -0.1722 -0.2050\n",
      " -0.0024 -0.1049 -0.1596  0.1453 -0.0869\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(3).经过一轮训练之后的卷积参数\n",
    "for epoch in range(1, 1 + 1):\n",
    "    train(epoch)\n",
    "    \n",
    "print( model.conv1.weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积weight分析总结:\n",
    "   (1). seed可以使得每次初始化的值固定,这样在做实验的时候可以对比,大家的初始化参数都是一样的;\n",
    "\n",
    "   (2). 在实际项目中一般不需要加seed;\n",
    "   \n",
    "   (3). 这里也可以再次看到conv的参数个数为 10x1x5x5 (10为输出的卷积个数, 1为上层feature map 层数, 5x5为卷积核大小);\n",
    "    \n",
    "   (4). 可以发现每次train之后,更新的参数是conv的参数, 这里我们可以这样理解: train的过程就是为了找到合适的卷积核参数, 使得对于输入的data, 可以有效的提取特征, 进而根据所提取的特征作进一步的工作, 比如分类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout2d (p=0.5)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#  dropout分析\n",
    "#  可见默认值为 p = 0.5 , 这里p表示随机失活神经元概率, 即0.5的神经元在单次传播过程中不进行梯度更新.\n",
    "\n",
    "print(model.conv2_drop)\n",
    "print(model.conv2_drop.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear (320 -> 50)\n",
      "320\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 线性函数,可以等价于全连接\n",
    "# 参数包括 weight, bias 即权重和偏置(权重系数以及常数项系数)\n",
    "\n",
    "print(model.fc1)\n",
    "print(model.fc1.in_features)    #输入的神经元个数\n",
    "print(model.fc1.out_features)    #输出的神经元个数\n",
    "\n",
    "#若想打印参数信息, 如下:\n",
    "# print(model.fc1.weight)\n",
    "# print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二, 本网络中 data 结构的相关分析 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchvision.datasets.mnist.MNIST object at 0x7fd0738ad8d0>\n",
      "60000\n",
      "(\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.0424\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1995  2.6051\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  2.3633\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5940\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1315\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  1.7523  2.3633\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  0.2758  1.7650  2.4524  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  1.3068  2.7960  2.7960  2.7960  2.2742\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.3860 -0.1951 -0.1951 -0.1951  1.1795  1.3068\n",
      "  0.0340  0.7722  1.5359  1.7396  2.7960  2.7960  2.7960  2.7960  2.7960\n",
      "  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7706\n",
      "  2.7960  2.7960  2.7960  2.7960  2.7960  2.0960  1.8923  2.7197  2.6433\n",
      "  1.5614  0.9377  2.7960  2.7960  2.1851 -0.2842 -0.4242  0.1231  1.5359\n",
      " -0.2460 -0.4115  1.5359  2.7960  0.7213 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242  1.3450  2.7960  1.9942 -0.3988 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.2842  1.9942  2.7960  0.4668 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242  0.0213  2.6433  2.4396  1.6123  0.9504 -0.4115\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  0.6068  2.6306  2.7960  2.7960  1.0904\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1486  1.9432  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.2206  0.7595  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.7451\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1613  1.2305  1.9051  2.7960\n",
      " -0.4242 -0.4242 -0.4242  0.0722  1.4596  2.4906  2.7960  2.7960  2.7960\n",
      " -0.4242 -0.1187  1.0268  2.3887  2.7960  2.7960  2.7960  2.7960  2.1342\n",
      "  0.4159  2.2869  2.7960  2.7960  2.7960  2.7960  2.0960  0.6068 -0.3988\n",
      "  2.7960  2.7960  2.7960  2.7960  2.0578  0.5940 -0.3097 -0.4242 -0.4242\n",
      "  2.7960  2.7960  2.6815  1.2686 -0.2842 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.2941  1.2559 -0.2206 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.8032 -0.0933  1.6887  2.8215  2.7197  1.1923 -0.4242 -0.4242 -0.4242\n",
      "  2.4396  1.7650  2.7960  2.6560  2.0578  0.3904 -0.4242 -0.4242 -0.4242\n",
      "  0.7595  0.6195  0.6195  0.2886  0.0722 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.1060 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.4850 -0.0806 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  1.9560 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.7451  0.3904 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.2105 -0.3988 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7578  1.8923 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.5686 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.FloatTensor of size 1x28x28]\n",
      ", 5)\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     3\n",
      "    0     0     0     0     0     0     0     0    30    36    94   154   170\n",
      "    0     0     0     0     0     0     0    49   238   253   253   253   253\n",
      "    0     0     0     0     0     0     0    18   219   253   253   253   253\n",
      "    0     0     0     0     0     0     0     0    80   156   107   253   253\n",
      "    0     0     0     0     0     0     0     0     0    14     1   154   253\n",
      "    0     0     0     0     0     0     0     0     0     0     0   139   253\n",
      "    0     0     0     0     0     0     0     0     0     0     0    11   190\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0    35\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0    39\n",
      "    0     0     0     0     0     0     0     0     0     0    24   114   221\n",
      "    0     0     0     0     0     0     0     0    23    66   213   253   253\n",
      "    0     0     0     0     0     0    18   171   219   253   253   253   253\n",
      "    0     0     0     0    55   172   226   253   253   253   253   244   133\n",
      "    0     0     0     0   136   253   253   253   212   135   132    16     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   18    18    18   126   136   175    26   166   255   247   127     0     0\n",
      "  253   253   253   253   253   225   172   253   242   195    64     0     0\n",
      "  253   253   253   253   251    93    82    82    56    39     0     0     0\n",
      "  253   198   182   247   241     0     0     0     0     0     0     0     0\n",
      "  205    11     0    43   154     0     0     0     0     0     0     0     0\n",
      "   90     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  190     2     0     0     0     0     0     0     0     0     0     0     0\n",
      "  253    70     0     0     0     0     0     0     0     0     0     0     0\n",
      "  241   225   160   108     1     0     0     0     0     0     0     0     0\n",
      "   81   240   253   253   119    25     0     0     0     0     0     0     0\n",
      "    0    45   186   253   253   150    27     0     0     0     0     0     0\n",
      "    0     0    16    93   252   253   187     0     0     0     0     0     0\n",
      "    0     0     0     0   249   253   249    64     0     0     0     0     0\n",
      "    0    46   130   183   253   253   207     2     0     0     0     0     0\n",
      "  148   229   253   253   253   250   182     0     0     0     0     0     0\n",
      "  253   253   253   253   201    78     0     0     0     0     0     0     0\n",
      "  253   253   198    81     2     0     0     0     0     0     0     0     0\n",
      "  195    80     9     0     0     0     0     0     0     0     0     0     0\n",
      "   11     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 27 \n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.ByteTensor of size 28x28]\n",
      "\n",
      "<bound method _type of \n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     3\n",
      "    0     0     0     0     0     0     0     0    30    36    94   154   170\n",
      "    0     0     0     0     0     0     0    49   238   253   253   253   253\n",
      "    0     0     0     0     0     0     0    18   219   253   253   253   253\n",
      "    0     0     0     0     0     0     0     0    80   156   107   253   253\n",
      "    0     0     0     0     0     0     0     0     0    14     1   154   253\n",
      "    0     0     0     0     0     0     0     0     0     0     0   139   253\n",
      "    0     0     0     0     0     0     0     0     0     0     0    11   190\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0    35\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0    39\n",
      "    0     0     0     0     0     0     0     0     0     0    24   114   221\n",
      "    0     0     0     0     0     0     0     0    23    66   213   253   253\n",
      "    0     0     0     0     0     0    18   171   219   253   253   253   253\n",
      "    0     0     0     0    55   172   226   253   253   253   253   244   133\n",
      "    0     0     0     0   136   253   253   253   212   135   132    16     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   18    18    18   126   136   175    26   166   255   247   127     0     0\n",
      "  253   253   253   253   253   225   172   253   242   195    64     0     0\n",
      "  253   253   253   253   251    93    82    82    56    39     0     0     0\n",
      "  253   198   182   247   241     0     0     0     0     0     0     0     0\n",
      "  205    11     0    43   154     0     0     0     0     0     0     0     0\n",
      "   90     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  190     2     0     0     0     0     0     0     0     0     0     0     0\n",
      "  253    70     0     0     0     0     0     0     0     0     0     0     0\n",
      "  241   225   160   108     1     0     0     0     0     0     0     0     0\n",
      "   81   240   253   253   119    25     0     0     0     0     0     0     0\n",
      "    0    45   186   253   253   150    27     0     0     0     0     0     0\n",
      "    0     0    16    93   252   253   187     0     0     0     0     0     0\n",
      "    0     0     0     0   249   253   249    64     0     0     0     0     0\n",
      "    0    46   130   183   253   253   207     2     0     0     0     0     0\n",
      "  148   229   253   253   253   250   182     0     0     0     0     0     0\n",
      "  253   253   253   253   201    78     0     0     0     0     0     0     0\n",
      "  253   253   198    81     2     0     0     0     0     0     0     0     0\n",
      "  195    80     9     0     0     0     0     0     0     0     0     0     0\n",
      "   11     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 27 \n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.ByteTensor of size 28x28]\n",
      ">\n",
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 对原始下载的train data 进行分析\n",
    "\n",
    "print(train_loader.dataset)    #输入网络的train data (经过了数据转换, 归一化之后的)\n",
    "print(len(train_loader.dataset))    # train data 图片张数\n",
    "print(train_loader.dataset[0])    # 该输出的为 转化后的第一张图片的数字结构\n",
    "print(train_loader.dataset.train_data[0])   #该输出的为 变回原始输入的第一张图片的数字结构\n",
    "print(train_loader.dataset.train_data[0].type) #该输出的为 变回原始输入的第一张图片的数据类型, 数据类型为torch.ByteTensor 张量\n",
    "print(train_loader.dataset.train_data[0].shape)  #图像大小为1x28x28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 可以看到对于输入到model中进行训练的数据有如下特点:\n",
    "\n",
    "(1).输入数据维度为(60000,1,28,28) 即60000张图片, 每张图片为单通道灰度图, 长和宽均为28\n",
    "\n",
    "(2).输入数据的数据类型为 归一化的float tensor (torch.FloatTensor )\n",
    "\n",
    "(3).可以通过 transforms 对数据类型进行转换\n",
    "\n",
    "(4).可以通过train_loader.dataset.train_data直接得到原始图像归一化前的张量数据结构\n",
    "\n",
    "#### 2. 我们知道这里数据类型是 Tensor 张量, 如果需要以图片形式输出可以有如下几种方式:\n",
    "    \n",
    "(1).使用 \" .numpy()\" 将 Tensor类型  转换为 numpy, 然后使用 opencv 或 matplotlib 作图;\n",
    "    \n",
    "(2).使用transforms.ToPILImage() 将 Tensor类型 转化为 PIL类型输出;\n",
    "    \n",
    "(3).使用visdom工具, 直接对 Tensor 做可视化 (尝试失败,不想研究,感兴趣可以自己研究下).\n",
    "\n",
    "(4).其他,例如使用TensorFlow的可视化工具\n",
    "\n",
    "#### 下面选取了第一种方式进行作图, 其他方式感兴趣可以自己尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHVCAYAAAAtoIVHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd0bbac30f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始图像可视化输出 : 使用 opencv 或 matplotlib 作图\n",
    "\n",
    "import numpy as np    #导入numpy\n",
    "img_tensor = train_loader.dataset.train_data[0] #导入tensor 数据\n",
    "img_np = img_tensor.numpy()  #将tensor 数据 转化为numpy\n",
    "\n",
    "# #下面几行代码使用opencv画图, 不建议在jupyter notebook内使用, 因为cv2.waitKey() 会导致后面程序无法正常运行  \n",
    "# import cv2    #导入opencv\n",
    "# cv2.imshow(\"image\", img_np)\n",
    "# cv2.waitKey()  \n",
    "\n",
    "#使用matplotlib 作图, 在jupyter notebook中推荐\n",
    "import  matplotlib.pyplot as plt    #导入matplotlib画图工具\n",
    "#plt图像嵌入jupyter内\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "# 图1: 输出原图\n",
    "ax1 = fig.add_subplot(221) \n",
    "ax1.imshow(img_np)  \n",
    "\n",
    "# 图2: 输出热力图\n",
    "ax2 = fig.add_subplot(222) \n",
    "ax2.imshow(img_np,  cmap=\"hot\" )  \n",
    "\n",
    "# 图3: 输出灰度图\n",
    "ax3 = fig.add_subplot(223) \n",
    "ax3.imshow(img_np, cmap=plt.cm.gray)  \n",
    "\n",
    "# 图4: 输出反向灰度图\n",
    "ax4 = fig.add_subplot(224) \n",
    "ax4.imshow(img_np, cmap=plt.cm.gray_r)  \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHVCAYAAAAU6/ZZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYVfWV7vF3URSjaACFIKJohJCoCbTllBiHOFzjNaKd\nqCFDk7R9iUk0aptEYw8x3Ulfk6hp5wRbhEyajkOku5PY4iWatEMYxAERBwIKIogjymBRte4fHLtL\nUmvX4Ux7n/p9P89TD1XnrX32qiOrarnrsI65uwAAAIAU9cm7AAAAACAvDMMAAABIFsMwAAAAksUw\nDAAAgGQxDAMAACBZDMMAAABIFsMwAAAAksUwDAAAgGQxDAMAACBZVQ3DZnacmS01s6fM7IJaFQWg\nPuhZoHnQr0BjWKUvx2xmLZKekHSMpJWS5kma4u6PRcf0s/4+QIMrOh/QG63Xy+vcfZdGnGt7e5Z+\nBd6uyP0q0bPAtsrt2b5VnONASU+5+zJJMrObJE2WFDbqAA3WQXZUFacEepc5fvOKBp5uu3qWfgXe\nrsj9KtGzwLbK7dlqniYxWtKzXT5eWboNQDHRs0DzoF+BBqnmynBZzGyapGmSNECD6n06AFWgX4Hm\nQs8C1avmyvAqSWO6fLxb6ba3cffp7t7m7m2t6l/F6QBUqceepV+BwuBnLNAg1QzD8ySNM7M9zayf\npE9Iml2bsgDUAT0LNA/6FWiQip8m4e5bzOxMSXdIapE0w90X16wyADVFzwLNg34FGqeq5wy7+68k\n/apGtQCoM3oWaB70K9AYvAIdAAAAksUwDAAAgGQxDAMAACBZDMMAAABIFsMwAAAAksUwDAAAgGQx\nDAMAACBZDMMAAABIFsMwAAAAksUwDAAAgGQxDAMAACBZDMMAAABIFsMwAAAAksUwDAAAgGQxDAMA\nACBZDMMAAABIFsMwAAAAksUwDAAAgGQxDAMAACBZDMMAAABIVt+8CwAAVG7Lh/cPs9Vf3BxmDx0y\nK8zef9/UMNv16n5h1jJ3YZgBQFFxZRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSLYRgAAADJqmqb\nhJktl7ReUoekLe7eVouiANQHPQs0D/oVaIxarFY70t3X1eB+sJ2sb/yfr2WXnetyzqVfGRtmHYM6\nw2yPd60Ns0FftDB7/rJ4jdPCtp+H2bqON8LsoF+cF2Z7//X9YdaL0LNNpPPwSZn5FTOuCrO9W+Pv\nEXG3Sg8eckOYLW3rCLOvjj04415RIfoVdfPGxw8Ks+9899ow+8dT/yLMfP6jVdWUB54mAQAAgGRV\nOwy7pDlmtsDMptWiIAB1Rc8CzYN+BRqg2qdJHOruq8xshKQ7zexxd7+n6yeUGniaJA3QoCpPB6BK\nmT1LvwKFws9YoAGqujLs7qtKf66VdJukA7v5nOnu3ububa3qX83pAFSpp56lX4Hi4Gcs0BgVD8Nm\nNtjMhrz1vqRjJTXfs6aBRNCzQPOgX4HGqeZpEiMl3WZmb93Pz9z9NzWpCkA90LNA86BfgQapeBh2\n92WS3l/DWppey3vGhZn3bw2z5w5/R5htPDheETZspzj73fvjtWN5+PWGIWH2nauOC7MH9vtZmP2x\nfWOYXbzmmDDb9XceZr0ZPVtc7cfG62O/ds2PM48d3xqvH+zMWKC2rL09zF7tjH/dPinjN/GbP3JA\nmA2c+0iYdW7aFN9popqhXzdO/pNnbfxPNrwlzIbNuK8e5aACa9viJwj84/KPNrCSfLFaDQAAAMli\nGAYAAECyGIYBAACQLIZhAAAAJIthGAAAAMliGAYAAECyqn055uR0HPFnYXbZzKvDLGv9UW/R7h1h\n9vdXfjbM+r4Rrzo75BdnhtmQVVvCrP+6eO3aoPkPhBlQjZYddwyzNw6bEGbnfj9eIXjkwNd7OGtl\n1zRmvvyBMLvrmkPC7L8uuiLM7vyXH4TZe38S9/Je57Nqqxk9d1j8d2/Qu16JD5xRh2IQ6xOvufPd\n45+VR414PMzusvj7RzPiyjAAAACSxTAMAACAZDEMAwAAIFkMwwAAAEgWwzAAAACSxTAMAACAZLFa\nbTv1X/pcmC3YNCbMxreuqUc5FTlv9cGZ+bLXdw6zme+6Ocxe7YxXpI284t6eC6uhuBKgflb+aHSY\nzTsgXr2Yh38YMS/MfrNDvDbpc8uPDbNZY+eE2Y7vfbG8wtA0vnnCL8LsO0vivydorJZ37RFmjx8e\n77mb+IdPh9mu8x6pqqai4cowAAAAksUwDAAAgGQxDAMAACBZDMMAAABIFsMwAAAAksUwDAAAgGSx\nWm07bVn9fJhd+Z1Twuzbx70RZi0P7xBmD33xyvIK28a31r0vzJ46elDmsR2vrA6zTx7yxTBb/uX4\nPvfUQ5nnBJrFlg/vH2Y3TrwqzPqoX0Xn+9yKozLz+XPeE2aPnB7XM3fjgDAbMX9jmD318oQwa/2n\nuWHWx8IITarVtuRdAsrQ9182VHTcxqd3rHElxcWVYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyG\nYQAAACSLYRgAAADJ6nG1mpnNkHSCpLXuvm/ptmGSfi5prKTlkk5195frV2ZzGHbDfWG2y78ND7OO\nF18Ks332/cswW3zYjDCbPf3wMBvxyr1h1hO7L16Rtmf85aOB6NnqdR4+KcyumBGvK9u7Nf6W2qnO\nMDvx8ZPDrOXj8VpGSXrH//Ywe++Pzwyz8Vc/G2Z9nn0wzIb+Lq6l/dsdYXbL++LvV395ZLyXsWXu\nwviEvUDR+7Xz0Ilh9qEBv29gJajU2MEvVnTcmDlxP/c25VwZninpuG1uu0DSXe4+TtJdpY8BFMNM\n0bNAs5gp+hXIVY/DsLvfI2nbS5eTJc0qvT9L0kk1rgtAhehZoHnQr0D+Kn3O8Eh3f+tlyp6XNLJG\n9QCoD3oWaB70K9BAVf8DOnd3SeGT1sxsmpnNN7P57dpc7ekAVCmrZ+lXoFj4GQvUX6XD8BozGyVJ\npT/XRp/o7tPdvc3d21rVv8LTAahSWT1LvwKFwM9YoIEqHYZnS5paen+qpNtrUw6AOqFngeZBvwIN\nVM5qtRslHSFpZzNbKekbki6W9K9mdrqkFZJOrWeRvUHHuspWm7S/1q+i4/b51GNh9sK1LdkHd6az\nTqU3omfLY/vvE2br/npjmI1vjXtyQcZvqf/f6+8NsxdvGhNmw1/O3lm400/uj7OM47Zk3mvtjWyJ\nr1q+eM6GMBsxtx7VFEfR+3XFCQPDbETLoAZWgix9x+4eZh8fNrui+xz4x3ibX2+bEnocht19ShAd\nVeNaANQAPQs0D/oVyB+vQAcAAIBkMQwDAAAgWQzDAAAASBbDMAAAAJLFMAwAAIBk9bhNAvl6z/lP\nhNnn9ov/sfENe9wVZoef8qXMcw75ebyqCWgmfQbFq5+2fPe1MLt/wq1h9sctb4bZX194XpgN/d0z\nYTZicPiaCr1uhVF3Dhy1IsyWN64MdKPv3usrOm7T4++ocSXI8uw/Dw6zD/bvDLPrX9stvtNX4u+R\nvQ1XhgEAAJAshmEAAAAki2EYAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki9VqBdfxyqth9uIX3hNm\nz8zeGGYXfOtHmef8+qknh5k/uFOYjfn2ffGdumeeE6iHjYfvE2Z3TLimovv8q7PPDbMhv4zXEm6p\n6GxAcxoxP17nlbqWnYeH2ZqPjQ+zYaeuDLO7x1+fccYBYXLt1SeF2Yg192bcZ+/ClWEAAAAki2EY\nAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAAyWK1WhPrfGhJmH3im18Ns59+45LM+110cMbq\ntYPjaJ/BZ4bZuOtWh9mWZcsz6wEq9b5/XBRmfTKuBXxuxVFhNvCXf6iqpt6s1VrCrD1ju2KLsXqx\nt9k4LO6vwXU4X+eHJoWZt1iYPXt0/zB7c9f2MOvTryPM/vNDV4aZJLXG5ej5jriev1sWrz19qTNe\nZTeoT1zryAfWh1lKXcmVYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSLYRgAAADJ6nEY\nNrMZZrbWzB7tcttFZrbKzBaV3o6vb5kAykXPAs2DfgXyV86e4ZmSrpK07fLZ77t79sJa5GbYjPvC\n7MylX8o8dseLV4bZjXvdEWaL/+KqMJsw5q/C7N3fjP+frOPJZWGG0Ewl1LOvfOaQMPvbkfGX26l+\nYbbgP98bZrvr3vIKS1C7x/tMOxXvQf3NkvjxHqeFVdXUBGaqwP26eVNrmHVmbKK94cLvh9nsMydW\nVVN3zh/+L2HWR/Fi343+Zpg91xH/fb7qhSPC7Og554SZJL3jwfh7z6j/XBNmtiL+2fzCkoFhNrIl\n3pfs8x4Js5T0eGXY3e+R9FIDagFQA/Qs0DzoVyB/1Txn+Cwze7j0K56hNasIQL3Qs0DzoF+BBql0\nGL5W0l6SJkpaLenS6BPNbJqZzTez+e3aXOHpAFSprJ6lX4FC4Gcs0EAVDcPuvsbdO9y9U9J1kg7M\n+Nzp7t7m7m2til9zG0D9lNuz9CuQP37GAo1V0TBsZqO6fHiypEejzwWQP3oWaB70K9BYPW6TMLMb\nJR0haWczWynpG5KOMLOJklzSckmfr2ONALYDPQs0D/oVyF+Pw7C7T+nm5uvrUAsaxP5rUWa+4eMj\nwuyA084KswfOvzzMHj8yXnvzqbHHhtmrh4YRAqn17JZ4o5B26hOvMLpvU/wr5b1+9Fx8vrKqam59\nBg0Ks8cv2TfjyAVh8qllHwmzCWf/Mczi5Va9Q9H7de9PPxhm+/zfM8NszAGr6lFOaO7a8WH2wq93\nC7Phi+O1Y/1+My/jjPFx4zU/47hsWX/fV53/gTA7oH+8TvWm10dXXE8qeAU6AAAAJIthGAAAAMli\nGAYAAECyGIYBAACQLIZhAAAAJIthGAAAAMnqcbUa0tOxZm2YjbwizjZ9LV46NcjiFVfXjf33MDvh\n5HPi+7ztgTADevJixw5htmXZ8sYVkpOs9WlLL94vzB6ffFWY/XrDTmH23NV7h9mQl+8PMxTXnl+P\n13kVySg9k3cJNTHosBcqOu5v534szMbrD5WW06twZRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSL\nYRgAAADJYhgGAABAslitlqDOQydm5k+fMiDM9p24PMyy1qdlufKlSfF93j6/ovsEevKV/zolzMZr\nQQMrqZ/Ow+PeWvvXG8NsSVu8Pu2oR04Ls8HHLQuzIWJ9GpCHPW73vEsoPK4MAwAAIFkMwwAAAEgW\nwzAAAACSxTAMAACAZDEMAwAAIFkMwwAAAEgWq9WamLXtG2ZPfDlec3bdB2dl3u9hA96suKbIZm8P\ns/tf2jM+sHN1zWtBL2Nx1Cfj//cvP/TGMLta46upqKFW/MMhYXbLX1wWZuNb4+8Rf/aHqWG268mP\nlVcYADQJrgwDAAAgWQzDAAAASBbDMAAAAJLFMAwAAIBkMQwDAAAgWT0Ow2Y2xszmmtljZrbYzM4u\n3T7MzO40sydLfw6tf7kAstCvQHOhZ4H8lbNabYuk89x9oZkNkbTAzO6U9FlJd7n7xWZ2gaQLJJ1f\nv1J7r7577hFmT39u1zC76LSbwuxjO6yrqqZKXLimLczuvvzgMBs66756lJOq9PrV46hTnWF2+MAX\nw+ycmfuH2btuiO+z9fn1Ybbm8F3CbNhpK8PsrN3vCjNJ+sigBWE2+42RYfYXjxwXZjv/cHDmOVFT\n6fUsaq7F4mubL49vDbN3/roe1TSfHq8Mu/tqd19Yen+9pCWSRkuaLOmthbWzJJ1UryIBlId+BZoL\nPQvkb7ueM2xmYyVNkvSApJHu/tYrIjwvKb4EAaDh6FegudCzQD7KHobNbAdJt0g6x91f65q5uyv4\nZaWZTTOz+WY2v12bqyoWQHnoV6C50LNAfsoahs2sVVub9Kfufmvp5jVmNqqUj5K0trtj3X26u7e5\ne1ur+teiZgAZ6FegudCzQL7K2SZhkq6XtMTdu77Q/WxJb72A/VRJt9e+PADbg34Fmgs9C+SvnG0S\nH5T0GUmPmNmi0m0XSrpY0r+a2emSVkg6tT4lAtgO9CvQXOhZIGc9DsPu/ntJFsRH1bac5tZ37O5h\n9ur+o8LstH/4TZid8Y5bw6xezlsdr0G775p4fdqwmX8Is6GdrE9rBPq1fAMs/va35JgfhNnvPzQg\nzJ7c/M4w+9xOy8uqa3ud/dyHwuw3904Ms3Fn31+PcrCd6FnUQofHKx95ebWe8RABAAAgWQzDAAAA\nSBbDMAAAAJLFMAwAAIBkMQwDAAAgWQzDAAAASFY5e4aT03dUvB7ppRmDw+wLe94dZlOGrKmqpu11\n5qpDw2zhtfG6JUna+eZHw2zYelakoVhG/rbbF+aSJJ3/+UPC7DvvrOzv8mED3gyzQwcsr+g+H9wc\nX5eYcve0zGPHf25BmI0T69OA1G04YEPeJRQeV4YBAACQLIZhAAAAJIthGAAAAMliGAYAAECyGIYB\nAACQLIZhAAAAJKtXr1Z783+1xdm5L4XZhXv/KsyOHfhGVTVtrzUdG8PssNnnhdmEv308zIa9kr1S\nqrPnsoDC6Hji6TB78pSxYfbes84Ks8dOvbKakro14VdfDLN3XxOvPhr/YLw6DQAkqcW4tlkNHj0A\nAAAki2EYAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAAyerVq9WWnxTP+k/s94uan+/qV94V\nZpfffWyYWYeF2YRv/THMxq15IMw6wgRIx5Zly8Ns73Pj7MRzD6h5LeM1L8y85mcD0NtsnrNLmHVM\nZClqNbgyDAAAgGQxDAMAACBZDMMAAABIFsMwAAAAksUwDAAAgGT1OAyb2Rgzm2tmj5nZYjM7u3T7\nRWa2yswWld6Or3+5ALLQr0BzoWeB/Jl79lIfMxslaZS7LzSzIZIWSDpJ0qmSXnf3S8o92Y42zA+y\no6qpF+hV5vjNC9y9rVb3R78C9VPrfpXoWaCeyu3ZHvcMu/tqSatL7683syWSRldfIoBao1+B5kLP\nAvnbrucMm9lYSZMkvfVqD2eZ2cNmNsPMhta4NgBVoF+B5kLPAvkoexg2sx0k3SLpHHd/TdK1kvaS\nNFFb/6/20uC4aWY238zmt2tzDUoG0BP6FWgu9CyQn7KGYTNr1dYm/am73ypJ7r7G3TvcvVPSdZIO\n7O5Yd5/u7m3u3taq/rWqG0CAfgWaCz0L5KucbRIm6XpJS9z9si63j+ryaSdLerT25QHYHvQr0Fzo\nWSB/Pf4DOkkflPQZSY+Y2aLSbRdKmmJmEyW5pOWSPl+XCgFsD/oVaC70LJCzcrZJ/F6SdRP9qvbl\nAKgG/Qo0F3oWyB+vQAcAAIBkMQwDAAAgWQzDAAAASBbDMAAAAJLFMAwAAIBkMQwDAAAgWQzDAAAA\nSBbDMAAAAJLFMAwAAIBkMQwDAAAgWQzDAAAASBbDMAAAAJLFMAwAAIBkmbs37mRmL0haUfpwZ0nr\nGnbynhWpHmrpXm+sZQ9336UG91Nz2/Sr1Dsf/1qglu4VqRapNvUUtl+lQv+MpZZYkerpjbWU1bMN\nHYbfdmKz+e7elsvJu1Gkeqile9SSryJ9zdTSPWqJFa2eeivS10stsSLVk3ItPE0CAAAAyWIYBgAA\nQLLyHIan53ju7hSpHmrpHrXkq0hfM7V0j1piRaun3or09VJLrEj1JFtLbs8ZBgAAAPLG0yQAAACQ\nrFyGYTM7zsyWmtlTZnZBHjV0qWW5mT1iZovMbH4O559hZmvN7NEutw0zszvN7MnSn0NzrOUiM1tV\nenwWmdnxDapljJnNNbPHzGyxmZ1dur3hj01GLbk8No1WpH4t1ZNbz9KvYS30a4EUqWfp18xa6NeC\n9GvDnyZhZi2SnpB0jKSVkuZJmuLujzW0kP+pZ7mkNnfPZbeemR0m6XVJP3L3fUu3fVfSS+5+cekb\n2VB3Pz+nWi6S9Lq7X1Lv829TyyhJo9x9oZkNkbRA0kmSPqsGPzYZtZyqHB6bRipav5ZqWq6cepZ+\nDWuhXwuiaD1Lv2bWcpHo10L0ax5Xhg+U9JS7L3P3NyXdJGlyDnUUgrvfI+mlbW6eLGlW6f1Z2voX\nI69acuHuq919Yen99ZKWSBqtHB6bjFpSQL92Qb92j34tFHq2hH7tHv36p/IYhkdLerbLxyuV7zcq\nlzTHzBaY2bQc6+hqpLuvLr3/vKSReRYj6Swze7j0a56G/EqpKzMbK2mSpAeU82OzTS1Szo9NAxSt\nX6Xi9Sz92gX9mrui9Sz9mo1+7b4WqYGPDf+ATjrU3SdK+oikL5V+lVEYvvV5LHmu/LhW0l6SJkpa\nLenSRp7czHaQdIukc9z9ta5Zox+bbmrJ9bFJWGF7ln6lX/En6NcY/RrX0tDHJo9heJWkMV0+3q10\nWy7cfVXpz7WSbtPWXzHlbU3peTRvPZ9mbV6FuPsad+9w905J16mBj4+ZtWprc/zU3W8t3ZzLY9Nd\nLXk+Ng1UqH6VCtmz9Kvo1wIpVM/SrzH6Na6l0Y9NHsPwPEnjzGxPM+sn6ROSZudQh8xscOkJ2zKz\nwZKOlfRo9lENMVvS1NL7UyXdnlchbzVGyclq0ONjZibpeklL3P2yLlHDH5uolrwemwYrTL9Khe1Z\n+pV+LZLC9Cz9mo1+LVC/unvD3yQdr63/2vVpSX+TRw2lOvaS9FDpbXEetUi6UVt/BdCurc/tOl3S\ncEl3SXpS0hxJw3Ks5ceSHpH0sLY2yqgG1XKotv6K5mFJi0pvx+fx2GTUkstjk8Pf0UL0a6mWXHuW\nfg1roV8L9FaUnqVfe6yFfi1Iv/IKdAAAAEgW/4AOAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EY\nAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAA\nyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAAyWIY\nBgAAQLIYhgEAAJCsqoZhMzvOzJaa2VNmdkGtigJQH/Qs0DzoV6AxzN0rO9CsRdITko6RtFLSPElT\n3P2x6Jh+1t8HaHBF5wN6o/V6eZ2779KIc21vz9KvwNsVuV8lehbYVrk927eKcxwo6Sl3XyZJZnaT\npMmSwkYdoME6yI6q4pRA7zLHb17RwNNtV8/Sr8DbFblfJXoW2Fa5PVvN0yRGS3q2y8crS7cBKCZ6\nFmge9CvQINVcGS6LmU2TNE2SBmhQvU8HoAr0K9Bc6FmgetVcGV4laUyXj3cr3fY27j7d3dvcva1V\n/as4HYAq9diz9CtQGPyMBRqkmmF4nqRxZranmfWT9AlJs2tTFoA6oGeB5kG/Ag1S8dMk3H2LmZ0p\n6Q5JLZJmuPvimlUGoKboWaB50K9A41T1nGF3/5WkX9WoFgB1Rs8CzYN+BRqDV6ADAABAshiGAQAA\nkCyGYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyG\nYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAA\nACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSr\nbzUHm9lySesldUja4u5ttSgKvc/T3zskzJZ88qowa7WWMDvsi9PCbOAv/1BeYYmhZ4HmQb/2Ti3D\nh4WZ7bRjmD3zsV3DbNPOHmZ7f/OhMOvcsCHMUlLVMFxypLuvq8H9AGgMehZoHvQrUGc8TQIAAADJ\nqnYYdklzzGyBmcW/swZQFPQs0DzoV6ABqn2axKHuvsrMRki608wed/d7un5CqYGnSdIADarydACq\nlNmz9CtQKPyMBRqgqivD7r6q9OdaSbdJOrCbz5nu7m3u3taq/tWcDkCVeupZ+hUoDn7GAo1R8TBs\nZoPNbMhb70s6VtKjtSoMQG3Rs0DzoF+BxqnmaRIjJd1mZm/dz8/c/Tc1qQpN6flzPxBmvz3tu2HW\n7v0qO2G8SQbdo2eB5kG/FliffSeE2ZNfH5h57F/ud2+YnTf8joprirxn5BlhNu6zC2p+vmZU8TDs\n7sskvb+GtQCoI3oWaB70K9A4rFYDAABAshiGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAsqp9BTrg\nv70+pjPMhvWpcH0a0Eu8+b/awmzFp+Le+cKf3Z15v+cMfaKievb7l7PCbNDqeG/hKx/YHGZ7/DS+\nvtLvjvnlFQY0kB2wX5g9dW5LmP320KvCbJeW7Bc/6ZNxHfI/NgwNs2WbR4TZl4YuDbMfH3ZdmP3j\nAVPDzOc9Ema9DVeGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSL1WrYLq+f\nclCY3XLy5RlHWpj84JUJYTbn1Hgd1eAVi8MsXlQF1M8LZxwSZld+7eowa+vfEWZZa5gkaeryo8Ns\n0k7PhNlDf5XVr7Gsej4wbEqYDbujotMBZWnZZZcwe+Ly0WH2bx+4Jsz2am3NOGP2+rQsN7w2Jsx+\n+bFDw6yzf1zPl/49Xq2W9f1l48iBYTYgTHofrgwDAAAgWQzDAAAASBbDMAAAAJLFMAwAAIBkMQwD\nAAAgWQzDAAAASBar1fAnNp1wYJh94//OCLPxrfH6tCyzrjsuzN752L0V3SdQDWvtF2abjn5/mN3y\n9e+F2a5941VMp684JsxWXPLuMJOkwf+xKMzmDto9zO6+bXyY3TJuduY5I68tGh5mwyq6R6A8qz49\nLswWH561RjBrfVplfpKxOk2SfnnSB8KsY+kTYWaT9qm4JmTjyjAAAACSxTAMAACAZDEMAwAAIFkM\nwwAAAEgWwzAAAACSxTAMAACAZPW4Ws3MZkg6QdJad9+3dNswST+XNFbSckmnuvvL9SsTjbT605vC\n7MiBcSa1hMnU5UeH2TsvZ31aLdGz1Vt9ZluY/eErWWua4vVppzz10TDb8rH2MBu07oGM80mekT03\nbf8we2CtCZ0HAAASRUlEQVRc1tcR+/WGIWG29w+fDbMtFZ2t96Nfa2P0ictrfp83v/7OMLvsiaPC\nbOTXsrpS6lj6ZEX1vLzfjhUdh56Vc2V4pqRtF8FeIOkudx8n6a7SxwCKYaboWaBZzBT9CuSqx2HY\n3e+R9NI2N0+WNKv0/ixJJ9W4LgAVomeB5kG/Avmr9DnDI919den95yWNrFE9AOqDngWaB/0KNFDV\n/4DO3V0ZT1szs2lmNt/M5rdrc7WnA1ClrJ6lX4Fi4WcsUH+VDsNrzGyUJJX+XBt9ortPd/c2d29r\nzfjHJQDqqqyepV+BQuBnLNBAlQ7DsyVNLb0/VdLttSkHQJ3Qs0DzoF+BBipntdqNko6QtLOZrZT0\nDUkXS/pXMztd0gpJp9azSNRW391GZ+aLP3RDmLV7R5gtibdD6ZnLxofZYGWvjsL2oWfL8+SVB4XZ\n0j+/Msw6M+7zPXeeEWYTvrI8zDrWvZhxr5U74wu1n6G+9e2pYTb02ftqfr7ejn6tkf8TXxV/75fO\nCrMxd8Y/0wYvfj7Mdl7xRJjF91idDSOtTveMHodhd58SRPGSPQC5oWeB5kG/AvnjFegAAACQLIZh\nAAAAJIthGAAAAMliGAYAAECyGIYBAACQrB63SaA5tezz7jBr+9mjdTnnabd+Oczedcv9dTknkOXp\nSw8Os6V/fnWYvdq5KcxOefyTYfbuszLWLa1fH2ZZ+gwenJm/+PH3hdnkHb4X368GhtmEX3wpzPae\nyfo0FE/HU38Ms73PjbMsWyotpk7aD6jsewh6xpVhAAAAJIthGAAAAMliGAYAAECyGIYBAACQLIZh\nAAAAJIthGAAAAMlitVovteLE4WF28/AHezi6JUw++fRHw2z8xU+HWUcPZwQq1TJyRJjNOvmaMOtU\nZ5hlrU/rd8yKjPusTJ+J7w2zfWcsyTz2WyOvyEj7h8kHF30izN59UXxOehmQnvn7D4TZlkGefbBl\nZBmH/vm4ytYanrnyiDAb+JuFlZTS63BlGAAAAMliGAYAAECyGIYBAACQLIZhAAAAJIthGAAAAMli\nGAYAAECyWK3WxF763CFhdtsZ38s4sjXzfs949vAwa58ar2rqeOGZzPsF6sEGxH8n2/pXtghs4Jf7\nxefbY0yYPXnGbmF27NHxCqNzR0wPs937DgwzKXudW4fHy5Hs5zvHx73yZOY5gWbSsuOOYbbpwHFh\n1vr1NWH28IQrK66n1eL1pe1e2fesuRsHhdnKabuHmW/JXt2YCq4MAwAAIFkMwwAAAEgWwzAAAACS\nxTAMAACAZDEMAwAAIFkMwwAAAEhWj8Owmc0ws7Vm9miX2y4ys1Vmtqj0dnx9ywRQLnoWaB70K5C/\ncvYMz5R0laQfbXP79939kppXhLdp2efdYXbvt67KOHJAxee8b+XYMBuz/NEwQ2HMVEI965s2h9kD\nm+Od2gf1bw+z2+fcFGadmZt9KzNnY7zz98n2eFewJB058PUwm/9mvC/5HT+6r+fC0AgzlVC/VsP6\nxzvF3zx8vzA795ofh9mRA+8KszUd8feWuRuHhtnfPzE5zCTpxn1mhtmufeOvMcuAPvH3s2WnviPM\n9loazwqdmzZVVEsz6vHKsLvfI+mlBtQCoAboWaB50K9A/qp5zvBZZvZw6Vc88f8iASgKehZoHvQr\n0CCVDsPXStpL0kRJqyVdGn2imU0zs/lmNr9d8a8cANRVWT1LvwKFwM9YoIEqGobdfY27d7h7p6Tr\nJB2Y8bnT3b3N3dtaVdlzYQBUp9yepV+B/PEzFmisioZhMxvV5cOTJfGvqoACo2eB5kG/Ao3V4zYJ\nM7tR0hGSdjazlZK+IekIM5soySUtl/T5OtYIYDvQs0DzoF+B/PU4DLv7lG5uvr4OtaAbT1w4KMza\nvaMu59z94jjLXvKEIkitZzvWrA2zb3zhr8Lskh9cE2bvizeS6SevjQmzb919YpiNnxmvKeq75tUw\nG3Fj9qKBI8f8vzCbOjf++sdrfub9ojFS69ee9BkQr/p68bRJYfa7f7qiovPtc+NZYbbb3PhnbP//\nmBdmw0fF6w4l6cY79g+z84ZX9kuArFWRD382fmwOefbLYTbyRw+FWeeGDeUV1iR4BToAAAAki2EY\nAAAAyWIYBgAAQLIYhgEAAJAshmEAAAAki2EYAAAAyepxtRrqr/PweF3Mt9p+WfPzHfPoJzLzHeaz\n3x29Q7874vVhF+4ZvqhXxcbrDxUdt35yXMt/7H575rHtHl/TGLg8Y0cckBPrH79S3uOXvS/OJle2\nPm3y0pPCbPz3loVZ1trGvmN2C7P3z34ms56vDn8szF7tfDPMDrrlvDAbNSGu9a79fh5m9/1d/Jie\nNuWEMFt3xX5hNuDFeM1blpbfLqzouFrgyjAAAACSxTAMAACAZDEMAwAAIFkMwwAAAEgWwzAAAACS\nxTAMAACAZLFarQC+PXN6mO3b6hXd51dWHxZmO015OfPYjorOCKBSWwbG1yXaPbsjO9UZZnvOjFc8\nbem5LKBi1jceL5b+8/vD7PETrw6zlVs2h9mJP/xamI2d8XSYbclYn9Z+9P5htu93Hgyzb4xYEGaS\ndMNre4TZj//mo2G29633h1nLzsPD7IhjzgqzN057Ncxum3RdmO12RbweL8u/vxHXOX38XhXdZy1w\nZRgAAADJYhgGAABAshiGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAslitVgCT+lW+Vily3w1/FmYj\nXr63ovsEUB9DbopXJunSxtUB1MqzXz0wzB4/8fIwey5jfdopF381zMb+clmYvfThPcPMPz0kzG7e\nN65zl5Z4tdg+N8WrzCRp/PR1YTZo6QOZx0Y61r0YZjvemJXF9/nxL8br6kZ+fEVZdf2J896RES6u\n7D5rgCvDAAAASBbDMAAAAJLFMAwAAIBkMQwDAAAgWQzDAAAASFaPw7CZjTGzuWb2mJktNrOzS7cP\nM7M7zezJ0p9D618ugCz0K9Bc6Fkgf+WsVtsi6Tx3X2hmQyQtMLM7JX1W0l3ufrGZXSDpAknn16/U\n5vbszfuGWastqvn5Rv02Xt1S2bI2NAn6tQmt/8TBGemChtWBXPTKnr32/1xT0XEDLM4+esY9YTb6\nyy+H2dQd/62iWqSM9Wk/+3KY7f31eZn32rFlS4X1NNaIa+I1rF7Zf15Jqyo9sK56vDLs7qvdfWHp\n/fWSlkgaLWmypFmlT5sl6aR6FQmgPPQr0FzoWSB/2/WcYTMbK2mSpAckjXT31aXoeUkja1oZgKrQ\nr0BzoWeBfJQ9DJvZDpJukXSOu7/WNXN3l+TBcdPMbL6ZzW9X/MoyAGqHfgWaCz0L5KesYdjMWrW1\nSX/q7reWbl5jZqNK+ShJa7s71t2nu3ubu7e1Zjz/BkBt0K9Ac6FngXyVs03CJF0vaYm7X9Ylmi1p\naun9qZJur315ALYH/Qo0F3oWyF852yQ+KOkzkh4x+++1BxdKuljSv5rZ6ZJWSDq1PiUC2A70K9Bc\n6FkgZz0Ow+7+e0nRspOjaltOc+s8fFKY/fPEn4RZu8fLzl7t3BRmB/z6nDCbsOKxMEPvRb82p1f3\n4vWPUtVbe/ae1yeE2UH9HwmzYS3xUz0u3LmyNaQnPP7nYfbMfbuF2V43vxpmey+OVx56k6xOw//g\nOzAAAACSxTAMAACAZDEMAwAAIFkMwwAAAEgWwzAAAACSxTAMAACAZJWzZxhl2jSsX5gdOuCNjCNb\nwuSODbuH2fhp88KsM+NsAIpl9N0bwqz1zPj7gyS1d/sivUC+7j1y1zA76FMfDrNX3/9mmPV9oTXM\nxv9gVXzc892+eJ8kaeymZ8OMn6Pp4MowAAAAksUwDAAAgGQxDAMAACBZDMMAAABIFsMwAAAAksUw\nDAAAgGSxWg0Acmb/tSjMZr42IvPYKUPilVIb9hkVZv2eXdlzYUCFOl58KcxGXnFvnFV4vi0VHgdI\nXBkGAABAwhiGAQAAkCyGYQAAACSLYRgAAADJYhgGAABAshiGAQAAkCxWq9XQjoueD7OzVn44zH4w\n5u56lAOgF/j+Dz+emU/5yuVhNurvngqzF195X3yn9z/cY10A0FtwZRgAAADJYhgGAABAshiGAQAA\nkCyGYQAAACSLYRgAAADJ6nEYNrMxZjbXzB4zs8Vmdnbp9ovMbJWZLSq9HV//cgFkoV+B5kLPAvkr\nZ7XaFknnuftCMxsiaYGZ3VnKvu/ul9SvvOay5Y8rwmzlwfFxJ2j/OlSDRNGvvczoHy/NzE876YQw\n+/ne/x5mh//9lDAb9smdwqzjlVcz68F2o2eBnPU4DLv7akmrS++vN7MlkkbXuzAA249+BZoLPQvk\nb7ueM2xmYyVNkvRA6aazzOxhM5thZkNrXBuAKtCvQHOhZ4F8lD0Mm9kOkm6RdI67vybpWkl7SZqo\nrf9Xe2lw3DQzm29m89u1uQYlA+gJ/Qo0F3oWyE9Zw7CZtWprk/7U3W+VJHdf4+4d7t4p6TpJB3Z3\nrLtPd/c2d29rVf9a1Q0gQL8CzYWeBfJVzjYJk3S9pCXuflmX20d1+bSTJT1a+/IAbA/6FWgu9CyQ\nv3K2SXxQ0mckPWJmi0q3XShpiplNlOSSlkv6fF0qBLA96FegudCzQM7K2Sbxe0nWTfSr2pcDoBr0\na+/Tse7FzPzNjw0Ps/dcGs9PS47+YZidOOH0+IT3P5xZD7YPPQvkj1egAwAAQLIYhgEAAJAshmEA\nAAAki2EYAAAAyWIYBgAAQLIYhgEAAJCscvYMAwAKKmv12ripcXaiDsi4V9anAUgHV4YBAACQLIZh\nAAAAJIthGAAAAMliGAYAAECyGIYBAACQLIZhAAAAJMvcvXEnM3tB0orShztLWtewk/esSPVQS/d6\nYy17uPsuNbifmtumX6Xe+fjXArV0r0i1SLWpp7D9KhX6Zyy1xIpUT2+spayebegw/LYTm81397Zc\nTt6NItVDLd2jlnwV6Wumlu5RS6xo9dRbkb5eaokVqZ6Ua+FpEgAAAEgWwzAAAACSlecwPD3Hc3en\nSPVQS/eoJV9F+pqppXvUEitaPfVWpK+XWmJFqifZWnJ7zjAAAACQN54mAQAAgGTlMgyb2XFmttTM\nnjKzC/KooUsty83sETNbZGbzczj/DDNba2aPdrltmJndaWZPlv4cmmMtF5nZqtLjs8jMjm9QLWPM\nbK6ZPWZmi83s7NLtDX9sMmrJ5bFptCL1a6me3HqWfg1roV8LpEg9S79m1kK/FqRfG/40CTNrkfSE\npGMkrZQ0T9IUd3+soYX8Tz3LJbW5ey679czsMEmvS/qRu+9buu27kl5y94tL38iGuvv5OdVykaTX\n3f2Sep9/m1pGSRrl7gvNbIikBZJOkvRZNfixyajlVOXw2DRS0fq1VNNy5dSz9GtYC/1aEEXrWfo1\ns5aLRL8Wol/zuDJ8oKSn3H2Zu78p6SZJk3OooxDc/R5JL21z82RJs0rvz9LWvxh51ZILd1/t7gtL\n76+XtETSaOXw2GTUkgL6tQv6tXv0a6HQsyX0a/fo1z+VxzA8WtKzXT5eqXy/UbmkOWa2wMym5VhH\nVyPdfXXp/ecljcyzGElnmdnDpV/zNORXSl2Z2VhJkyQ9oJwfm21qkXJ+bBqgaP0qFa9n6dcu6Nfc\nFa1n6dds9Gv3tUgNfGz4B3TSoe4+UdJHJH2p9KuMwvCtz2PJc+XHtZL2kjRR0mpJlzby5Ga2g6Rb\nJJ3j7q91zRr92HRTS66PTcIK27P0K/2KP0G/xujXuJaGPjZ5DMOrJI3p8vFupdty4e6rSn+ulXSb\ntv6KKW9rSs+jeev5NGvzKsTd17h7h7t3SrpODXx8zKxVW5vjp+5+a+nmXB6b7mrJ87FpoEL1q1TI\nnqVfRb8WSKF6ln6N0a9xLY1+bPIYhudJGmdme5pZP0mfkDQ7hzpkZoNLT9iWmQ2WdKykR7OPaojZ\nkqaW3p8q6fa8CnmrMUpOVoMeHzMzSddLWuLul3WJGv7YRLXk9dg0WGH6VSpsz9Kv9GuRFKZn6dds\n9GuB+tXdG/4m6Xht/deuT0v6mzxqKNWxl6SHSm+L86hF0o3a+iuAdm19btfpkoZLukvSk5LmSBqW\nYy0/lvSIpIe1tVFGNaiWQ7X1VzQPS1pUejs+j8cmo5ZcHpsc/o4Wol9LteTas/RrWAv9WqC3ovQs\n/dpjLfRrQfqVV6ADAABAsvgHdAAAAEgWwzAAAACSxTAMAACAZDEMAwAAIFkMwwAAAEgWwzAAAACS\nxTAMAACAZDEMAwAAIFn/H1XsRkiWN5f4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd068a0e518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#输出 前6张图\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "num =231\n",
    "for i in range(6):\n",
    "    img_tensor = train_loader.dataset.train_data[i] #导入tensor 数据\n",
    "    img_np = img_tensor.numpy()  #将tensor 数据 转化为numpy\n",
    "    ax1 = fig.add_subplot(num) \n",
    "    ax1.imshow(img_np)  \n",
    "    num += 1\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面, 我们展示一下网络的详细训练过程\n",
    "\n",
    "#### 在此, 我将展示这个网络每一步前向传播中feature map发生的变化, 以及反向传播的weight发生的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化 model\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "#选择优化方式, 这里使用的是SGD随机梯度下降, 当然也可以选择别的\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "#导入 train数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个batch的图片总个数:\n",
      "64\n",
      "这个batch图片的类别信息:\n",
      "[5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "#第一个batch的一次前向传播以及反向传播\n",
    "\n",
    "#以比较愚蠢的方式导入一个batch的数据, 这里一个batch包含64张图\n",
    "model.train()\n",
    "for batch_idx, (data_i, target_i) in enumerate(train_loader):\n",
    "    if batch_idx !=0:\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "print (\"一个batch的图片总个数:\" )\n",
    "print (len(data))\n",
    "print (\"这个batch图片的类别信息:\" )\n",
    "print ([j for j in target])\n",
    "\n",
    "#转换数据类型\n",
    "data, target = Variable(data), Variable(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体流程\n",
    "在此, 我将展示这个前向传播以及反向传播过程中, 参数是如何变化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向传播前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0740  0.0984  0.0291  0.0744 -0.0649\n",
      "  0.0625 -0.1482  0.0495  0.1548  0.0458\n",
      " -0.1725 -0.0281 -0.1418  0.1696 -0.0733\n",
      " -0.1665  0.1341 -0.0754 -0.1999 -0.1256\n",
      "  0.1634 -0.1857  0.1749  0.1064 -0.0227\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.1455 -0.1978 -0.0071 -0.1224 -0.1842\n",
      "  0.1258  0.0140 -0.1828 -0.1968  0.1506\n",
      "  0.1372  0.1532 -0.0065 -0.1868 -0.1032\n",
      " -0.0818  0.1343  0.0123 -0.0034 -0.0858\n",
      "  0.0863 -0.0504 -0.1059  0.1758 -0.0583\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1138  0.1978 -0.1051 -0.1497 -0.0010\n",
      "  0.1934 -0.1352 -0.0477  0.1994  0.0043\n",
      "  0.0989  0.1776 -0.0782 -0.0267 -0.0714\n",
      " -0.0152  0.1020 -0.0377  0.1476  0.1843\n",
      " -0.0009  0.0749  0.1425 -0.1291 -0.1313\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1619 -0.1021  0.0280 -0.0271 -0.1045\n",
      " -0.0619 -0.0198 -0.1829 -0.1694 -0.0061\n",
      "  0.0325  0.1816 -0.1160 -0.1559 -0.1713\n",
      " -0.1914 -0.0868 -0.0691  0.1461  0.1906\n",
      "  0.1825 -0.0355  0.1749 -0.1248  0.0533\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0972 -0.1073  0.0533  0.0308  0.0003\n",
      " -0.1667  0.0635  0.1222 -0.0063  0.1415\n",
      " -0.1676 -0.1665  0.0203  0.1587  0.1569\n",
      " -0.1361 -0.1495 -0.1857  0.1050  0.1101\n",
      "  0.1430  0.1045 -0.0758  0.0173  0.0054\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1451 -0.1009 -0.0645 -0.1393 -0.0360\n",
      " -0.1263  0.0898 -0.0153  0.0487  0.1313\n",
      " -0.1020  0.1283  0.1160 -0.0856  0.1764\n",
      "  0.0959 -0.0986  0.1798  0.0718 -0.0006\n",
      " -0.0081  0.0396 -0.0959  0.1515 -0.1476\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0313  0.0674 -0.1024 -0.1426  0.0763\n",
      "  0.0478  0.1991  0.1148 -0.1622  0.1632\n",
      " -0.0497  0.0366 -0.1889  0.1366  0.0326\n",
      " -0.1503  0.1278  0.1534 -0.1783  0.1207\n",
      " -0.1894 -0.0699 -0.1727 -0.1484 -0.0266\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.0650  0.1457  0.1763  0.1939 -0.1967\n",
      " -0.1926  0.1483 -0.1786 -0.1758 -0.0263\n",
      " -0.1659  0.0747 -0.1326 -0.1463  0.0569\n",
      " -0.1023 -0.1474 -0.0777  0.0383 -0.0745\n",
      " -0.0889 -0.1137  0.1461  0.1881  0.1989\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1285  0.1260  0.1488 -0.0720  0.1056\n",
      "  0.0266  0.0821  0.0793 -0.0408 -0.0844\n",
      " -0.0260 -0.0671  0.1615  0.1830  0.0801\n",
      " -0.0738  0.1425 -0.1389  0.0774 -0.1525\n",
      " -0.0774 -0.0248  0.0547  0.0098  0.0360\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.0495 -0.0707 -0.0510 -0.0572  0.1765\n",
      "  0.1957 -0.1907 -0.1210  0.0128 -0.0720\n",
      " -0.0019 -0.1344  0.0173  0.0310  0.0436\n",
      "  0.0514 -0.0351 -0.0942  0.1216 -0.0706\n",
      " -0.1502 -0.1026  0.1954 -0.0911  0.1237\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "前向传播前, conv1的梯度大小: \n",
      "None\n",
      "输出前向传播的输出结果:\n",
      "Variable containing:\n",
      "-2.2446 -2.3548 -2.3868 -2.6543 -2.5154 -2.4614 -2.3000 -2.0199 -2.1572 -2.1027\n",
      "-2.1604 -2.3643 -2.4422 -2.3419 -2.2741 -2.2310 -2.3871 -2.3045 -2.2854 -2.2649\n",
      "-2.1930 -2.3468 -2.4327 -2.3220 -2.4077 -2.2074 -2.3383 -2.2962 -2.1809 -2.3352\n",
      "-2.5519 -2.5019 -2.1084 -2.5433 -2.2231 -2.3912 -2.0769 -2.3265 -2.2004 -2.2388\n",
      "-2.0940 -2.6369 -2.6659 -2.2191 -2.3812 -2.2462 -2.3643 -2.0186 -2.2271 -2.3640\n",
      "-2.3216 -2.3520 -2.2508 -2.1552 -2.5579 -2.2785 -2.4998 -2.0807 -2.2172 -2.4107\n",
      "-2.2470 -2.5732 -2.3553 -2.5028 -2.5807 -2.0380 -2.0388 -2.0122 -2.4867 -2.4282\n",
      "-2.3706 -2.4370 -2.1594 -2.4340 -2.2128 -2.6643 -2.2803 -2.2698 -2.0559 -2.2685\n",
      "-2.2574 -2.4457 -2.5030 -2.3586 -2.3557 -2.3858 -2.4450 -2.0405 -2.1969 -2.1402\n",
      "-2.2736 -2.4441 -2.5020 -2.1962 -2.1687 -2.2796 -2.3238 -2.1192 -2.3990 -2.3902\n",
      "-2.2799 -2.3319 -2.3668 -2.1758 -2.2588 -2.2939 -2.6071 -2.3522 -2.1396 -2.2894\n",
      "-2.2572 -2.4185 -2.3031 -2.1879 -2.2833 -2.5383 -2.4969 -2.1464 -2.1542 -2.3226\n",
      "-2.2504 -2.5481 -2.2772 -2.5948 -2.4844 -2.4792 -2.2625 -1.9982 -2.0479 -2.2698\n",
      "-2.6361 -2.5128 -2.4262 -2.5625 -1.8431 -2.5052 -2.5007 -1.8435 -2.1879 -2.4144\n",
      "-2.4782 -2.1584 -2.5326 -2.5392 -2.3099 -2.1540 -2.8225 -2.3132 -1.9382 -2.0830\n",
      "-2.6211 -2.1641 -2.3414 -3.0415 -2.4286 -2.2438 -2.4511 -2.3228 -1.7877 -2.0978\n",
      "-2.3673 -2.3293 -2.1831 -2.1920 -2.5962 -2.1182 -2.2550 -2.2386 -2.3033 -2.5474\n",
      "-2.4094 -2.2588 -2.2402 -2.2751 -2.2325 -2.5798 -2.6285 -2.4616 -2.0004 -2.1152\n",
      "-2.3162 -2.4483 -2.5119 -2.5287 -2.2777 -2.3178 -2.4361 -2.2953 -1.9744 -2.0719\n",
      "-2.5507 -2.4213 -2.3059 -2.5361 -2.0599 -2.4182 -2.3516 -2.0050 -2.1813 -2.3539\n",
      "-2.3994 -2.2788 -2.3309 -2.1755 -2.2149 -2.2686 -2.5774 -2.5344 -1.9839 -2.4010\n",
      "-2.4995 -2.2801 -2.2512 -2.3026 -2.2993 -2.2564 -2.3779 -2.1623 -2.2155 -2.4262\n",
      "-2.4225 -2.3278 -2.2040 -2.5798 -2.3008 -2.2198 -2.1444 -2.1467 -2.4027 -2.3608\n",
      "-2.1476 -2.6366 -2.3243 -2.2545 -2.3888 -2.4992 -2.5286 -2.0955 -2.0472 -2.2723\n",
      "-2.3422 -2.3996 -2.4114 -2.2403 -1.9795 -2.0984 -2.6982 -2.3903 -2.1601 -2.5015\n",
      "-2.2102 -2.3263 -2.3521 -2.0788 -2.0743 -2.4986 -2.5280 -2.6828 -2.3234 -2.1333\n",
      "-2.1283 -2.2884 -2.4218 -2.3399 -2.4971 -2.1381 -2.5133 -2.3233 -2.2450 -2.2131\n",
      "-2.4563 -2.4601 -2.4663 -2.6368 -2.2261 -2.7487 -2.4758 -2.1009 -1.8995 -1.9361\n",
      "-2.3100 -2.3348 -2.3114 -2.3354 -2.1239 -2.3672 -2.4542 -2.1264 -2.2702 -2.4509\n",
      "-2.3996 -2.2788 -2.2024 -2.2824 -2.4157 -2.3393 -2.4360 -2.1842 -2.2294 -2.2937\n",
      "-2.2153 -2.4007 -2.4010 -2.4401 -2.3119 -2.2687 -2.3699 -2.1302 -2.2486 -2.2820\n",
      "-2.5757 -2.1396 -2.4186 -2.8894 -2.2722 -2.3356 -2.6945 -2.3622 -1.8196 -1.9818\n",
      "-2.3595 -2.6060 -2.3334 -2.2377 -2.3550 -2.4754 -2.3330 -1.9824 -2.1385 -2.3390\n",
      "-2.2748 -2.5568 -2.1575 -2.1872 -2.6605 -2.5090 -2.3318 -2.0070 -2.0922 -2.4582\n",
      "-2.3541 -2.2763 -2.4705 -2.3342 -2.1613 -2.4210 -2.6209 -2.4689 -2.0107 -2.0746\n",
      "-2.5450 -2.3926 -2.2870 -2.4895 -2.1716 -2.4048 -2.3720 -2.0500 -2.1256 -2.3030\n",
      "-2.0690 -2.3706 -2.5272 -2.6876 -2.4067 -2.1427 -2.3418 -2.1421 -2.1966 -2.3011\n",
      "-2.4002 -2.4395 -2.5549 -2.6509 -2.1668 -2.6148 -2.4668 -2.1003 -1.8794 -2.0706\n",
      "-2.5901 -2.2866 -2.1682 -2.5531 -2.0565 -2.3365 -2.4270 -2.3411 -2.0531 -2.3672\n",
      "-2.2195 -2.7559 -2.2351 -2.4092 -2.3165 -2.4272 -2.6627 -2.3128 -1.8400 -2.1502\n",
      "-2.6232 -2.5428 -2.0413 -2.3364 -2.6011 -2.4685 -2.6661 -2.0505 -1.8279 -2.2596\n",
      "-2.4345 -2.3389 -2.3769 -2.7289 -2.4374 -2.3307 -2.4613 -2.1122 -1.8848 -2.1625\n",
      "-2.3860 -2.4551 -2.2693 -2.3436 -2.0566 -2.6433 -2.3421 -2.1097 -2.2037 -2.3438\n",
      "-2.4709 -2.2119 -2.2502 -2.4811 -2.7266 -2.1463 -2.8140 -2.3163 -1.7595 -2.2645\n",
      "-2.2814 -2.4169 -2.5021 -2.2945 -2.3507 -2.4493 -2.2592 -2.0651 -2.2480 -2.2310\n",
      "-1.9738 -2.6548 -2.5256 -2.1090 -2.1479 -2.1477 -2.6090 -2.3256 -2.2243 -2.5714\n",
      "-2.2995 -2.4206 -2.4205 -2.3102 -2.2950 -2.2975 -2.6261 -2.2625 -1.9487 -2.2778\n",
      "-2.2867 -2.4456 -2.1360 -2.2811 -2.3536 -2.2314 -2.6105 -2.3382 -1.9691 -2.5326\n",
      "-2.3539 -2.4994 -2.3507 -2.3790 -2.2829 -2.4365 -2.4442 -2.3156 -2.0205 -2.0608\n",
      "-2.0960 -2.3567 -2.3318 -2.2079 -2.4784 -2.2270 -2.4912 -2.3675 -2.1094 -2.4568\n",
      "-2.3851 -2.4356 -2.1597 -2.2981 -2.5468 -2.1225 -2.3720 -2.1578 -2.1586 -2.4992\n",
      "-2.1856 -2.3894 -2.1216 -2.0987 -2.1180 -2.1917 -2.5913 -2.4226 -2.3858 -2.7206\n",
      "-2.0776 -2.4691 -2.5453 -2.3887 -2.2256 -2.4246 -2.3707 -2.1245 -2.3465 -2.1647\n",
      "-2.4905 -2.3698 -2.3549 -2.4047 -2.2780 -2.2954 -2.3544 -2.1589 -2.2159 -2.1562\n",
      "-2.3946 -2.2071 -2.2842 -2.4113 -2.2663 -2.3520 -2.4847 -2.3638 -2.1520 -2.1656\n",
      "-2.1370 -2.7190 -2.5815 -2.1925 -2.1254 -2.4357 -2.3301 -2.1176 -2.1927 -2.3792\n",
      "-2.1214 -2.5656 -2.3453 -2.0435 -2.4579 -2.6622 -2.4091 -2.2397 -2.0277 -2.3598\n",
      "-2.2576 -2.4853 -2.2069 -2.3755 -2.2664 -2.5961 -2.5616 -2.0930 -2.1531 -2.1696\n",
      "-2.3303 -2.4890 -2.2446 -2.4088 -2.1555 -2.3835 -2.5502 -2.0659 -2.0314 -2.5276\n",
      "-2.1617 -2.3774 -2.4942 -2.3672 -2.4266 -2.0589 -2.5277 -2.5619 -2.0142 -2.2125\n",
      "-2.5079 -2.3152 -2.3843 -2.5495 -1.9143 -2.3469 -2.4780 -2.0646 -2.1864 -2.4852\n",
      "-2.3082 -2.3397 -2.4023 -2.2425 -2.1375 -2.2674 -2.5081 -2.2630 -2.1918 -2.4215\n",
      "-2.1192 -2.4906 -2.5433 -2.2687 -2.2453 -2.3313 -2.6057 -2.1700 -2.2125 -2.1676\n",
      "-2.1092 -2.5503 -2.2847 -2.1133 -2.3194 -2.4856 -2.6812 -2.0788 -2.1858 -2.4034\n",
      "[torch.cuda.FloatTensor of size 64x10 (GPU 0)]\n",
      "\n",
      "输出本次前向传波的loss值:\n",
      "Variable containing:\n",
      " 2.3068\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "前向传播后反向传播前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0740  0.0984  0.0291  0.0744 -0.0649\n",
      "  0.0625 -0.1482  0.0495  0.1548  0.0458\n",
      " -0.1725 -0.0281 -0.1418  0.1696 -0.0733\n",
      " -0.1665  0.1341 -0.0754 -0.1999 -0.1256\n",
      "  0.1634 -0.1857  0.1749  0.1064 -0.0227\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.1455 -0.1978 -0.0071 -0.1224 -0.1842\n",
      "  0.1258  0.0140 -0.1828 -0.1968  0.1506\n",
      "  0.1372  0.1532 -0.0065 -0.1868 -0.1032\n",
      " -0.0818  0.1343  0.0123 -0.0034 -0.0858\n",
      "  0.0863 -0.0504 -0.1059  0.1758 -0.0583\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1138  0.1978 -0.1051 -0.1497 -0.0010\n",
      "  0.1934 -0.1352 -0.0477  0.1994  0.0043\n",
      "  0.0989  0.1776 -0.0782 -0.0267 -0.0714\n",
      " -0.0152  0.1020 -0.0377  0.1476  0.1843\n",
      " -0.0009  0.0749  0.1425 -0.1291 -0.1313\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1619 -0.1021  0.0280 -0.0271 -0.1045\n",
      " -0.0619 -0.0198 -0.1829 -0.1694 -0.0061\n",
      "  0.0325  0.1816 -0.1160 -0.1559 -0.1713\n",
      " -0.1914 -0.0868 -0.0691  0.1461  0.1906\n",
      "  0.1825 -0.0355  0.1749 -0.1248  0.0533\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0972 -0.1073  0.0533  0.0308  0.0003\n",
      " -0.1667  0.0635  0.1222 -0.0063  0.1415\n",
      " -0.1676 -0.1665  0.0203  0.1587  0.1569\n",
      " -0.1361 -0.1495 -0.1857  0.1050  0.1101\n",
      "  0.1430  0.1045 -0.0758  0.0173  0.0054\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1451 -0.1009 -0.0645 -0.1393 -0.0360\n",
      " -0.1263  0.0898 -0.0153  0.0487  0.1313\n",
      " -0.1020  0.1283  0.1160 -0.0856  0.1764\n",
      "  0.0959 -0.0986  0.1798  0.0718 -0.0006\n",
      " -0.0081  0.0396 -0.0959  0.1515 -0.1476\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0313  0.0674 -0.1024 -0.1426  0.0763\n",
      "  0.0478  0.1991  0.1148 -0.1622  0.1632\n",
      " -0.0497  0.0366 -0.1889  0.1366  0.0326\n",
      " -0.1503  0.1278  0.1534 -0.1783  0.1207\n",
      " -0.1894 -0.0699 -0.1727 -0.1484 -0.0266\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.0650  0.1457  0.1763  0.1939 -0.1967\n",
      " -0.1926  0.1483 -0.1786 -0.1758 -0.0263\n",
      " -0.1659  0.0747 -0.1326 -0.1463  0.0569\n",
      " -0.1023 -0.1474 -0.0777  0.0383 -0.0745\n",
      " -0.0889 -0.1137  0.1461  0.1881  0.1989\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1285  0.1260  0.1488 -0.0720  0.1056\n",
      "  0.0266  0.0821  0.0793 -0.0408 -0.0844\n",
      " -0.0260 -0.0671  0.1615  0.1830  0.0801\n",
      " -0.0738  0.1425 -0.1389  0.0774 -0.1525\n",
      " -0.0774 -0.0248  0.0547  0.0098  0.0360\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.0495 -0.0707 -0.0510 -0.0572  0.1765\n",
      "  0.1957 -0.1907 -0.1210  0.0128 -0.0720\n",
      " -0.0019 -0.1344  0.0173  0.0310  0.0436\n",
      "  0.0514 -0.0351 -0.0942  0.1216 -0.0706\n",
      " -0.1502 -0.1026  0.1954 -0.0911  0.1237\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "前向传播后反向传播前, conv1的梯度大小: \n",
      "None\n",
      "反向传播后参数更新前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0740  0.0984  0.0291  0.0744 -0.0649\n",
      "  0.0625 -0.1482  0.0495  0.1548  0.0458\n",
      " -0.1725 -0.0281 -0.1418  0.1696 -0.0733\n",
      " -0.1665  0.1341 -0.0754 -0.1999 -0.1256\n",
      "  0.1634 -0.1857  0.1749  0.1064 -0.0227\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.1455 -0.1978 -0.0071 -0.1224 -0.1842\n",
      "  0.1258  0.0140 -0.1828 -0.1968  0.1506\n",
      "  0.1372  0.1532 -0.0065 -0.1868 -0.1032\n",
      " -0.0818  0.1343  0.0123 -0.0034 -0.0858\n",
      "  0.0863 -0.0504 -0.1059  0.1758 -0.0583\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1138  0.1978 -0.1051 -0.1497 -0.0010\n",
      "  0.1934 -0.1352 -0.0477  0.1994  0.0043\n",
      "  0.0989  0.1776 -0.0782 -0.0267 -0.0714\n",
      " -0.0152  0.1020 -0.0377  0.1476  0.1843\n",
      " -0.0009  0.0749  0.1425 -0.1291 -0.1313\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1619 -0.1021  0.0280 -0.0271 -0.1045\n",
      " -0.0619 -0.0198 -0.1829 -0.1694 -0.0061\n",
      "  0.0325  0.1816 -0.1160 -0.1559 -0.1713\n",
      " -0.1914 -0.0868 -0.0691  0.1461  0.1906\n",
      "  0.1825 -0.0355  0.1749 -0.1248  0.0533\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0972 -0.1073  0.0533  0.0308  0.0003\n",
      " -0.1667  0.0635  0.1222 -0.0063  0.1415\n",
      " -0.1676 -0.1665  0.0203  0.1587  0.1569\n",
      " -0.1361 -0.1495 -0.1857  0.1050  0.1101\n",
      "  0.1430  0.1045 -0.0758  0.0173  0.0054\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1451 -0.1009 -0.0645 -0.1393 -0.0360\n",
      " -0.1263  0.0898 -0.0153  0.0487  0.1313\n",
      " -0.1020  0.1283  0.1160 -0.0856  0.1764\n",
      "  0.0959 -0.0986  0.1798  0.0718 -0.0006\n",
      " -0.0081  0.0396 -0.0959  0.1515 -0.1476\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0313  0.0674 -0.1024 -0.1426  0.0763\n",
      "  0.0478  0.1991  0.1148 -0.1622  0.1632\n",
      " -0.0497  0.0366 -0.1889  0.1366  0.0326\n",
      " -0.1503  0.1278  0.1534 -0.1783  0.1207\n",
      " -0.1894 -0.0699 -0.1727 -0.1484 -0.0266\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.0650  0.1457  0.1763  0.1939 -0.1967\n",
      " -0.1926  0.1483 -0.1786 -0.1758 -0.0263\n",
      " -0.1659  0.0747 -0.1326 -0.1463  0.0569\n",
      " -0.1023 -0.1474 -0.0777  0.0383 -0.0745\n",
      " -0.0889 -0.1137  0.1461  0.1881  0.1989\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1285  0.1260  0.1488 -0.0720  0.1056\n",
      "  0.0266  0.0821  0.0793 -0.0408 -0.0844\n",
      " -0.0260 -0.0671  0.1615  0.1830  0.0801\n",
      " -0.0738  0.1425 -0.1389  0.0774 -0.1525\n",
      " -0.0774 -0.0248  0.0547  0.0098  0.0360\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.0495 -0.0707 -0.0510 -0.0572  0.1765\n",
      "  0.1957 -0.1907 -0.1210  0.0128 -0.0720\n",
      " -0.0019 -0.1344  0.0173  0.0310  0.0436\n",
      "  0.0514 -0.0351 -0.0942  0.1216 -0.0706\n",
      " -0.1502 -0.1026  0.1954 -0.0911  0.1237\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "反向传播后参数更新前, conv1的梯度大小: \n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -1.0372 -0.8457 -1.1499 -0.6643 -0.9267\n",
      "  -1.1824 -0.9839 -1.0684 -0.9089 -0.1955\n",
      "  -0.2479 -0.0322 -0.5401 -0.1562  0.1122\n",
      "   0.1398  0.9660  0.3318  0.1776  0.0432\n",
      "  -0.1815  0.5070  0.1805 -0.2315 -0.5794\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.3565 -0.1246  0.0918 -0.2375 -0.1538\n",
      "   0.0096  0.0227 -0.2321 -0.3619 -0.0758\n",
      "  -0.0260  0.0624 -0.2159 -0.1610 -0.0708\n",
      "   0.5948 -0.0016 -0.2587 -0.3824 -0.6132\n",
      "   0.5623 -0.0066 -0.2380 -0.2854 -0.1162\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.5105  0.9412  0.8232  0.3477  0.4434\n",
      "   0.1334  0.9441  0.9655  1.0043  0.5279\n",
      "   0.9695  1.1358  1.5895  0.8876 -0.0281\n",
      "   0.4173  1.0829  1.2350  0.5928  0.5113\n",
      "  -0.9108 -0.1956  0.3366  0.5489  0.6283\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.5606  0.4815  0.5976  0.6718  0.5324\n",
      "   0.7243  0.6578  0.5206  0.4971  0.4256\n",
      "   0.9250  0.7229  0.2218  0.1197  0.1004\n",
      "   1.0840  0.9480  0.3554 -0.5181 -0.6242\n",
      "   0.9051  0.6728 -0.5608 -0.6264 -0.0033\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   1.2280  0.6073  1.0656  1.0944  1.2258\n",
      "  -1.4568 -1.3058  0.7971  0.7856  1.0130\n",
      "  -1.7419 -1.0092  0.5222  0.6426  0.2356\n",
      "  -0.5151  0.5400  0.8664  0.4295 -0.0963\n",
      "   0.9436  1.0974  0.4199  0.1021 -0.0399\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.4797  0.2784 -0.2238  0.3488 -0.1584\n",
      "   0.3427  0.2265  0.4242  1.1669  0.7089\n",
      "   0.2356  0.3699  0.4513  0.8053  0.8374\n",
      "  -0.0094 -0.0651  0.2554  0.5529  1.1648\n",
      "  -0.3865 -0.3261 -0.4502  0.5546  0.7899\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.4748  1.3432  0.3240  0.0506 -0.0493\n",
      "   0.0432  0.5416  0.4233  0.5716  0.7750\n",
      "   0.2338  0.0935 -0.3214  0.2137  0.0947\n",
      "   0.2938 -0.1867 -0.4194 -0.2574  0.3200\n",
      "  -0.1562 -0.0694 -0.2038 -0.7076 -0.4608\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.2176  0.8607  0.7205  1.1964  0.5542\n",
      "   0.2984  1.0869  0.8136  0.1569 -0.1165\n",
      "   0.0689  0.7616  0.9181 -0.3003  0.3623\n",
      "   0.2487  1.0050  0.7028  0.3992  1.0963\n",
      "  -0.1846 -0.0777  0.2434  1.0964  1.6056\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.3570 -1.4612 -1.7705 -2.2865 -1.8437\n",
      "  -1.2203 -1.4242 -0.9335 -0.9136 -0.1158\n",
      "  -0.7352 -0.5661  0.0617 -0.5602  0.5350\n",
      "   0.0416 -0.2301 -0.0285  0.0828  0.4640\n",
      "  -0.0254  0.0081  0.0231  0.4133  1.1542\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.8765  0.1971  0.0150 -0.1861 -0.7622\n",
      "   0.9132  0.1723  0.1861 -0.1675 -0.7367\n",
      "   0.6508  0.1507  0.0129  0.2499 -0.0148\n",
      "   0.5107  0.2229 -0.0632  0.5904  0.3913\n",
      "   0.2712  0.5055  0.7002  0.9357  1.2786\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "参数更新后, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0739  0.0985  0.0292  0.0745 -0.0649\n",
      "  0.0627 -0.1481  0.0496  0.1549  0.0458\n",
      " -0.1724 -0.0281 -0.1418  0.1696 -0.0734\n",
      " -0.1665  0.1340 -0.0754 -0.1999 -0.1256\n",
      "  0.1634 -0.1857  0.1749  0.1064 -0.0226\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.1455 -0.1977 -0.0071 -0.1223 -0.1842\n",
      "  0.1258  0.0140 -0.1828 -0.1968  0.1506\n",
      "  0.1372  0.1532 -0.0064 -0.1868 -0.1032\n",
      " -0.0819  0.1343  0.0124 -0.0033 -0.0857\n",
      "  0.0862 -0.0504 -0.1058  0.1758 -0.0583\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1137  0.1977 -0.1052 -0.1498 -0.0011\n",
      "  0.1934 -0.1353 -0.0478  0.1993  0.0042\n",
      "  0.0988  0.1774 -0.0783 -0.0268 -0.0714\n",
      " -0.0153  0.1019 -0.0378  0.1475  0.1842\n",
      " -0.0008  0.0749  0.1424 -0.1291 -0.1313\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1620 -0.1022  0.0279 -0.0271 -0.1046\n",
      " -0.0619 -0.0199 -0.1830 -0.1694 -0.0062\n",
      "  0.0324  0.1815 -0.1160 -0.1559 -0.1713\n",
      " -0.1915 -0.0869 -0.0691  0.1461  0.1906\n",
      "  0.1824 -0.0355  0.1750 -0.1247  0.0533\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0970 -0.1073  0.0532  0.0306  0.0002\n",
      " -0.1665  0.0636  0.1221 -0.0064  0.1414\n",
      " -0.1675 -0.1664  0.0203  0.1586  0.1569\n",
      " -0.1360 -0.1495 -0.1858  0.1050  0.1101\n",
      "  0.1429  0.1044 -0.0758  0.0173  0.0054\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1451 -0.1010 -0.0644 -0.1393 -0.0360\n",
      " -0.1264  0.0898 -0.0154  0.0485  0.1312\n",
      " -0.1020  0.1282  0.1159 -0.0857  0.1763\n",
      "  0.0959 -0.0986  0.1798  0.0718 -0.0007\n",
      " -0.0081  0.0396 -0.0958  0.1515 -0.1477\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0313  0.0673 -0.1024 -0.1426  0.0763\n",
      "  0.0478  0.1991  0.1148 -0.1622  0.1631\n",
      " -0.0497  0.0366 -0.1888  0.1366  0.0326\n",
      " -0.1504  0.1279  0.1534 -0.1782  0.1207\n",
      " -0.1894 -0.0699 -0.1726 -0.1484 -0.0266\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "  0.0650  0.1457  0.1762  0.1938 -0.1968\n",
      " -0.1927  0.1482 -0.1787 -0.1758 -0.0263\n",
      " -0.1659  0.0746 -0.1327 -0.1463  0.0569\n",
      " -0.1023 -0.1475 -0.0778  0.0382 -0.0746\n",
      " -0.0889 -0.1137  0.1461  0.1880  0.1987\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1285  0.1261  0.1490 -0.0717  0.1058\n",
      "  0.0268  0.0823  0.0794 -0.0407 -0.0844\n",
      " -0.0260 -0.0670  0.1615  0.1830  0.0800\n",
      " -0.0738  0.1426 -0.1389  0.0774 -0.1526\n",
      " -0.0774 -0.0248  0.0547  0.0097  0.0358\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.0496 -0.0707 -0.0510 -0.0572  0.1766\n",
      "  0.1956 -0.1907 -0.1210  0.0128 -0.0719\n",
      " -0.0019 -0.1344  0.0173  0.0310  0.0436\n",
      "  0.0513 -0.0351 -0.0942  0.1215 -0.0706\n",
      " -0.1502 -0.1027  0.1954 -0.0912  0.1236\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 预先记录前向传播前的参数大小(以conv1的weiht为例)\n",
    "print(\"前向传播前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"前向传播前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#初始化梯度\n",
    "optimizer.zero_grad()  # zero the gradient buffers，必须要置零\n",
    "\n",
    "#前向传播\n",
    "output = model(data)   #前向传播的输出结果, 是一个 64x10的结果, 其中64表示batch_size,10为类别数\n",
    "print(\"输出前向传播的输出结果:\")\n",
    "print(output)\n",
    "\n",
    "#计算loss\n",
    "loss = F.nll_loss(output, target)    # 计算本次前向传波的loss值\n",
    "print(\"输出本次前向传波的loss值:\")\n",
    "print(loss)\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"前向传播后反向传播前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"前向传播后反向传播前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#反向传播\n",
    "loss.backward()    # loss的反向传播\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"反向传播后参数更新前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"反向传播后参数更新前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#参数更新\n",
    "optimizer.step()\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"参数更新后, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程详细分析\n",
    "(1). 前向传播的目的是为了通过网络计算出最终的预测结果, \n",
    "再通过损失函数计算出预测值与真实值之间的偏差,即loss, \n",
    "将loss值极小化作为优化目标, 极小化预测值与真实值之间的差别.\n",
    "\n",
    "(2). 优化的方式就是通过计算梯度, 来更新参数包括所有的weight, bias. \n",
    "进而使得对于输入的data都可以准确的预测其结果.\n",
    "\n",
    "(3).于是反向传播的过程也就是根据loss计算相关参数的grad, 计算方式采用的是连式法则.\n",
    "\n",
    "(4).在计算完各层参数的grad之后,对参数进行更新,\n",
    "这个更新后的参数,可以使得通过网络得到的预测结果与真实结果之间的差异变小,达到优化的目的.\n",
    "\n",
    "(5). 而在pytorch代码层面上, 可以发现weight的更新时发生在optimizer.step() (即参数更新指令)之后的, \n",
    "而梯度生成时发生在loss.backward() (反向传播)之后的, \n",
    "并且梯度是单次有效的,每次前向传播,反向传播后的梯度都是根据本次loss计算出来的,\n",
    "因此在进行前向传播之前,要将梯度初始化.\n",
    "\n",
    "#### 当然, 感兴趣的话还可以打印每次卷积前后data发生的变化,这对于理解卷积过程也是有所帮助的, 这部分内容我将放在后面检测过程中的单步结果中讲."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面, 我们展示一下网络的详细检测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 1 ====. Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      "-24.1064 -21.4635 -16.1562 -14.9996 -24.7177 -27.1383 -41.7026   0.0000\n",
      "\n",
      "Columns 8 to 9 \n",
      "-19.3323 -14.3816\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n",
      "==== 2 ==== Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      "-24.1064 -21.4635 -16.1562 -14.9996 -24.7177 -27.1383 -41.7026   0.0000\n",
      "\n",
      "Columns 8 to 9 \n",
      "-19.3323 -14.3816\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n",
      "==== 3 ==== 检测结果为: \t \n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "==== 4 ==== 真实结果为: \t Variable containing:\n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#以比较蠢的方式选取第一张图片数据\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "#如下两种方式得到,softmax结果:\n",
    "output = model(data)\n",
    "print(\"==== 1 ====.\", output) \n",
    "\n",
    "output = model.forward(data)\n",
    "print(\"==== 2 ====\", output) \n",
    "\n",
    "#对比检测结果与真实结果: (softmax最大的值对应的index对应的label即为检测结果)\n",
    "pred = output.data.max(1, keepdim=True)[1] \n",
    "print(\"==== 3 ==== 检测结果为: \\t {}\".format(pred))\n",
    "print(\"==== 4 ==== 真实结果为: \\t {}\".format(target))\n",
    "\n",
    "#print(output.data.max(1, keepdim=True))\n",
    "# 可见检测结果是正确的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测过程详细分析\n",
    "\n",
    "(1). 检测就是一个前向传播的过程, 将通过卷积,全连最终接得到feature map, 通过softmax进行分类.\n",
    "\n",
    "(2). pytorch代码上, 可以发现model() 和 model,forward() 都可以得到结果.\n",
    "\n",
    "(3). 对于softmax的原理, 可以看机器学习相关资料, 这里不再展开.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Variable containing:\n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n",
    "\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "print(target)  #打印真实label,可见这张图是7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输入图片的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.6450  1.9305  1.5996\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.4015  2.8088  2.8088\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.4286  1.0268  0.4922\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.4978  0.3395  0.0340 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  2.8088  2.6433  2.0960  2.0960  2.0960  2.0960  2.0960  2.0960\n",
      "  1.0268  1.6505  2.4651  2.8088  2.4396  2.8088  2.8088  2.8088  2.7578\n",
      " -0.4242 -0.4242 -0.2078  0.4159 -0.2460  0.4286  0.4286  0.4286  0.3268\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1442\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.2177\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.3268  2.7451\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.2686  2.8088\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.3097  2.1851  2.7324\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.1795  2.8088  1.8923\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5304  2.7706  2.6306  0.3013\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.1824  2.3887  2.8088  1.6887 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.3860  2.1596  2.8088  2.3633  0.0213 -0.4242\n",
      " -0.4242 -0.4242 -0.4242  0.0595  2.8088  2.8088  0.5559 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.0296  2.4269  2.8088  1.0395 -0.4115 -0.4242 -0.4242\n",
      " -0.4242 -0.4242  1.2686  2.8088  2.8088  0.2377 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  0.3522  2.6560  2.8088  2.8088  0.2377 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.1159  2.8088  2.8088  2.3633  0.0849 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.1159  2.8088  2.2105 -0.1951 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.0960  2.0960  1.7396  0.2377 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.4906  2.8088  2.8088  1.3577 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.1569  2.5797  2.8088  0.9250 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.6322  2.7960  2.2360 -0.1951 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.5415  2.8215  0.6322 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  2.6051  0.1358 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  0.3649 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.9560 -0.3606 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.3140 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.cuda.FloatTensor of size 1x1x28x28 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "conv1卷积核参数:\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0123  0.3203  0.1902  0.1928 -0.2151\n",
      " -0.1781 -0.1174  0.1169 -0.3019 -0.2756\n",
      " -0.2560 -0.1765 -0.2874 -0.1935 -0.1010\n",
      "  0.0216 -0.0437  0.1859  0.1419  0.3472\n",
      "  0.1440  0.1472  0.2561  0.1922  0.0595\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0016 -0.0248 -0.3665 -0.4912 -0.2821\n",
      "  0.1046  0.1147 -0.2822 -0.4187 -0.3407\n",
      "  0.1260  0.1242  0.2976 -0.0996  0.1708\n",
      "  0.2753  0.3852  0.5968  0.5663  0.1342\n",
      " -0.1583 -0.0122  0.0794  0.3302  0.2862\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.0810  0.0697 -0.1096  0.2153 -0.0935\n",
      " -0.1730 -0.2413 -0.2268  0.1703  0.2024\n",
      " -0.3691 -0.1627 -0.1603  0.1336  0.3938\n",
      " -0.2512 -0.1425 -0.1225  0.3342  0.2061\n",
      " -0.1618 -0.0900  0.2425  0.3462  0.2142\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1188 -0.1875 -0.3333 -0.1055 -0.1180\n",
      "  0.3119  0.0332 -0.0895 -0.2843 -0.2190\n",
      "  0.1439  0.1984 -0.1964 -0.2702 -0.2439\n",
      "  0.1588  0.0948  0.0925 -0.2787 -0.1529\n",
      "  0.2804  0.0442 -0.1196 -0.2150 -0.0718\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.3026 -0.2946 -0.1362  0.0057 -0.1184\n",
      " -0.0352 -0.2338 -0.2373 -0.1217  0.0501\n",
      " -0.3166 -0.0830 -0.0237  0.0686 -0.0180\n",
      "  0.0827 -0.0026 -0.0307  0.1955  0.0694\n",
      " -0.1676  0.0875  0.1867  0.4337  0.3001\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1338  0.2160  0.1940 -0.0899  0.1094\n",
      "  0.2020  0.0985  0.1436 -0.2401 -0.1019\n",
      "  0.3219  0.3379 -0.2712 -0.1827 -0.0157\n",
      "  0.3901  0.1064 -0.2358 -0.1889 -0.2654\n",
      "  0.2028  0.2036 -0.1028 -0.1948 -0.1644\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0259  0.2258  0.2716 -0.0582 -0.1946\n",
      "  0.0848  0.3939  0.1691 -0.0092 -0.2873\n",
      "  0.2543  0.3636 -0.0158 -0.1379 -0.0847\n",
      "  0.0509  0.1609  0.2634 -0.0617 -0.0719\n",
      "  0.1676  0.2454  0.2518  0.3333  0.3134\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.0988  0.1237 -0.0082 -0.0691 -0.3364\n",
      " -0.1331  0.2243  0.3230 -0.2617 -0.3043\n",
      " -0.2055  0.0793  0.5101  0.0776 -0.2322\n",
      " -0.3484 -0.0189  0.3834  0.3236  0.0023\n",
      " -0.2941 -0.4093  0.0343  0.1458 -0.0819\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0502 -0.0256 -0.0227 -0.1724  0.2219\n",
      "  0.0894 -0.1123 -0.0731  0.1553  0.2353\n",
      "  0.0148 -0.0742 -0.2119  0.0528  0.3231\n",
      "  0.1032  0.0562  0.0610  0.3411 -0.0412\n",
      " -0.1019  0.0764 -0.0626  0.1609  0.1817\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1226  0.0963 -0.0259 -0.2276 -0.1783\n",
      "  0.0745 -0.0410  0.0266 -0.2041  0.0982\n",
      " -0.2325  0.0239 -0.0581 -0.2092 -0.1822\n",
      " -0.2436 -0.0952 -0.2417 -0.2094 -0.2479\n",
      " -0.0129 -0.1149 -0.1617  0.1344 -0.1182\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "conv1偏置项参数:\n",
      "Parameter containing:\n",
      " 0.0776\n",
      " 0.2462\n",
      "-0.1159\n",
      " 0.1092\n",
      " 0.0825\n",
      "-0.2185\n",
      "-0.2424\n",
      "-0.1871\n",
      "-0.1153\n",
      "-0.0580\n",
      "[torch.cuda.FloatTensor of size 10 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "经过conv1后的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "           ...             ⋱             ...          \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      "           ...             ⋱             ...          \n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      "           ...             ⋱             ...          \n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "           ...             ⋱             ...          \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      "           ...             ⋱             ...          \n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "           ...             ⋱             ...          \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "[torch.cuda.FloatTensor of size 1x10x24x24 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层分析\n",
    "data_ori = data\n",
    "print(\"原始输入图片的数字结构:\")\n",
    "print(data_ori)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"conv1卷积核参数:\")\n",
    "print(model.conv1.weight)\n",
    "print(\"conv1偏置项参数:\")\n",
    "print(model.conv1.bias)\n",
    "\n",
    "print(\"========================================\")\n",
    "data_1 = model.conv1(data_ori)\n",
    "print(\"经过conv1后的数字结构:\")\n",
    "print(data_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积过程详细分析:\n",
    "（1）这里原始输入data_ori的维度是（1x1x28x28）\n",
    "\n",
    "（2）卷积核conv1.weight的维度是（10x1x5x5）\n",
    "\n",
    "（3）偏置项conv1.bias的维度是 10\n",
    "\n",
    "（4）可见输出的feature map 的维度是（1x10x24x24）\n",
    "\n",
    "#### 输出的feature map 大小计算公式 \n",
    "    输入：（N_0, C_0, H_0, W_0）\n",
    "    输出：（N_1, C_1, H_1, W_1）\n",
    "    有： W_1 = (W_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         H_1 = (H_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         C_1 = 卷积核个数 （备注：卷积核大小为（C_1, C_0, kernel_size, kernel_size））\n",
    "         N_1 = N_0 = batch_size\n",
    "    其中, \n",
    "        pad为padding的大小（其就是在原始输入外侧加 pad圈 0, 那么原始为h×w的大小，加padding之后就变为了(h+2*pad)x(w+2*pad)的大小）\n",
    "        kernel_size为卷积核的大小\n",
    "        stride为卷积步长,这里等于默认值1.\n",
    "   ##### 需要注意的是N表示batch_size, 即当前批次共有batch_size张图, 这个是不变的, 即有N_1 = N_0\n",
    "    \n",
    "#### 本例中输出的feature map 详细计算过程\n",
    "    可以看到输出的feature map中第一个输出值: data_1(1,1,1,1) = -0.0017\n",
    "    它是由：data_ori中左上角, 大小为(N_0 x C_0 x 5 x 5), 即\n",
    "    data_ori(:,:,0:5,0:5)=\n",
    "     [-0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242]\n",
    "    与 第一层的 conv1.weight\n",
    "    conv1.weight(0 ,：,.,.) = \n",
    "      [0.0153  0.3259  0.1931  0.1950 -0.2066\n",
    "      -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
    "      -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
    "      0.0233 -0.0447  0.1835  0.1321  0.3404\n",
    "      0.1407  0.1476  0.2579  0.1959  0.0585]\n",
    "    做点乘, 然后加上\n",
    "    conv1.bias(0) = 0.0825\n",
    "    可以使用计算器计算下结果：得到 -0.00174612\n",
    "    可见与他计算的data_1(1,1,1,1) = -0.0017是相当的\n",
    "\n",
    "#### 总结输出的feature map 详细计算公式\n",
    "    已知输入data_ori(N_0, C_0, H_0, W_0),  conv1.weight(C_1, C_0, kernel_size, kernel_size),  conv1.bias(C_1)\n",
    "    求输出的data_1(n,c,h,w)其值为\n",
    "    data_ori(n, :, h×stride:h×stride+kernel_size, w×stride:w×stride+kernel_size)\n",
    "    与\n",
    "    conv.weight(c ,: , : ,:)做点乘\n",
    "    加上\n",
    "    conv.bias(c)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "经过max pooling后的feature map数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.3459  1.2641  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  1.7049  3.4035  ...   2.5072  0.7461  0.0003\n",
      "           ...             ⋱             ...          \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276  0.7995  1.2142  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276  2.7994  6.8383  ...   3.4475  0.0896 -0.2276\n",
      "           ...             ⋱             ...          \n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452  0.6293  1.3420  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452  3.0415  3.3249  ...  -0.1258 -0.3523 -0.2452\n",
      "           ...             ⋱             ...          \n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.2709  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.5553  3.0084  ...  -1.5453 -0.1381  0.0566\n",
      "           ...             ⋱             ...          \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925  0.0075  0.0873  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925  1.4562  1.4477  ...   0.2013 -0.5254 -0.5925\n",
      "           ...             ⋱             ...          \n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.3921 -0.6667  ...   0.6031  0.9827  0.9912\n",
      "           ...             ⋱             ...          \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "输入的feature map的第一层输入的数字结构:\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0639  0.3459  0.8471  1.2641  1.4342  1.3037  0.8584\n",
      "  0.0003  0.0003  0.5395  1.7049  2.7731  3.4035  3.3593  3.1610  2.6762\n",
      "  0.0003  0.0003  0.9240  1.3291  1.6911  1.3675  1.3559  1.6335  2.2994\n",
      "  0.0003  0.0003 -0.2837 -1.2200 -2.2249 -2.5042 -3.1440 -2.4800 -1.8441\n",
      "  0.0003  0.0003 -1.0947 -2.3557 -1.9327 -1.9447 -2.2410 -2.8753 -2.9737\n",
      "  0.0003  0.0003 -0.8425 -0.8077 -0.1252  0.8413  0.3887  0.0133 -0.2410\n",
      "  0.0003  0.0003 -0.1831 -0.1474  0.2452  0.4141  0.4835  0.3067  0.4359\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003 -0.0462 -0.1386\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0026\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0423\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0238  0.4098\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.2380  1.5044\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0465  0.8806  1.8402\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.3614  1.3884  1.2491\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.5481  0.8167 -0.0447\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.1654 -0.7308 -1.8339\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.5069  0.1778  0.0663  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  2.4552  2.1938  2.1028  2.0141  2.0141  2.0141  2.0141  1.9929  1.8351\n",
      "  3.0194  3.6558  3.9795  4.1746  4.1728  4.2003  4.1548  4.0163  3.3928\n",
      " -1.2416 -0.6445 -0.3278  0.1512  0.1074  0.0641  0.1709  0.6054  0.9390\n",
      " -3.9578 -4.3229 -4.6641 -4.5334 -4.5780 -4.7406 -3.5122 -2.3541 -1.3536\n",
      " -0.5692 -1.0407 -1.6638 -1.8820 -1.7682 -1.2688  0.0704  0.5917  0.2511\n",
      "  0.9623  1.2072  0.9354  0.9371  1.3234  2.6951  2.7669  1.3915  1.2425\n",
      "  0.1651  0.0804  0.2870  0.2556  1.3153  2.2377  1.0499 -0.3242  0.3252\n",
      "  0.0003  0.0003  0.0003  0.3617  1.4817  1.1350 -0.8216 -0.6363  0.0140\n",
      "  0.0003  0.0003  0.0071  0.6893  1.1032 -0.1904 -1.2024 -0.2878  0.2204\n",
      "  0.0003  0.0003  0.1355  1.0449  0.5501 -0.7974 -0.4412  0.4010 -0.0077\n",
      "  0.0003  0.0571  0.9190  1.4766  0.3548 -0.4413  0.6205  0.4483 -0.4064\n",
      "  0.0147  0.5455  1.8462  1.4399 -0.4128 -0.1173  0.6226 -0.1249 -0.4115\n",
      "  0.2453  1.6134  2.0473  0.3657 -0.9008 -0.0504  0.0653 -0.4639 -0.3226\n",
      "  1.1635  1.8399  1.0303 -1.1694 -0.6379 -0.0883 -0.3650 -0.3782 -0.1003\n",
      "  1.6976  1.2770 -0.9056 -1.2230 -0.6104 -0.0849 -0.4367 -0.2200  0.0093\n",
      "  1.6519 -0.0981 -0.9202 -0.6223  0.1699 -0.2526 -0.0824 -0.0530  0.0003\n",
      "  1.0359 -0.3929  0.2503  0.9409  0.3576  0.0596 -0.0004  0.0058  0.0003\n",
      "  0.2157  0.2713  1.0229  0.5956  0.0891  0.0101  0.0101  0.0003  0.0003\n",
      " -0.1788  0.1166  0.3251 -0.6584 -1.0154 -0.2538  0.0005  0.0003  0.0003\n",
      " -2.4688 -1.7121 -1.2508 -1.0951 -1.1997 -0.2397  0.0003  0.0003  0.0003\n",
      "\n",
      "Columns 18 to 23 \n",
      "   0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  1.4158  0.8514  0.4094  0.0956  0.0003  0.0003\n",
      "  2.5072  1.4813  0.7461  0.2712  0.0003  0.0003\n",
      "  0.5044  0.2973 -0.0140  0.0637  0.0003  0.0003\n",
      " -1.2631 -1.4467 -1.1770 -0.5115  0.0003  0.0003\n",
      "  0.1153 -1.0048 -1.4121 -0.6493  0.0003  0.0003\n",
      "  1.1271 -0.1589 -0.7411 -0.2767  0.0003  0.0003\n",
      "  0.5045 -0.3921 -0.2867 -0.0238  0.0003  0.0003\n",
      " -0.3262 -0.6144 -0.2250  0.0031  0.0003  0.0003\n",
      " -0.4003 -0.4272 -0.0864  0.0003  0.0003  0.0003\n",
      " -0.1559  0.0602  0.0072  0.0003  0.0003  0.0003\n",
      " -0.3274 -0.0013  0.0003  0.0003  0.0003  0.0003\n",
      " -0.0814  0.0011  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0094  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "[torch.cuda.FloatTensor of size 1x24x24 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "经过max pooling后的输出的feature map第一层输出的数字结构:\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      "  0.0003  0.3459  1.2641  1.4342  0.8584  0.1778  0.0003  0.0003  0.0003\n",
      "  0.0003  1.7049  3.4035  3.3593  3.0194  3.9795  4.1746  4.2003  4.0163\n",
      "  0.0003 -0.2837 -1.9327 -2.2410 -1.2416 -0.3278  0.1512  0.1709  0.9390\n",
      "  0.0003 -0.1474  0.8413  0.4835  0.9623  1.2072  1.3234  2.7669  1.3915\n",
      "  0.0003  0.0003  0.0003  0.0003  0.1651  0.2870  1.4817  2.2377  0.3252\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0003  0.1355  1.1032 -0.1904  0.4010\n",
      "  0.0003  0.0003  0.0003  0.0003  0.0147  1.8462  1.4766  0.6226  0.4483\n",
      "  0.0003  0.0003  0.0003  0.0003  1.1635  2.0473  0.3657  0.0653 -0.1003\n",
      "  0.0003  0.0003  0.0003  0.2380  1.6976  1.2770  0.1699 -0.0824  0.0093\n",
      "  0.0003  0.0003  0.0003  1.3884  1.8402  1.0229  0.9409  0.0596  0.0058\n",
      "  0.0003  0.0003  0.0003  0.8167 -0.0447  0.3251 -0.6584  0.0005  0.0003\n",
      "\n",
      "Columns 9 to 11 \n",
      "   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003\n",
      "  2.5072  0.7461  0.0003\n",
      "  0.5044  0.0637  0.0003\n",
      "  1.1271 -0.2767  0.0003\n",
      "  0.5045  0.0031  0.0003\n",
      "  0.0602  0.0072  0.0003\n",
      "  0.0011  0.0003  0.0003\n",
      "  0.0094  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003\n",
      "[torch.cuda.FloatTensor of size 1x12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pooling层分析\n",
    "print(\"========================================\")\n",
    "data_2 = F.max_pool2d(data_1, 2)\n",
    "print(\"经过max pooling后的feature map数字结构:\")\n",
    "print(data_2)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"输入的feature map的第一层输入的数字结构:\")\n",
    "print(data_1[:,0,:,:])\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"经过max pooling后的输出的feature map第一层输出的数字结构:\")\n",
    "print(data_2[:,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max pooling详细分析\n",
    "（1）这里输入的feature map为 data_1, 其维度是（1x10x24x24）\n",
    "\n",
    "（2）max pooling的 kernel_size 为2\n",
    "\n",
    "（3）max pooling的 stride = 2, padding = 0(默认值)\n",
    "\n",
    "（3）max pooling不存在参数\n",
    "\n",
    "（4）可见输出的feature map 的维度是（1x10x12x12）\n",
    "\n",
    "#### max pooling输出feature map 大小计算公式:\n",
    "    输入：（N_0, C_0, H_0, W_0）\n",
    "    输出：（N_1, C_1, H_1, W_1）\n",
    "    有: W_1 = (W_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         H_1 = (H_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         N_1 = N_0\n",
    "         C_1 = C_0\n",
    "         \n",
    "#### 本例中输出的feature map 详细计算过程(以第一层输入输出为例)\n",
    "    可以看到输出的feature map中第一个输出值: data_2(1,1,1,1) = -0.0017\n",
    "    它是由：data_ori中左上角, 大小为(N_0 x C_0 x 2 x 2), 即\n",
    "    data_ori(:,:,0:2,0:2)=\n",
    "     [-0.0017 -0.0017 \n",
    "     -0.0017 -0.0017]\n",
    "    中取最大的数即 -0.0017 \n",
    "\n",
    "#### 总结输出的feature map 详细计算公式\n",
    "    已知输入data_1(N_0, C_0, H_0, W_0),  max_pooling(kernel_size, kernel_size)\n",
    "    求输出的data_2(n,c,h,w)其值为\n",
    "    data_ori(n, c, h×stride:h×stride+kernel_size, w×stride:w×stride+kernel_size)\n",
    "    中最大的值\n",
    "    \n",
    "#### 除此max pooling外, 还有average pooling, 将取最大改为求平均即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.3459  1.2641  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  1.7049  3.4035  ...   2.5072  0.7461  0.0003\n",
      "           ...             ⋱             ...          \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.7995  1.2142  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  2.7994  6.8383  ...   3.4475  0.0896  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.6293  1.3420  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  3.0415  3.3249  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.2709  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.5553  3.0084  ...   0.0000  0.0000  0.0566\n",
      "           ...             ⋱             ...          \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0075  0.0873  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  1.4562  1.4477  ...   0.2013  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.3921  0.0000  ...   0.6031  0.9827  0.9912\n",
      "           ...             ⋱             ...          \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# relu分析\n",
    "data_3 = F.relu(data_2)\n",
    "print(data_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可见relu的作用就是将所有的负值全变为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.3459  1.2641  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  1.7049  3.4035  ...   2.5072  0.7461  0.0003\n",
      "           ...             ⋱             ...          \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.7995  1.2142  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  2.7994  6.8383  ...   3.4475  0.0896  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.6293  1.3420  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  3.0415  3.3249  ...   0.0000  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.2709  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.5553  3.0084  ...   0.0000  0.0000  0.0566\n",
      "           ...             ⋱             ...          \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0075  0.0873  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  1.4562  1.4477  ...   0.2013  0.0000  0.0000\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.3921  0.0000  ...   0.6031  0.9827  0.9912\n",
      "           ...             ⋱             ...          \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      " 0.0003  0.3459  1.2641  1.4342  0.8584  0.1778  0.0003  0.0003  0.0003  0.0003\n",
      " 0.0003  1.7049  3.4035  3.3593  3.0194  3.9795  4.1746  4.2003  4.0163  2.5072\n",
      " 0.0003  0.0000  0.0000  0.0000  0.0000  0.0000  0.1512  0.1709  0.9390  0.5044\n",
      " 0.0003  0.0000  0.8413  0.4835  0.9623  1.2072  1.3234  2.7669  1.3915  1.1271\n",
      " 0.0003  0.0003  0.0003  0.0003  0.1651  0.2870  1.4817  2.2377  0.3252  0.5045\n",
      " 0.0003  0.0003  0.0003  0.0003  0.0003  0.1355  1.1032  0.0000  0.4010  0.0602\n",
      " 0.0003  0.0003  0.0003  0.0003  0.0147  1.8462  1.4766  0.6226  0.4483  0.0011\n",
      " 0.0003  0.0003  0.0003  0.0003  1.1635  2.0473  0.3657  0.0653  0.0000  0.0094\n",
      " 0.0003  0.0003  0.0003  0.2380  1.6976  1.2770  0.1699  0.0000  0.0093  0.0003\n",
      " 0.0003  0.0003  0.0003  1.3884  1.8402  1.0229  0.9409  0.0596  0.0058  0.0003\n",
      " 0.0003  0.0003  0.0003  0.8167  0.0000  0.3251  0.0000  0.0005  0.0003  0.0003\n",
      "\n",
      "Columns 10 to 11 \n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.7461  0.0003\n",
      " 0.0637  0.0003\n",
      " 0.0000  0.0003\n",
      " 0.0031  0.0003\n",
      " 0.0072  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      "[torch.cuda.FloatTensor of size 12x12 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003  0.0003\n",
      " 0.0003  0.3459  1.2641  1.4342  0.8584  0.1778  0.0003  0.0003  0.0003  0.0003\n",
      " 0.0003  1.7049  3.4035  3.3593  3.0194  3.9795  4.1746  4.2003  4.0163  2.5072\n",
      " 0.0003  0.0000  0.0000  0.0000  0.0000  0.0000  0.1512  0.1709  0.9390  0.5044\n",
      " 0.0003  0.0000  0.8413  0.4835  0.9623  1.2072  1.3234  2.7669  1.3915  1.1271\n",
      " 0.0003  0.0003  0.0003  0.0003  0.1651  0.2870  1.4817  2.2377  0.3252  0.5045\n",
      " 0.0003  0.0003  0.0003  0.0003  0.0003  0.1355  1.1032  0.0000  0.4010  0.0602\n",
      " 0.0003  0.0003  0.0003  0.0003  0.0147  1.8462  1.4766  0.6226  0.4483  0.0011\n",
      " 0.0003  0.0003  0.0003  0.0003  1.1635  2.0473  0.3657  0.0653  0.0000  0.0094\n",
      " 0.0003  0.0003  0.0003  0.2380  1.6976  1.2770  0.1699  0.0000  0.0093  0.0003\n",
      " 0.0003  0.0003  0.0003  1.3884  1.8402  1.0229  0.9409  0.0596  0.0058  0.0003\n",
      " 0.0003  0.0003  0.0003  0.8167  0.0000  0.3251  0.0000  0.0005  0.0003  0.0003\n",
      "\n",
      "Columns 10 to 11 \n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.7461  0.0003\n",
      " 0.0637  0.0003\n",
      " 0.0000  0.0003\n",
      " 0.0031  0.0003\n",
      " 0.0072  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      " 0.0003  0.0003\n",
      "[torch.cuda.FloatTensor of size 12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropout分析\n",
    "data_4 = model.conv2_drop(data_3)\n",
    "print(data_4)\n",
    "#输出dropout 前后 data变化\n",
    "print(data_3[0][0])\n",
    "print(data_4[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以看到dropout对 feature map的值是不发生作用的, 发生作用的地方是在做反向传播的时候, 随机选取50%的参数不做更新, 这里的参数不是data, 而是weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  1.6815  4.6110\n",
      "  1.2361  3.9675  2.0352  5.8581\n",
      "  3.2634  4.4673  2.9598  4.5029\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      "  1.0545  0.0000  0.0000  0.0000\n",
      "  0.2465  0.0000  0.0000  0.8128\n",
      "  0.4452  0.1852  3.3923  4.4066\n",
      "  0.0000  2.0604  4.2397  3.2048\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      "  2.2066  0.3674  3.5227  4.8809\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0948  4.4055  4.3974  0.0000\n",
      "  1.9329  5.4394  1.6720  0.1013\n",
      "\n",
      "(0 ,3 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  1.5951  2.0995  6.3302  3.5979\n",
      "  0.0000  2.3259  4.6016  0.0000\n",
      "  0.0000  3.9489  4.3360  0.0000\n",
      "\n",
      "(0 ,4 ,.,.) = \n",
      "  4.3615  3.3015  2.9820  3.5302\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.2144  0.0000  0.9554  0.0000\n",
      "  0.5585  0.0000  1.6227  0.0000\n",
      "\n",
      "(0 ,5 ,.,.) = \n",
      "  7.3242  7.5375  5.5115  0.8173\n",
      "  0.5960  0.0000  0.0000  0.1460\n",
      "  0.0000  0.0000  0.0000  0.2680\n",
      "  0.6586  0.0000  0.0000  2.0573\n",
      "\n",
      "(0 ,6 ,.,.) = \n",
      "  2.3362  4.6414  5.0062  1.0336\n",
      "  1.4273  2.4007  0.5674  0.0000\n",
      "  0.0130  0.0543  1.1772  0.3561\n",
      "  0.0000  0.0000  1.0064  0.1363\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.3975\n",
      "  0.0000  0.0000  3.2507  5.8625\n",
      "  0.0000  0.0000  4.5867  3.7469\n",
      "  0.0000  4.1404  4.5831  0.0000\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.1123\n",
      "  0.0000  0.0000  0.0000  4.9171\n",
      "  0.0000  0.0000  3.5024  6.2392\n",
      "  0.0000  0.3333  6.4441  4.9429\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  3.7727  3.8011  3.0660  0.0842\n",
      "  4.0260  3.6661  1.8038  0.0058\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,10,.,.) = \n",
      "  7.0592  8.4039  6.3423  4.0258\n",
      "  1.7813  3.9914  3.4580  0.8867\n",
      "  0.1211  0.0000  0.0000  0.4575\n",
      "  0.0000  0.0000  0.0000  1.1513\n",
      "\n",
      "(0 ,11,.,.) = \n",
      "  5.2273  4.7793  2.9211  0.0000\n",
      "  2.1830  0.1572  0.0000  0.0000\n",
      "  0.0000  0.0000  1.0324  1.8854\n",
      "  0.0000  0.0000  1.6706  0.6881\n",
      "\n",
      "(0 ,12,.,.) = \n",
      "  0.0000  0.0000  2.4942  4.9140\n",
      "  0.0000  0.0000  3.5920  5.1732\n",
      "  0.0000  0.0000  1.4358  0.0000\n",
      "  0.0000  1.6858  0.0000  0.0000\n",
      "\n",
      "(0 ,13,.,.) = \n",
      "  0.4322  2.5688  5.1350  4.9783\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.2333  1.6857  0.0000  0.1821\n",
      "  2.0979  1.2090  0.3106  0.0473\n",
      "\n",
      "(0 ,14,.,.) = \n",
      "  2.5128  3.7576  4.2864  4.0404\n",
      "  0.2837  0.0000  0.7147  0.0000\n",
      "  0.0000  2.9017  0.5214  0.0000\n",
      "  3.7543  2.9190  0.0000  0.0000\n",
      "\n",
      "(0 ,15,.,.) = \n",
      "  4.0410  4.8875  6.2223  0.0000\n",
      "  3.1417  6.0685  7.4242  0.6214\n",
      "  0.9441  1.1414  0.0000  0.0150\n",
      "  1.7135  1.8013  0.0000  0.0000\n",
      "\n",
      "(0 ,16,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  0.0000  6.2175  6.6947  0.9218\n",
      "  2.4243  8.1030  0.7778  1.9176\n",
      "\n",
      "(0 ,17,.,.) = \n",
      "  2.9951  1.9605  2.3377  0.7657\n",
      "  0.0062  0.0000  0.0000  0.3296\n",
      "  0.0000  0.0000  0.0000  0.2193\n",
      "  0.0000  0.0000  0.0000  0.0422\n",
      "\n",
      "(0 ,18,.,.) = \n",
      "  0.0000  0.0000  0.0000  0.0000\n",
      "  5.7041  6.0541  5.0156  0.0000\n",
      "  0.0000  0.3210  0.0000  0.0000\n",
      "  0.0549  0.0000  0.0000  0.0000\n",
      "\n",
      "(0 ,19,.,.) = \n",
      "  2.8676  3.3154  1.9333  1.3696\n",
      "  2.1568  0.1766  0.0000  0.4081\n",
      "  0.0000  0.0000  0.0000  2.6994\n",
      "  0.0000  0.0000  1.1534  2.3494\n",
      "[torch.cuda.FloatTensor of size 1x20x4x4 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  1.6815  4.6110  1.2361  3.9675\n",
      "\n",
      "Columns 10 to 19 \n",
      " 2.0352  5.8581  3.2634  4.4673  2.9598  4.5029  1.0545  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 20 to 29 \n",
      " 0.2465  0.0000  0.0000  0.8128  0.4452  0.1852  3.3923  4.4066  0.0000  2.0604\n",
      "\n",
      "Columns 30 to 39 \n",
      " 4.2397  3.2048  2.2066  0.3674  3.5227  4.8809  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 40 to 49 \n",
      " 0.0948  4.4055  4.3974  0.0000  1.9329  5.4394  1.6720  0.1013  0.0000  0.0000\n",
      "\n",
      "Columns 50 to 59 \n",
      " 0.0000  0.0000  1.5951  2.0995  6.3302  3.5979  0.0000  2.3259  4.6016  0.0000\n",
      "\n",
      "Columns 60 to 69 \n",
      " 0.0000  3.9489  4.3360  0.0000  4.3615  3.3015  2.9820  3.5302  0.0000  0.0000\n",
      "\n",
      "Columns 70 to 79 \n",
      " 0.0000  0.0000  0.2144  0.0000  0.9554  0.0000  0.5585  0.0000  1.6227  0.0000\n",
      "\n",
      "Columns 80 to 89 \n",
      " 7.3242  7.5375  5.5115  0.8173  0.5960  0.0000  0.0000  0.1460  0.0000  0.0000\n",
      "\n",
      "Columns 90 to 99 \n",
      " 0.0000  0.2680  0.6586  0.0000  0.0000  2.0573  2.3362  4.6414  5.0062  1.0336\n",
      "\n",
      "Columns 100 to 109 \n",
      " 1.4273  2.4007  0.5674  0.0000  0.0130  0.0543  1.1772  0.3561  0.0000  0.0000\n",
      "\n",
      "Columns 110 to 119 \n",
      " 1.0064  0.1363  0.0000  0.0000  0.0000  0.3975  0.0000  0.0000  3.2507  5.8625\n",
      "\n",
      "Columns 120 to 129 \n",
      " 0.0000  0.0000  4.5867  3.7469  0.0000  4.1404  4.5831  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 130 to 139 \n",
      " 0.0000  0.1123  0.0000  0.0000  0.0000  4.9171  0.0000  0.0000  3.5024  6.2392\n",
      "\n",
      "Columns 140 to 149 \n",
      " 0.0000  0.3333  6.4441  4.9429  3.7727  3.8011  3.0660  0.0842  4.0260  3.6661\n",
      "\n",
      "Columns 150 to 159 \n",
      " 1.8038  0.0058  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 160 to 169 \n",
      " 7.0592  8.4039  6.3423  4.0258  1.7813  3.9914  3.4580  0.8867  0.1211  0.0000\n",
      "\n",
      "Columns 170 to 179 \n",
      " 0.0000  0.4575  0.0000  0.0000  0.0000  1.1513  5.2273  4.7793  2.9211  0.0000\n",
      "\n",
      "Columns 180 to 189 \n",
      " 2.1830  0.1572  0.0000  0.0000  0.0000  0.0000  1.0324  1.8854  0.0000  0.0000\n",
      "\n",
      "Columns 190 to 199 \n",
      " 1.6706  0.6881  0.0000  0.0000  2.4942  4.9140  0.0000  0.0000  3.5920  5.1732\n",
      "\n",
      "Columns 200 to 209 \n",
      " 0.0000  0.0000  1.4358  0.0000  0.0000  1.6858  0.0000  0.0000  0.4322  2.5688\n",
      "\n",
      "Columns 210 to 219 \n",
      " 5.1350  4.9783  0.0000  0.0000  0.0000  0.0000  0.2333  1.6857  0.0000  0.1821\n",
      "\n",
      "Columns 220 to 229 \n",
      " 2.0979  1.2090  0.3106  0.0473  2.5128  3.7576  4.2864  4.0404  0.2837  0.0000\n",
      "\n",
      "Columns 230 to 239 \n",
      " 0.7147  0.0000  0.0000  2.9017  0.5214  0.0000  3.7543  2.9190  0.0000  0.0000\n",
      "\n",
      "Columns 240 to 249 \n",
      " 4.0410  4.8875  6.2223  0.0000  3.1417  6.0685  7.4242  0.6214  0.9441  1.1414\n",
      "\n",
      "Columns 250 to 259 \n",
      " 0.0000  0.0150  1.7135  1.8013  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 260 to 269 \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  6.2175  6.6947  0.9218  2.4243  8.1030\n",
      "\n",
      "Columns 270 to 279 \n",
      " 0.7778  1.9176  2.9951  1.9605  2.3377  0.7657  0.0062  0.0000  0.0000  0.3296\n",
      "\n",
      "Columns 280 to 289 \n",
      " 0.0000  0.0000  0.0000  0.2193  0.0000  0.0000  0.0000  0.0422  0.0000  0.0000\n",
      "\n",
      "Columns 290 to 299 \n",
      " 0.0000  0.0000  5.7041  6.0541  5.0156  0.0000  0.0000  0.3210  0.0000  0.0000\n",
      "\n",
      "Columns 300 to 309 \n",
      " 0.0549  0.0000  0.0000  0.0000  2.8676  3.3154  1.9333  1.3696  2.1568  0.1766\n",
      "\n",
      "Columns 310 to 319 \n",
      " 0.0000  0.4081  0.0000  0.0000  0.0000  2.6994  0.0000  0.0000  1.1534  2.3494\n",
      "[torch.cuda.FloatTensor of size 1x320 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view函数的解释, 作用相当于reshape\n",
    "data_5 = F.relu(F.max_pool2d(model.conv2_drop(model.conv2(data_3)), 2))\n",
    "data_6 = data_5.view(-1, 320)\n",
    "print(data_5)\n",
    "print(data_6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view函数的详细解释(reshap)\n",
    "    (1)可以看到 data_5的 大小为(1x20x4x4)\n",
    "    (2)可以看到 data_6的 大小为(1x320)\n",
    "    \n",
    "    对于data_5.view(-1, 320) 这里-1代表的意思是如果一共有参数n个,第二维度大小为320,第一维度的大小就是n/320,\n",
    "    而这里参数个数一共有20*4*4=320个, 于是-1实际代表的是1\n",
    "    作用就是,将输入由四维大小(1,20,4,4)转化为二维大小(1,320)\n",
    "    \n",
    "#### 为什么要做这个操作?\n",
    "    因为在这后面要做的是全连接,即构造线性函数,每一个参数与之前的feature map的值都要相乘并加上对应的偏置项,\n",
    "    于是在做全连接之前,先把feature 拉成线性的, 根据计算知道这里feature map一共有320个值\n",
    "    这就是为什么view(-1, 320) 为什么取320而不取其他数,因为是算出来的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.0334 -0.0678  0.0335  ...   0.0095 -0.0122 -0.0236\n",
      "-0.0411  0.0857  0.0504  ...   0.0513  0.0570  0.0257\n",
      " 0.0102  0.0158  0.0261  ...  -0.0146 -0.0059  0.0315\n",
      "          ...             ⋱             ...          \n",
      " 0.0054 -0.1444 -0.0956  ...   0.0399 -0.0045 -0.0050\n",
      " 0.0519  0.0504  0.1271  ...  -0.0212  0.0465 -0.0374\n",
      "-0.1035 -0.0821  0.0782  ...  -0.0410  0.0097 -0.0393\n",
      "[torch.cuda.FloatTensor of size 50x320 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -2.1719\n",
      "  1.0995\n",
      " -3.3994\n",
      "  0.0892\n",
      "  5.1646\n",
      "  5.2229\n",
      "  7.0599\n",
      "  4.4584\n",
      " -2.3070\n",
      "  2.8873\n",
      " -3.1326\n",
      "  1.3012\n",
      " -0.6112\n",
      "  0.0461\n",
      "  3.1146\n",
      "  4.2107\n",
      "  3.2212\n",
      "  1.8483\n",
      "  0.5866\n",
      "  0.8990\n",
      "  7.3848\n",
      "  2.3017\n",
      " -2.5158\n",
      "  0.3531\n",
      "  7.6745\n",
      "  5.0921\n",
      "  6.5559\n",
      "  8.2033\n",
      "  7.0236\n",
      "  4.2666\n",
      " -0.8369\n",
      "  9.8231\n",
      "  3.0000\n",
      "  6.0697\n",
      "  4.4512\n",
      " -2.3542\n",
      "  5.4903\n",
      "  5.3761\n",
      "  0.8119\n",
      "  6.4712\n",
      "  4.2997\n",
      " -2.1138\n",
      " -1.8451\n",
      "  1.7010\n",
      "  3.1492\n",
      "  4.3697\n",
      "  0.3251\n",
      "  7.4496\n",
      "  7.7825\n",
      "  3.7520\n",
      "[torch.cuda.FloatTensor of size 50 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      " -1.1836  -8.3233   5.7096   9.1743   6.0700  -9.0474  -7.8866   6.0780\n",
      "\n",
      "Columns 8 to 15 \n",
      "  7.9732   6.3546  -4.7608  -9.1769  -4.6036  -5.1507  -9.3697  -7.1238\n",
      "\n",
      "Columns 16 to 23 \n",
      " -0.6651  -6.0349   7.0120  -2.1944  -7.7738  -5.8867 -10.1517  -0.9673\n",
      "\n",
      "Columns 24 to 31 \n",
      " -4.5028   8.4095  -4.3912 -14.0230  -5.2370  -4.2366   8.6516   6.8563\n",
      "\n",
      "Columns 32 to 39 \n",
      "  0.7160  -8.8062   6.2388  -9.1938   4.9110  -3.0809   6.6668  -5.5027\n",
      "\n",
      "Columns 40 to 47 \n",
      "  5.0674 -17.1998   1.8685 -11.7509 -11.5658  -0.5682  -2.8263  -3.5582\n",
      "\n",
      "Columns 48 to 49 \n",
      " -9.4076   6.5743\n",
      "[torch.cuda.FloatTensor of size 1x50 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "\t fc详细计算过程, 以第一个输出为例:\n",
      "========================================\n",
      "data_6的值:\n",
      "Variable containing:\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.6815\n",
      " 4.6110\n",
      " 1.2361\n",
      " 3.9675\n",
      " 2.0352\n",
      " 5.8581\n",
      " 3.2634\n",
      " 4.4673\n",
      " 2.9598\n",
      " 4.5029\n",
      " 1.0545\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2465\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.8128\n",
      " 0.4452\n",
      " 0.1852\n",
      " 3.3923\n",
      " 4.4066\n",
      " 0.0000\n",
      " 2.0604\n",
      " 4.2397\n",
      " 3.2048\n",
      " 2.2066\n",
      " 0.3674\n",
      " 3.5227\n",
      " 4.8809\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0948\n",
      " 4.4055\n",
      " 4.3974\n",
      " 0.0000\n",
      " 1.9329\n",
      " 5.4394\n",
      " 1.6720\n",
      " 0.1013\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.5951\n",
      " 2.0995\n",
      " 6.3302\n",
      " 3.5979\n",
      " 0.0000\n",
      " 2.3259\n",
      " 4.6016\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.9489\n",
      " 4.3360\n",
      " 0.0000\n",
      " 4.3615\n",
      " 3.3015\n",
      " 2.9820\n",
      " 3.5302\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2144\n",
      " 0.0000\n",
      " 0.9554\n",
      " 0.0000\n",
      " 0.5585\n",
      " 0.0000\n",
      " 1.6227\n",
      " 0.0000\n",
      " 7.3242\n",
      " 7.5375\n",
      " 5.5115\n",
      " 0.8173\n",
      " 0.5960\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.1460\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2680\n",
      " 0.6586\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.0573\n",
      " 2.3362\n",
      " 4.6414\n",
      " 5.0062\n",
      " 1.0336\n",
      " 1.4273\n",
      " 2.4007\n",
      " 0.5674\n",
      " 0.0000\n",
      " 0.0130\n",
      " 0.0543\n",
      " 1.1772\n",
      " 0.3561\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.0064\n",
      " 0.1363\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.3975\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.2507\n",
      " 5.8625\n",
      " 0.0000\n",
      " 0.0000\n",
      " 4.5867\n",
      " 3.7469\n",
      " 0.0000\n",
      " 4.1404\n",
      " 4.5831\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.1123\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 4.9171\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.5024\n",
      " 6.2392\n",
      " 0.0000\n",
      " 0.3333\n",
      " 6.4441\n",
      " 4.9429\n",
      " 3.7727\n",
      " 3.8011\n",
      " 3.0660\n",
      " 0.0842\n",
      " 4.0260\n",
      " 3.6661\n",
      " 1.8038\n",
      " 0.0058\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 7.0592\n",
      " 8.4039\n",
      " 6.3423\n",
      " 4.0258\n",
      " 1.7813\n",
      " 3.9914\n",
      " 3.4580\n",
      " 0.8867\n",
      " 0.1211\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.4575\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.1513\n",
      " 5.2273\n",
      " 4.7793\n",
      " 2.9211\n",
      " 0.0000\n",
      " 2.1830\n",
      " 0.1572\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.0324\n",
      " 1.8854\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.6706\n",
      " 0.6881\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.4942\n",
      " 4.9140\n",
      " 0.0000\n",
      " 0.0000\n",
      " 3.5920\n",
      " 5.1732\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.4358\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.6858\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.4322\n",
      " 2.5688\n",
      " 5.1350\n",
      " 4.9783\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2333\n",
      " 1.6857\n",
      " 0.0000\n",
      " 0.1821\n",
      " 2.0979\n",
      " 1.2090\n",
      " 0.3106\n",
      " 0.0473\n",
      " 2.5128\n",
      " 3.7576\n",
      " 4.2864\n",
      " 4.0404\n",
      " 0.2837\n",
      " 0.0000\n",
      " 0.7147\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.9017\n",
      " 0.5214\n",
      " 0.0000\n",
      " 3.7543\n",
      " 2.9190\n",
      " 0.0000\n",
      " 0.0000\n",
      " 4.0410\n",
      " 4.8875\n",
      " 6.2223\n",
      " 0.0000\n",
      " 3.1417\n",
      " 6.0685\n",
      " 7.4242\n",
      " 0.6214\n",
      " 0.9441\n",
      " 1.1414\n",
      " 0.0000\n",
      " 0.0150\n",
      " 1.7135\n",
      " 1.8013\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 6.2175\n",
      " 6.6947\n",
      " 0.9218\n",
      " 2.4243\n",
      " 8.1030\n",
      " 0.7778\n",
      " 1.9176\n",
      " 2.9951\n",
      " 1.9605\n",
      " 2.3377\n",
      " 0.7657\n",
      " 0.0062\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.3296\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.2193\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0422\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 5.7041\n",
      " 6.0541\n",
      " 5.0156\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.3210\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0549\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.8676\n",
      " 3.3154\n",
      " 1.9333\n",
      " 1.3696\n",
      " 2.1568\n",
      " 0.1766\n",
      " 0.0000\n",
      " 0.4081\n",
      " 0.0000\n",
      " 0.0000\n",
      " 0.0000\n",
      " 2.6994\n",
      " 0.0000\n",
      " 0.0000\n",
      " 1.1534\n",
      " 2.3494\n",
      "[torch.cuda.FloatTensor of size 320 (GPU 0)]\n",
      "\n",
      "第一层weight值:\n",
      "Variable containing:\n",
      "-0.0334\n",
      "-0.0678\n",
      " 0.0335\n",
      "-0.0301\n",
      " 0.0574\n",
      "-0.0756\n",
      "-0.0261\n",
      " 0.0510\n",
      "-0.0622\n",
      "-0.0852\n",
      " 0.0596\n",
      " 0.0480\n",
      "-0.0808\n",
      "-0.0583\n",
      " 0.0406\n",
      " 0.0432\n",
      " 0.0017\n",
      " 0.0233\n",
      "-0.0671\n",
      "-0.0337\n",
      " 0.0719\n",
      " 0.0375\n",
      "-0.0170\n",
      "-0.0486\n",
      "-0.0130\n",
      " 0.1808\n",
      " 0.0879\n",
      "-0.0597\n",
      "-0.1559\n",
      "-0.1452\n",
      "-0.0287\n",
      " 0.0429\n",
      "-0.0352\n",
      " 0.0362\n",
      " 0.0474\n",
      "-0.0562\n",
      "-0.0807\n",
      " 0.0869\n",
      "-0.0197\n",
      "-0.0188\n",
      "-0.1136\n",
      "-0.1080\n",
      "-0.0511\n",
      " 0.0642\n",
      " 0.1178\n",
      " 0.0663\n",
      " 0.0316\n",
      " 0.0090\n",
      "-0.0019\n",
      "-0.0084\n",
      "-0.0019\n",
      " 0.0226\n",
      "-0.0355\n",
      " 0.0055\n",
      "-0.0475\n",
      "-0.0291\n",
      "-0.0026\n",
      " 0.0771\n",
      " 0.0159\n",
      "-0.0441\n",
      "-0.0082\n",
      " 0.0792\n",
      " 0.0062\n",
      "-0.0540\n",
      " 0.0042\n",
      "-0.0011\n",
      " 0.0280\n",
      " 0.0010\n",
      "-0.0347\n",
      "-0.0099\n",
      " 0.0111\n",
      " 0.0338\n",
      "-0.0719\n",
      "-0.0307\n",
      "-0.0894\n",
      " 0.0524\n",
      " 0.0956\n",
      " 0.0814\n",
      "-0.0402\n",
      "-0.0028\n",
      " 0.0177\n",
      "-0.0116\n",
      "-0.0445\n",
      " 0.0922\n",
      " 0.0578\n",
      " 0.0757\n",
      " 0.0079\n",
      "-0.0279\n",
      " 0.1018\n",
      "-0.0199\n",
      "-0.1096\n",
      "-0.0753\n",
      " 0.0611\n",
      "-0.0396\n",
      "-0.0105\n",
      "-0.0666\n",
      " 0.0348\n",
      " 0.0467\n",
      " 0.0063\n",
      " 0.0291\n",
      "-0.0080\n",
      " 0.0252\n",
      " 0.0079\n",
      "-0.0512\n",
      " 0.0233\n",
      " 0.1040\n",
      " 0.0045\n",
      "-0.1623\n",
      " 0.0230\n",
      " 0.0744\n",
      "-0.0061\n",
      "-0.1126\n",
      "-0.0161\n",
      " 0.0286\n",
      "-0.0129\n",
      "-0.0152\n",
      " 0.0321\n",
      " 0.0785\n",
      " 0.0169\n",
      "-0.0576\n",
      "-0.1079\n",
      "-0.0702\n",
      "-0.0032\n",
      "-0.0304\n",
      "-0.1499\n",
      " 0.0302\n",
      " 0.0612\n",
      " 0.0944\n",
      "-0.0057\n",
      " 0.0051\n",
      " 0.0207\n",
      "-0.0576\n",
      " 0.0580\n",
      " 0.0211\n",
      " 0.0744\n",
      "-0.0218\n",
      "-0.1735\n",
      "-0.1386\n",
      "-0.0092\n",
      "-0.0543\n",
      "-0.0727\n",
      "-0.0194\n",
      " 0.1220\n",
      " 0.0187\n",
      "-0.0351\n",
      "-0.0372\n",
      " 0.0054\n",
      "-0.0314\n",
      "-0.0885\n",
      " 0.0521\n",
      " 0.0406\n",
      " 0.0217\n",
      "-0.0140\n",
      " 0.1496\n",
      " 0.0776\n",
      "-0.0533\n",
      "-0.0323\n",
      " 0.0340\n",
      "-0.0043\n",
      "-0.0892\n",
      " 0.0042\n",
      " 0.0294\n",
      " 0.0326\n",
      " 0.0435\n",
      "-0.0038\n",
      "-0.0041\n",
      "-0.0439\n",
      " 0.0251\n",
      " 0.0124\n",
      " 0.0392\n",
      " 0.0010\n",
      "-0.0298\n",
      " 0.0141\n",
      " 0.0569\n",
      "-0.0343\n",
      "-0.0830\n",
      " 0.0160\n",
      " 0.0414\n",
      "-0.0100\n",
      " 0.0096\n",
      " 0.0127\n",
      " 0.0282\n",
      " 0.0362\n",
      "-0.0639\n",
      " 0.0067\n",
      " 0.0550\n",
      "-0.0687\n",
      "-0.1281\n",
      "-0.0229\n",
      "-0.0601\n",
      " 0.0024\n",
      "-0.0212\n",
      "-0.0932\n",
      "-0.0063\n",
      "-0.0237\n",
      "-0.0405\n",
      "-0.0048\n",
      "-0.0471\n",
      "-0.0296\n",
      " 0.0044\n",
      "-0.0309\n",
      " 0.0061\n",
      " 0.0378\n",
      " 0.0268\n",
      " 0.0114\n",
      " 0.0281\n",
      " 0.0375\n",
      "-0.0151\n",
      "-0.0716\n",
      "-0.0569\n",
      " 0.0209\n",
      "-0.0169\n",
      "-0.0088\n",
      " 0.0193\n",
      " 0.0182\n",
      "-0.0314\n",
      "-0.0470\n",
      " 0.0615\n",
      "-0.0304\n",
      "-0.0570\n",
      " 0.0664\n",
      " 0.0645\n",
      " 0.0740\n",
      " 0.0191\n",
      "-0.0627\n",
      " 0.0108\n",
      "-0.0374\n",
      "-0.0135\n",
      " 0.0337\n",
      "-0.0045\n",
      " 0.0132\n",
      "-0.0023\n",
      "-0.0817\n",
      "-0.0655\n",
      "-0.0999\n",
      " 0.0348\n",
      " 0.1182\n",
      " 0.1218\n",
      " 0.0591\n",
      " 0.0250\n",
      " 0.0063\n",
      " 0.0165\n",
      "-0.0440\n",
      " 0.0275\n",
      "-0.0674\n",
      " 0.0115\n",
      "-0.0530\n",
      " 0.0269\n",
      "-0.1226\n",
      " 0.0663\n",
      " 0.0655\n",
      " 0.0188\n",
      " 0.0320\n",
      " 0.0392\n",
      " 0.0360\n",
      "-0.0383\n",
      " 0.0181\n",
      "-0.0201\n",
      "-0.0831\n",
      "-0.0101\n",
      " 0.0247\n",
      " 0.0759\n",
      "-0.0415\n",
      " 0.0262\n",
      "-0.0908\n",
      "-0.0279\n",
      "-0.0824\n",
      "-0.0165\n",
      "-0.0441\n",
      " 0.0659\n",
      "-0.0220\n",
      " 0.0666\n",
      "-0.0780\n",
      "-0.0018\n",
      " 0.0208\n",
      "-0.0440\n",
      "-0.0213\n",
      " 0.0881\n",
      " 0.0863\n",
      "-0.0115\n",
      " 0.0877\n",
      " 0.0400\n",
      "-0.0566\n",
      " 0.0279\n",
      "-0.0294\n",
      "-0.1040\n",
      "-0.0620\n",
      " 0.0103\n",
      " 0.0701\n",
      " 0.0205\n",
      " 0.0328\n",
      "-0.0250\n",
      "-0.0369\n",
      " 0.0016\n",
      "-0.0369\n",
      " 0.1038\n",
      "-0.0238\n",
      " 0.0101\n",
      " 0.0371\n",
      "-0.0537\n",
      " 0.0162\n",
      " 0.0700\n",
      " 0.0195\n",
      "-0.0271\n",
      "-0.0317\n",
      " 0.0208\n",
      "-0.0357\n",
      " 0.0912\n",
      " 0.0206\n",
      " 0.1004\n",
      " 0.0484\n",
      " 0.0203\n",
      " 0.0119\n",
      " 0.0324\n",
      "-0.0293\n",
      "-0.0364\n",
      " 0.0315\n",
      " 0.0095\n",
      "-0.0122\n",
      "-0.0236\n",
      "[torch.cuda.FloatTensor of size 320 (GPU 0)]\n",
      "\n",
      "第一个bias值:\n",
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -2.1719\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "手动计算结果:\n",
      "Variable containing:\n",
      "-1.1836\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "自动计算结果对比:\n",
      "Variable containing:\n",
      "-1.1836\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fc 全连接分析\n",
    "data_7 = model.fc1(data_6)\n",
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)\n",
    "print(data_7)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"\\t fc详细计算过程, 以第一个输出为例:\")\n",
    "print(\"========================================\")\n",
    "print(\"data_6的值:\")\n",
    "print(data_6[0])\n",
    "\n",
    "print(\"第一层weight值:\")\n",
    "w_1 = model.fc1.weight[0]\n",
    "print(w_1)\n",
    "\n",
    "print(\"第一个bias值:\")\n",
    "b_1 = model.fc1.bias[0]\n",
    "print(b_1)\n",
    "\n",
    "print(\"手动计算结果:\")\n",
    "res=0\n",
    "for ind_i in range(len(w_1)):\n",
    "    res += data_6[0][ind_i] * w_1[ind_i]\n",
    "res +=b_1\n",
    "print(res)\n",
    "print(\"自动计算结果对比:\")\n",
    "print(data_7[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fc详细分析:\n",
    "    (1) 由定义有: fc1 = nn.Linear(320, 50) \n",
    "            其中 320表示输入大小,50为输出大小\n",
    "    (2) 输入feature map大小为 (1x320)\n",
    "    (3) 输出feature map大小为 (1x50)\n",
    "    (4) weight大小(50x320)\n",
    "    (5) bias大小(50)\n",
    " \n",
    "#### 本例中fc计算演示:\n",
    "    data_7[i][j] = data_6[i, :] * weight[j, :] + bias[j]\n",
    "    \n",
    "## 至此, 关键部分全部分析完, 感兴趣可以再研究下softmax\n",
    "#### 最后, 我们看下如何将Variable 转为numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "           ...             ⋱             ...          \n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "  0.0003  0.0003  0.0003  ...   0.0003  0.0003  0.0003\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      "           ...             ⋱             ...          \n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      " -0.2276 -0.2276 -0.2276  ...  -0.2276 -0.2276 -0.2276\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      "           ...             ⋱             ...          \n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      " -0.2452 -0.2452 -0.2452  ...  -0.2452 -0.2452 -0.2452\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "           ...             ⋱             ...          \n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "  0.0566  0.0566  0.0566  ...   0.0566  0.0566  0.0566\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      "           ...             ⋱             ...          \n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      " -0.5925 -0.5925 -0.5925  ...  -0.5925 -0.5925 -0.5925\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "           ...             ⋱             ...          \n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "  0.9912  0.9912  0.9912  ...   0.9912  0.9912  0.9912\n",
      "[torch.cuda.FloatTensor of size 1x10x24x24 (GPU 0)]\n",
      "\n",
      "[[[[  3.14906240e-04   3.14906240e-04   3.14906240e-04 ...,\n",
      "      3.14906240e-04   3.14906240e-04   3.14906240e-04]\n",
      "   [  3.14906240e-04   3.14906240e-04   3.14906240e-04 ...,\n",
      "      3.14906240e-04   3.14906240e-04   3.14906240e-04]\n",
      "   [  3.14906240e-04   3.14906240e-04   3.14906240e-04 ...,\n",
      "      3.14906240e-04   3.14906240e-04   3.14906240e-04]\n",
      "   ..., \n",
      "   [  3.14906240e-04   3.14906240e-04   3.14906240e-04 ...,\n",
      "      3.14906240e-04   3.14906240e-04   3.14906240e-04]\n",
      "   [  3.14906240e-04   3.14906240e-04   3.14906240e-04 ...,\n",
      "      3.14906240e-04   3.14906240e-04   3.14906240e-04]\n",
      "   [  3.14906240e-04   3.14906240e-04   3.14906240e-04 ...,\n",
      "      3.14906240e-04   3.14906240e-04   3.14906240e-04]]\n",
      "\n",
      "  [[ -2.27559015e-01  -2.27559015e-01  -2.27559015e-01 ...,\n",
      "     -2.27559015e-01  -2.27559015e-01  -2.27559015e-01]\n",
      "   [ -2.27559015e-01  -2.27559015e-01  -2.27559015e-01 ...,\n",
      "     -2.27559015e-01  -2.27559015e-01  -2.27559015e-01]\n",
      "   [ -2.27559015e-01  -2.27559015e-01  -2.27559015e-01 ...,\n",
      "     -2.27559015e-01  -2.27559015e-01  -2.27559015e-01]\n",
      "   ..., \n",
      "   [ -2.27559015e-01  -2.27559015e-01  -2.27559015e-01 ...,\n",
      "     -2.27559015e-01  -2.27559015e-01  -2.27559015e-01]\n",
      "   [ -2.27559015e-01  -2.27559015e-01  -2.27559015e-01 ...,\n",
      "     -2.27559015e-01  -2.27559015e-01  -2.27559015e-01]\n",
      "   [ -2.27559015e-01  -2.27559015e-01  -2.27559015e-01 ...,\n",
      "     -2.27559015e-01  -2.27559015e-01  -2.27559015e-01]]\n",
      "\n",
      "  [[ -2.45227188e-01  -2.45227188e-01  -2.45227188e-01 ...,\n",
      "     -2.45227188e-01  -2.45227188e-01  -2.45227188e-01]\n",
      "   [ -2.45227188e-01  -2.45227188e-01  -2.45227188e-01 ...,\n",
      "     -2.45227188e-01  -2.45227188e-01  -2.45227188e-01]\n",
      "   [ -2.45227188e-01  -2.45227188e-01  -2.45227188e-01 ...,\n",
      "     -2.45227188e-01  -2.45227188e-01  -2.45227188e-01]\n",
      "   ..., \n",
      "   [ -2.45227188e-01  -2.45227188e-01  -2.45227188e-01 ...,\n",
      "     -2.45227188e-01  -2.45227188e-01  -2.45227188e-01]\n",
      "   [ -2.45227188e-01  -2.45227188e-01  -2.45227188e-01 ...,\n",
      "     -2.45227188e-01  -2.45227188e-01  -2.45227188e-01]\n",
      "   [ -2.45227188e-01  -2.45227188e-01  -2.45227188e-01 ...,\n",
      "     -2.45227188e-01  -2.45227188e-01  -2.45227188e-01]]\n",
      "\n",
      "  ..., \n",
      "  [[  5.65648973e-02   5.65648973e-02   5.65648973e-02 ...,\n",
      "      5.65648973e-02   5.65648973e-02   5.65648973e-02]\n",
      "   [  5.65648973e-02   5.65648973e-02   5.65648973e-02 ...,\n",
      "      5.65648973e-02   5.65648973e-02   5.65648973e-02]\n",
      "   [  5.65648973e-02   5.65648973e-02   5.65648973e-02 ...,\n",
      "      5.65648973e-02   5.65648973e-02   5.65648973e-02]\n",
      "   ..., \n",
      "   [  5.65648973e-02   5.65648973e-02   5.65648973e-02 ...,\n",
      "      5.65648973e-02   5.65648973e-02   5.65648973e-02]\n",
      "   [  5.65648973e-02   5.65648973e-02   5.65648973e-02 ...,\n",
      "      5.65648973e-02   5.65648973e-02   5.65648973e-02]\n",
      "   [  5.65648973e-02   5.65648973e-02   5.65648973e-02 ...,\n",
      "      5.65648973e-02   5.65648973e-02   5.65648973e-02]]\n",
      "\n",
      "  [[ -5.92476845e-01  -5.92476845e-01  -5.92476845e-01 ...,\n",
      "     -5.92476845e-01  -5.92476845e-01  -5.92476845e-01]\n",
      "   [ -5.92476845e-01  -5.92476845e-01  -5.92476845e-01 ...,\n",
      "     -5.92476845e-01  -5.92476845e-01  -5.92476845e-01]\n",
      "   [ -5.92476845e-01  -5.92476845e-01  -5.92476845e-01 ...,\n",
      "     -5.92476845e-01  -5.92476845e-01  -5.92476845e-01]\n",
      "   ..., \n",
      "   [ -5.92476845e-01  -5.92476845e-01  -5.92476845e-01 ...,\n",
      "     -5.92476845e-01  -5.92476845e-01  -5.92476845e-01]\n",
      "   [ -5.92476845e-01  -5.92476845e-01  -5.92476845e-01 ...,\n",
      "     -5.92476845e-01  -5.92476845e-01  -5.92476845e-01]\n",
      "   [ -5.92476845e-01  -5.92476845e-01  -5.92476845e-01 ...,\n",
      "     -5.92476845e-01  -5.92476845e-01  -5.92476845e-01]]\n",
      "\n",
      "  [[  9.91199195e-01   9.91199195e-01   9.91199195e-01 ...,\n",
      "      9.91199195e-01   9.91199195e-01   9.91199195e-01]\n",
      "   [  9.91199195e-01   9.91199195e-01   9.91199195e-01 ...,\n",
      "      9.91199195e-01   9.91199195e-01   9.91199195e-01]\n",
      "   [  9.91199195e-01   9.91199195e-01   9.91199195e-01 ...,\n",
      "      9.91199195e-01   9.91199195e-01   9.91199195e-01]\n",
      "   ..., \n",
      "   [  9.91199195e-01   9.91199195e-01   9.91199195e-01 ...,\n",
      "      9.91199195e-01   9.91199195e-01   9.91199195e-01]\n",
      "   [  9.91199195e-01   9.91199195e-01   9.91199195e-01 ...,\n",
      "      9.91199195e-01   9.91199195e-01   9.91199195e-01]\n",
      "   [  9.91199195e-01   9.91199195e-01   9.91199195e-01 ...,\n",
      "      9.91199195e-01   9.91199195e-01   9.91199195e-01]]]]\n"
     ]
    }
   ],
   "source": [
    "print(data_1)\n",
    "out = data_1.data.cpu().numpy()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 恭喜!  结束了!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
