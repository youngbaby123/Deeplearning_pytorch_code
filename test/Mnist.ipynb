{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch 手写字体训练与检测  --  Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable    #自动求导的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#超参数配置\n",
    "batch_size=64  #单次训练导入数据个数\n",
    "test_batch_size=1000\n",
    "epochs=10  #训练时,每张图片训练次数\n",
    "lr=0.01   #学习率\n",
    "momentum=0.5    #动量\n",
    "no_cuda=False    #是否使用GPU, False表示使用\n",
    "seed=1\n",
    "log_interval=10\n",
    "cuda = not no_cuda and torch.cuda.is_available()  #当no_cuda=False并且cuda可用时使用GPU\n",
    "\n",
    "#设置随机种子, 保证训练时初始化参数一值, 用以作对比\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "#下载 训练以及检测数据\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造网络结构, 可以看到构造了2个隐层\n",
    "#\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)    # 卷积层, 1为输入的channel, 10为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)    # 卷积层, 10为输入的channel, 20为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2_drop = nn.Dropout2d()    # Dropout层 \n",
    "        self.fc1 = nn.Linear(320, 50)  #线性函数,输入大小为320个神经元, 输出为50个神经元\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    #前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对forward的相关解释:\n",
    "(1)  x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "\n",
    "该操作拆开来包含三个基本操作:\n",
    "\n",
    "    1).  self.conv1(x) : 对输入的x做卷积操作,卷积大小为之前定义的 self.conv1\n",
    "\n",
    "    2).  F.max_pool2d() : max pooling操作 参数2表示 filter size\n",
    "\n",
    "    3).  F.relu() : relu函数, 用以非线性化 (备注 relu(x) = max( x , 0) )\n",
    "    \n",
    "(2)  x.view(-1, 320) \n",
    "\n",
    "    类似reshape操作, 将第二维度转化为320, 第一维随之变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#训练与检测的函数定义\n",
    "#在后面会详细分析\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.390087\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.350225\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.288934\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279396\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272692\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.276944\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.233707\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.222649\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181699\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.159016\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055779\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.065889\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.916480\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.910268\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.704504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.554525\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.621117\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.521305\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.533543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.339470\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.382869\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.338094\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.271376\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.084347\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.204976\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.879644\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.776240\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.967955\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.913667\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.190108\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.847057\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.893181\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.072115\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.982275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.725554\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.858674\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.810363\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.785607\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.735104\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584800\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.722361\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.745441\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.510521\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.767265\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.596351\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.570569\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.742379\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.806328\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.818956\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.650601\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.857444\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.441356\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.674468\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.589339\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.547562\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.502929\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.498676\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.514354\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.683679\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.760724\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.780452\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.788970\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.711024\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.615807\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.835485\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.478996\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.693961\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.616784\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.804852\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.475907\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.514661\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.536382\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.306062\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.494869\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.655255\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.447693\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.339177\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.501230\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.539886\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.361920\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.595520\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.589029\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.436210\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.515586\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.577152\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.378896\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.694943\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.516973\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.531180\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.404211\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.382927\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.312191\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.555723\n",
      "\n",
      "Test set: Average loss: 0.2045, Accuracy: 9411/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.482963\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.540409\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.888776\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.479911\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.331446\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.282954\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.241638\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.397932\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.307764\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.309925\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.340144\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.357300\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.396121\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.381501\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.603631\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.376141\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.510827\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.549115\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.303920\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.383781\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.332750\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.332781\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.546738\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.360727\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.613650\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.672101\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.296241\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.208624\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.501349\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.437223\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.412267\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.305260\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.378011\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.206897\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.450698\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.432614\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.297219\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.344567\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.354139\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.530652\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.488007\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.574334\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.377289\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.315710\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.404866\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.219381\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.589289\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.477881\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.283155\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.336600\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.384248\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.318843\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.320776\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.393156\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.500240\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.336879\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.290465\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.576536\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.329086\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.309050\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.379384\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.408451\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.368441\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.228924\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.320052\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.474999\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.273678\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.301360\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.350196\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.451958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.460257\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.383947\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.446170\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.270974\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.176238\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.366832\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.305981\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.437950\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.441521\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.382037\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.314038\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.313957\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.243404\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.382839\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.227108\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.323998\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.594506\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.353640\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.567365\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.513594\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.307632\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.276574\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.286016\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.550669\n",
      "\n",
      "Test set: Average loss: 0.1271, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.244726\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.377229\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.182524\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.232277\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.433053\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.286238\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.348580\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.328114\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.243544\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.356603\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.349173\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.225375\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.382064\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.248156\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.342567\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.477455\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.305342\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.367537\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.379717\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.180241\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.293135\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.152118\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.356929\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.392745\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.259510\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.293468\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.278100\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.181546\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.233645\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.454868\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.274238\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.404012\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.361588\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.497382\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.408341\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.317685\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.381138\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.335469\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.372553\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.364072\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.136939\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.246330\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.302804\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.256975\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.272221\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.258216\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.267893\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.277976\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.536351\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.324018\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.341157\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.280326\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.295445\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.187201\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.403235\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.140075\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.279113\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.400142\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.229258\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.380720\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.146427\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.244362\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.284263\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.288335\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.337595\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.305314\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.312865\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.254344\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.502342\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.361663\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.560820\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.308705\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.421139\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.325064\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.220075\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.191842\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.274433\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.328376\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.314249\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.164637\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.273880\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.178951\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.246356\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.424897\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.210615\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.166817\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.449913\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.444375\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.273248\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.239431\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.334471\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.137796\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.427027\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.190735\n",
      "\n",
      "Test set: Average loss: 0.0991, Accuracy: 9692/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.240001\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.209796\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.250792\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.317354\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.311744\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.332502\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.412302\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.215945\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.365816\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.206914\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.142898\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.426490\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.247585\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.345553\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.324081\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.220968\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.301214\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.290254\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.230801\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.362256\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.192307\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.218795\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.443133\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.489705\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.223200\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.136741\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.378204\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.445922\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.269286\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.209497\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.144474\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.185186\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.112941\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.294957\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.418573\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.296275\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.305515\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.278658\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.298916\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.376843\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.211499\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.347074\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.340614\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.283923\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.362985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.312022\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.248610\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.189675\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.360281\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.312034\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.204554\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.420910\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.269933\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.140063\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.211283\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.308067\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.139301\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.317885\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.371763\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.245708\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.135173\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.212181\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.418540\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.098431\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.084603\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.232459\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.133524\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.228192\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.227377\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.298516\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.588939\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.313169\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.073916\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.206633\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.367585\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.249774\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.155893\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.484742\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.203135\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.202597\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.100512\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.156164\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.447568\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.169493\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.175241\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.089534\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.206623\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.371025\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.278024\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.201367\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.396512\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.419460\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.126084\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.219539\n",
      "\n",
      "Test set: Average loss: 0.0869, Accuracy: 9725/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.262824\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.230022\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.245661\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.211292\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.123347\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.160367\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.187629\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.195330\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.133306\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.350655\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.220135\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.193299\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.235528\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.226026\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.221732\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.197797\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.257245\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.146776\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.236096\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.450244\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.497113\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.241283\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.114341\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.201125\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.176115\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.163433\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.186995\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.294751\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.138806\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.193099\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.288951\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.261705\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.264201\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.161865\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.174338\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.139812\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.217070\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.265579\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.435753\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.168440\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.119014\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.155716\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.207174\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.190166\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.155723\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.218033\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.261610\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.131539\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.289453\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.195718\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.184074\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.113504\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.247896\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.293077\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.304391\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.221466\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.171556\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.347620\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.187456\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.084127\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.244392\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.244358\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.108862\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.144924\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.044558\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.102701\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.410675\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.238480\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.439990\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.138937\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.408402\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.188662\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.223975\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.162944\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.168063\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.155796\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.200989\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.193108\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.178834\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.298455\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.181244\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.253128\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.333147\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.397416\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.120691\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.297166\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.070789\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.182619\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.106401\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.179319\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.100786\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.072160\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.199062\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.234719\n",
      "\n",
      "Test set: Average loss: 0.0779, Accuracy: 9746/10000 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.186716\n",
      "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.204624\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.195249\n",
      "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.264294\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.212957\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.207209\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.206973\n",
      "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.424083\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.400194\n",
      "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.093562\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.165559\n",
      "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.430574\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.139768\n",
      "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.099547\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.130331\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.201055\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.240729\n",
      "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.280416\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.610923\n",
      "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.501422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.363544\n",
      "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.145745\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.156219\n",
      "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.319050\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.380941\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.272696\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.187006\n",
      "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.113817\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.165745\n",
      "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.299145\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.264856\n",
      "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.352371\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.261457\n",
      "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.283135\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.166152\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.137011\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.491373\n",
      "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.175530\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.255817\n",
      "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.186823\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.284251\n",
      "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.167647\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.191980\n",
      "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.182222\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.448086\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.689309\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.148358\n",
      "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.410923\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.135334\n",
      "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.301040\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.290796\n",
      "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.150353\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.302671\n",
      "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.125405\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.151562\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.224435\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.305343\n",
      "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.218390\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.083430\n",
      "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.298937\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.169788\n",
      "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.091588\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.166235\n",
      "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.181083\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.177937\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.122552\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.292286\n",
      "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.127837\n",
      "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.102840\n",
      "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.217698\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.205793\n",
      "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.143251\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.198330\n",
      "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.158921\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.127507\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.569069\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.348077\n",
      "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.086600\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.296569\n",
      "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.132282\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.172131\n",
      "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.286252\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.134943\n",
      "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.242985\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.202652\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.174045\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.123847\n",
      "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.182554\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.519771\n",
      "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.136212\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.138969\n",
      "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.222574\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.172045\n",
      "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.119424\n",
      "\n",
      "Test set: Average loss: 0.0685, Accuracy: 9781/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.222856\n",
      "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.068287\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.171672\n",
      "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.271985\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.236762\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.127789\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.168851\n",
      "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.125307\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.177637\n",
      "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.264307\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.243299\n",
      "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.213548\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.248449\n",
      "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.328545\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.260468\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.150418\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.570853\n",
      "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.247341\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.193532\n",
      "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.166721\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.135523\n",
      "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.102584\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.212279\n",
      "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.224511\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.119335\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.256640\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.116061\n",
      "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.143227\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.467509\n",
      "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.207756\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.247519\n",
      "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.351385\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.323165\n",
      "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.311391\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.348652\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.145162\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.413330\n",
      "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.296585\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.417191\n",
      "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.164668\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.212559\n",
      "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.190995\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.098108\n",
      "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.180677\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.106520\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.176798\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.188557\n",
      "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.223464\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.278433\n",
      "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.137529\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.142824\n",
      "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.363460\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.132572\n",
      "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.189262\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.326954\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.127265\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.268739\n",
      "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.305555\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.166591\n",
      "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.174062\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.255230\n",
      "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.226043\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.139790\n",
      "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.149293\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.161164\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.114728\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.215040\n",
      "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.344006\n",
      "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.309238\n",
      "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.145308\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.349393\n",
      "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.164041\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.250183\n",
      "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.066115\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.174719\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.128111\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.135598\n",
      "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.151441\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.225368\n",
      "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.114886\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.159715\n",
      "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.278121\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.358920\n",
      "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.317313\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.100155\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.326570\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.149849\n",
      "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.207947\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.170566\n",
      "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.208217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.411357\n",
      "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.160009\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.108378\n",
      "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.315160\n",
      "\n",
      "Test set: Average loss: 0.0675, Accuracy: 9767/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.425770\n",
      "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.158569\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.188077\n",
      "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.294055\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.201091\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.342126\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.344117\n",
      "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.195364\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.252965\n",
      "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.408598\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.116551\n",
      "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.184732\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.233030\n",
      "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.195725\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.112294\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.166328\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.174327\n",
      "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.111097\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.221963\n",
      "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.235917\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.398708\n",
      "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.142354\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.215257\n",
      "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.155237\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.210086\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.219207\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.086068\n",
      "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.097402\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.156539\n",
      "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.275218\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.155669\n",
      "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.176150\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.181055\n",
      "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.194872\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.132665\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.148617\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.338822\n",
      "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.116039\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.148565\n",
      "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.385521\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.213505\n",
      "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.117001\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.128613\n",
      "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.185035\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.329553\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.327154\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.177240\n",
      "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.238248\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.140675\n",
      "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.302887\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.165185\n",
      "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.112730\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.068169\n",
      "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.240616\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.274167\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.103494\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.262444\n",
      "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.390024\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.145869\n",
      "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.204185\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.195810\n",
      "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.094355\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.110680\n",
      "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.181714\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.142647\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.179618\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.246865\n",
      "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.288334\n",
      "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.274286\n",
      "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.172703\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.131784\n",
      "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.224920\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.237600\n",
      "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.101492\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.037708\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.127811\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.234603\n",
      "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.086372\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.224356\n",
      "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.118903\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.117938\n",
      "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.181666\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.166172\n",
      "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.118559\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.315469\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.157368\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.198038\n",
      "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.283255\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.199524\n",
      "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.254166\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.133568\n",
      "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.167027\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.249942\n",
      "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.389590\n",
      "\n",
      "Test set: Average loss: 0.0620, Accuracy: 9797/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.148609\n",
      "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.204868\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.161421\n",
      "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.104968\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.252315\n",
      "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.238592\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.220346\n",
      "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.150219\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.097070\n",
      "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.128420\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.119076\n",
      "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.186357\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.197041\n",
      "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.265092\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.132394\n",
      "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.090197\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.266519\n",
      "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.210455\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.372077\n",
      "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.276598\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.222003\n",
      "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.351401\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.180421\n",
      "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.182909\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.067284\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.054666\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.141703\n",
      "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.219230\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.218569\n",
      "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.213488\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.388417\n",
      "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.250038\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.121013\n",
      "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.218316\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.175760\n",
      "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.220210\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.101918\n",
      "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.102532\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.344335\n",
      "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.270155\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.066995\n",
      "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.126777\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.221878\n",
      "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.250385\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.100528\n",
      "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.218000\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.242897\n",
      "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.247335\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.049149\n",
      "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.100964\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.051004\n",
      "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.200271\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.257285\n",
      "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.409869\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.427574\n",
      "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.178462\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.109406\n",
      "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.199576\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.270166\n",
      "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.198111\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.275805\n",
      "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.076246\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.257666\n",
      "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.194502\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.226086\n",
      "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.207893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.210587\n",
      "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.201186\n",
      "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.145674\n",
      "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.080825\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.354076\n",
      "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.091128\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.285766\n",
      "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.155581\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.300247\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.086452\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.205141\n",
      "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.056156\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.162006\n",
      "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.301807\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.235189\n",
      "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.094852\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.157344\n",
      "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.091938\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.271281\n",
      "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.192830\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.339928\n",
      "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.298579\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.201226\n",
      "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.117828\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.236234\n",
      "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.061334\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.096723\n",
      "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.183641\n",
      "\n",
      "Test set: Average loss: 0.0559, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.071916\n",
      "Train Epoch: 10 [640/60000 (1%)]\tLoss: 0.171978\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.092039\n",
      "Train Epoch: 10 [1920/60000 (3%)]\tLoss: 0.318726\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.148046\n",
      "Train Epoch: 10 [3200/60000 (5%)]\tLoss: 0.107391\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.098254\n",
      "Train Epoch: 10 [4480/60000 (7%)]\tLoss: 0.258685\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.693809\n",
      "Train Epoch: 10 [5760/60000 (10%)]\tLoss: 0.162831\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.269938\n",
      "Train Epoch: 10 [7040/60000 (12%)]\tLoss: 0.311209\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.078687\n",
      "Train Epoch: 10 [8320/60000 (14%)]\tLoss: 0.142779\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.182288\n",
      "Train Epoch: 10 [9600/60000 (16%)]\tLoss: 0.254462\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.186114\n",
      "Train Epoch: 10 [10880/60000 (18%)]\tLoss: 0.218071\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.266125\n",
      "Train Epoch: 10 [12160/60000 (20%)]\tLoss: 0.093573\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.271608\n",
      "Train Epoch: 10 [13440/60000 (22%)]\tLoss: 0.203264\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.160763\n",
      "Train Epoch: 10 [14720/60000 (25%)]\tLoss: 0.394348\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.109391\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 0.132838\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.168173\n",
      "Train Epoch: 10 [17280/60000 (29%)]\tLoss: 0.303811\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.235999\n",
      "Train Epoch: 10 [18560/60000 (31%)]\tLoss: 0.256949\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.204740\n",
      "Train Epoch: 10 [19840/60000 (33%)]\tLoss: 0.451918\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.114965\n",
      "Train Epoch: 10 [21120/60000 (35%)]\tLoss: 0.132444\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.077190\n",
      "Train Epoch: 10 [22400/60000 (37%)]\tLoss: 0.175805\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.249242\n",
      "Train Epoch: 10 [23680/60000 (39%)]\tLoss: 0.129351\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.080032\n",
      "Train Epoch: 10 [24960/60000 (42%)]\tLoss: 0.304824\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.346584\n",
      "Train Epoch: 10 [26240/60000 (44%)]\tLoss: 0.178664\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.060513\n",
      "Train Epoch: 10 [27520/60000 (46%)]\tLoss: 0.107455\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.070249\n",
      "Train Epoch: 10 [28800/60000 (48%)]\tLoss: 0.166764\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.185059\n",
      "Train Epoch: 10 [30080/60000 (50%)]\tLoss: 0.137627\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.193248\n",
      "Train Epoch: 10 [31360/60000 (52%)]\tLoss: 0.149422\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.112564\n",
      "Train Epoch: 10 [32640/60000 (54%)]\tLoss: 0.107753\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.273596\n",
      "Train Epoch: 10 [33920/60000 (57%)]\tLoss: 0.451379\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.110369\n",
      "Train Epoch: 10 [35200/60000 (59%)]\tLoss: 0.113059\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.467966\n",
      "Train Epoch: 10 [36480/60000 (61%)]\tLoss: 0.148453\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.319540\n",
      "Train Epoch: 10 [37760/60000 (63%)]\tLoss: 0.248467\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.096790\n",
      "Train Epoch: 10 [39040/60000 (65%)]\tLoss: 0.396672\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.216507\n",
      "Train Epoch: 10 [40320/60000 (67%)]\tLoss: 0.167586\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.166437\n",
      "Train Epoch: 10 [41600/60000 (69%)]\tLoss: 0.253444\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.150293\n",
      "Train Epoch: 10 [42880/60000 (71%)]\tLoss: 0.185738\n",
      "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.170288\n",
      "Train Epoch: 10 [44160/60000 (74%)]\tLoss: 0.169894\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.101328\n",
      "Train Epoch: 10 [45440/60000 (76%)]\tLoss: 0.114227\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.285066\n",
      "Train Epoch: 10 [46720/60000 (78%)]\tLoss: 0.223718\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.303862\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 0.128788\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.092466\n",
      "Train Epoch: 10 [49280/60000 (82%)]\tLoss: 0.164249\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.073438\n",
      "Train Epoch: 10 [50560/60000 (84%)]\tLoss: 0.108920\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.206498\n",
      "Train Epoch: 10 [51840/60000 (86%)]\tLoss: 0.129922\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.082471\n",
      "Train Epoch: 10 [53120/60000 (88%)]\tLoss: 0.139976\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.212181\n",
      "Train Epoch: 10 [54400/60000 (91%)]\tLoss: 0.193191\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.138370\n",
      "Train Epoch: 10 [55680/60000 (93%)]\tLoss: 0.206277\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.105462\n",
      "Train Epoch: 10 [56960/60000 (95%)]\tLoss: 0.125182\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.165515\n",
      "Train Epoch: 10 [58240/60000 (97%)]\tLoss: 0.104530\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.268866\n",
      "Train Epoch: 10 [59520/60000 (99%)]\tLoss: 0.086356\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9815/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#训练与检测过程\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型的保存 与导入\n",
    "#方式一: 保存model结构以及全部参数\n",
    "torch.save(model, './model.pkl')\n",
    "model_load = torch.load('./model.pkl')\n",
    "\n",
    "#方式二: 仅保存参数\n",
    "torch.save(model.state_dict(), './model_state.pkl')\n",
    "model_load_1 = Net() #初始化模型结构\n",
    "model_load_1.load_state_dict(torch.load('./model_state.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以上即为整个网络的结构以及相应的训练与检测过程\n",
    "\n",
    "## 接下来对网络结构进行进一步详细分析\n",
    "\n",
    "    (1). 网络结构中包含两部分主要数据: data 以及 weight , 训练过程就是对输入的data 找到最合适的weight, 使得输出最能代表data的属性特征.\n",
    "\n",
    "    (2). data由原始数据而来, 一般而言可以分为train(训练集), val(验证集), test(测试集)\n",
    "\n",
    "    (3). 每一层的输出称为 feature map(特征图)\n",
    "\n",
    "    (4). 一般而言, cnn网络的浅层特征图可以包含更多局部信息, 而深层特征图包含更多全局信息.\n",
    "\n",
    "#### 第一, 本网络中 weight 结构的相关分析 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d (p=0.5)\n",
      "  (fc1): Linear (320 -> 50)\n",
      "  (fc2): Linear (50 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#打印所定义的model\n",
    "print( model)\n",
    "# 可以看到定义的基本网络结构包含conv1,conv2,conv2_drop,fc1,fc2这几个基本单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "1\n",
      "10\n",
      "(5, 5)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "#详细打印所定义的卷积层 conv1\n",
    "print( model.conv1)    #打印卷积核的整体参数\n",
    "print( model.conv1.in_channels)    #打印输入通道数\n",
    "print( model.conv1.out_channels)    #打印输出通道数\n",
    "print( model.conv1.kernel_size)    #打印卷积核kernel_size大小\n",
    "print( model.conv1.padding)    #打印卷积核padding大小\n",
    "\n",
    "# 解释:卷积大小 5x5x1x10 (10个5x5x1大小的卷积核, 其中1为上一层feature map层数)\n",
    "# 对于输入为28x28x1的图片, 经过该卷积, 输出为24x24x10\n",
    "# 并且可见当未对 stride(步长) 以及 padding(补偿) 进行定义时, 默认 stride = 1, padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0718 -0.0599  0.0084  0.1853 -0.0768\n",
      "  0.1293 -0.1749  0.0592  0.0821  0.1828\n",
      "  0.1663  0.1243 -0.0504  0.1141  0.1300\n",
      " -0.1124  0.0702  0.1318  0.1578  0.0617\n",
      " -0.0682  0.1047  0.0271 -0.1189 -0.1897\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0904 -0.0637  0.1429  0.0290 -0.0554\n",
      "  0.0337  0.0381  0.0624 -0.1583 -0.0627\n",
      "  0.1891 -0.0269 -0.0208 -0.0912  0.0931\n",
      "  0.1318  0.0365 -0.0591 -0.0320  0.0291\n",
      "  0.0797 -0.1200  0.1113 -0.1570  0.0432\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      " -0.1496 -0.1100  0.1010 -0.0170 -0.0978\n",
      "  0.0173 -0.0529  0.0389  0.1086 -0.1525\n",
      "  0.0892  0.1002  0.0454  0.1093 -0.1228\n",
      " -0.0084  0.0033 -0.0241 -0.0512 -0.1507\n",
      " -0.0800  0.0008 -0.1261 -0.1517  0.0656\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1841  0.1774 -0.1910 -0.1493  0.1080\n",
      " -0.1123 -0.0688 -0.1626 -0.0138  0.0164\n",
      " -0.0535 -0.1626  0.0468 -0.1920  0.1368\n",
      "  0.1302 -0.1057 -0.0190 -0.0736 -0.1097\n",
      " -0.0977 -0.0791  0.0472  0.0167  0.1136\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1334 -0.1217  0.1484  0.0244 -0.0805\n",
      " -0.1413 -0.1151 -0.1964 -0.1658  0.1137\n",
      " -0.1936 -0.1794  0.0982 -0.1419 -0.1451\n",
      " -0.1724 -0.0811 -0.1184 -0.1391  0.0542\n",
      "  0.0923 -0.0045  0.0354  0.1587 -0.1332\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.0933  0.1575 -0.0044 -0.0888  0.0488\n",
      "  0.1815 -0.1477  0.1556 -0.1853  0.1212\n",
      " -0.1131 -0.1757 -0.1836  0.0401  0.1334\n",
      " -0.1620  0.1551 -0.1530 -0.1430  0.0127\n",
      " -0.1313  0.1250 -0.1038  0.0597  0.1700\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.1718 -0.1985  0.0081 -0.1621 -0.0222\n",
      " -0.0622  0.1464 -0.0393  0.1361  0.0205\n",
      "  0.1034 -0.1369  0.0744  0.0157 -0.1044\n",
      " -0.1055  0.0956  0.0508 -0.0491  0.0022\n",
      "  0.0554 -0.0316 -0.0575  0.0952  0.0783\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1180 -0.0655 -0.0610 -0.0951  0.1857\n",
      "  0.0408  0.0199  0.1835  0.0559 -0.0893\n",
      "  0.0237  0.0028  0.0758  0.1155 -0.1758\n",
      "  0.1775  0.1404 -0.0148 -0.1047  0.0456\n",
      "  0.0171 -0.1974 -0.1359 -0.1116 -0.1345\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "  0.1836  0.0263  0.1904  0.0029 -0.0769\n",
      " -0.0371 -0.0407 -0.0456 -0.1744  0.0870\n",
      "  0.1378 -0.1964  0.0099 -0.0372  0.1531\n",
      "  0.0804 -0.0349  0.0985  0.0731 -0.0715\n",
      "  0.0693 -0.0319 -0.1852  0.1787  0.0618\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "  0.0207 -0.0407  0.0559  0.0685  0.1569\n",
      " -0.1204  0.0470  0.0402  0.1923  0.0864\n",
      "  0.1597 -0.1317 -0.0542 -0.0993 -0.1249\n",
      "  0.0943  0.0632 -0.1431 -0.1321 -0.1617\n",
      " -0.1606  0.1239  0.0068  0.0769 -0.0167\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(1).打印随机初始化的卷积核参数 (不存在seed)\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print( model.conv1.weight) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0332  0.1989  0.0881  0.1730 -0.2000\n",
      " -0.1488 -0.0791  0.1996 -0.1413 -0.1056\n",
      " -0.1631 -0.0414 -0.1255 -0.0448 -0.0618\n",
      "  0.0679 -0.0413  0.1742  0.0155  0.1385\n",
      " -0.0323 -0.0747  0.0741  0.0098 -0.1182\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.0226  0.1512 -0.1082 -0.1890  0.0138\n",
      "  0.0682  0.1656 -0.0331 -0.0171  0.0235\n",
      " -0.0277 -0.1438  0.1757 -0.1208  0.1114\n",
      "  0.1203  0.0864  0.1873  0.1211 -0.0746\n",
      " -0.1629  0.0769  0.0073  0.1506  0.1460\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1578  0.1317 -0.1660  0.1318 -0.1844\n",
      " -0.0908 -0.1321 -0.1763  0.1513  0.0682\n",
      " -0.1607  0.0372 -0.0316  0.0687  0.1832\n",
      " -0.0353  0.0133 -0.1210  0.0768 -0.0841\n",
      " -0.0738 -0.1432  0.0746  0.1133  0.1339\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.0350 -0.1927 -0.1863  0.1001  0.0496\n",
      "  0.1955  0.0643  0.0993 -0.0806 -0.0878\n",
      " -0.0215  0.1157 -0.1112 -0.1587 -0.1707\n",
      " -0.0208 -0.0123  0.1634 -0.1615 -0.0826\n",
      "  0.1613 -0.0849 -0.1522 -0.1480  0.0099\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.1923 -0.1666  0.0715  0.1667 -0.1153\n",
      "  0.1642 -0.0938 -0.0804 -0.0034  0.0338\n",
      " -0.1787  0.0264  0.0296  0.0456 -0.1413\n",
      "  0.1826  0.0357 -0.0956  0.0799 -0.1076\n",
      " -0.1591  0.0134 -0.0344  0.1800  0.0778\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0028 -0.0343  0.0162 -0.1800  0.1062\n",
      "  0.0144 -0.1819  0.0655 -0.1440  0.0060\n",
      "  0.1170  0.1778 -0.1881  0.0346  0.1533\n",
      "  0.1614  0.0163 -0.1450 -0.0208 -0.1443\n",
      "  0.1569  0.1230 -0.0490 -0.0409  0.0154\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.1339  0.0609  0.1710 -0.0555 -0.0609\n",
      "  0.0284  0.1003  0.0551  0.0904 -0.1495\n",
      "  0.1533  0.0761  0.0495  0.0591  0.1004\n",
      " -0.0584 -0.0604  0.1053 -0.0920 -0.0574\n",
      "  0.1584  0.1011 -0.0288  0.1525  0.1859\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1953  0.0654 -0.0008  0.0487 -0.1705\n",
      " -0.1541  0.1148  0.1798 -0.1744 -0.0200\n",
      " -0.0579  0.0314  0.1767 -0.0367 -0.0481\n",
      " -0.1052  0.1052  0.1614  0.1086  0.0295\n",
      " -0.0795 -0.1989  0.1091  0.0469 -0.1388\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0693  0.0315  0.0108 -0.1964  0.1544\n",
      "  0.0836 -0.0571 -0.0117  0.1634  0.1058\n",
      "  0.0493 -0.0130 -0.1937 -0.0924  0.1718\n",
      "  0.1327  0.0764  0.0205  0.1989 -0.1720\n",
      " -0.1311 -0.0110 -0.1451  0.0971  0.1730\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1232  0.0787 -0.0143 -0.1736 -0.1079\n",
      "  0.1022  0.0033  0.1016 -0.1166  0.1692\n",
      " -0.1803  0.0846  0.0075 -0.1503 -0.1311\n",
      " -0.1920 -0.0415 -0.1895 -0.1571 -0.1887\n",
      "  0.0039 -0.1015 -0.1591  0.1440 -0.0845\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0153  0.3259  0.1931  0.1950 -0.2066\n",
      " -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
      " -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
      "  0.0233 -0.0447  0.1835  0.1321  0.3404\n",
      "  0.1407  0.1476  0.2579  0.1959  0.0585\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0104 -0.0224 -0.3664 -0.4861 -0.2800\n",
      "  0.1141  0.1102 -0.2812 -0.4165 -0.3379\n",
      "  0.1372  0.1199  0.2881 -0.0999  0.1634\n",
      "  0.2905  0.3823  0.5954  0.5653  0.1324\n",
      " -0.1438  0.0064  0.0802  0.3379  0.2844\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.0798  0.0704 -0.1030  0.2154 -0.0975\n",
      " -0.1746 -0.2341 -0.2216  0.1732  0.1978\n",
      " -0.3685 -0.1535 -0.1572  0.1322  0.3971\n",
      " -0.2559 -0.1418 -0.1172  0.3395  0.2059\n",
      " -0.1713 -0.0901  0.2505  0.3522  0.2085\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1241 -0.1835 -0.3326 -0.1037 -0.1134\n",
      "  0.3162  0.0332 -0.0893 -0.2823 -0.2161\n",
      "  0.1476  0.1888 -0.1981 -0.2667 -0.2407\n",
      "  0.1627  0.0904  0.0908 -0.2753 -0.1488\n",
      "  0.2771  0.0394 -0.1264 -0.2166 -0.0730\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.3005 -0.2953 -0.1426 -0.0045 -0.1248\n",
      " -0.0297 -0.2309 -0.2401 -0.1252  0.0536\n",
      " -0.3134 -0.0809 -0.0239  0.0702 -0.0181\n",
      "  0.0806 -0.0049 -0.0313  0.1956  0.0620\n",
      " -0.1641  0.0897  0.1914  0.4325  0.2904\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1330  0.2172  0.1960 -0.0969  0.1130\n",
      "  0.1974  0.0932  0.1406 -0.2397 -0.0977\n",
      "  0.3167  0.3363 -0.2704 -0.1850 -0.0186\n",
      "  0.3908  0.1115 -0.2426 -0.1940 -0.2603\n",
      "  0.2042  0.2092 -0.1117 -0.2013 -0.1616\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0198  0.2373  0.2729 -0.0540 -0.1854\n",
      "  0.0802  0.3993  0.1696 -0.0028 -0.2808\n",
      "  0.2505  0.3697 -0.0181 -0.1314 -0.0736\n",
      "  0.0541  0.1614  0.2596 -0.0544 -0.0701\n",
      "  0.1702  0.2460  0.2558  0.3425  0.3234\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.0982  0.1252 -0.0135 -0.0752 -0.3418\n",
      " -0.1307  0.2256  0.3210 -0.2659 -0.3056\n",
      " -0.2033  0.0767  0.5127  0.0826 -0.2298\n",
      " -0.3450 -0.0224  0.3877  0.3312  0.0064\n",
      " -0.2941 -0.4151  0.0306  0.1472 -0.0846\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0570 -0.0232 -0.0239 -0.1755  0.2165\n",
      "  0.0881 -0.1158 -0.0787  0.1568  0.2332\n",
      "  0.0131 -0.0756 -0.2181  0.0534  0.3254\n",
      "  0.0991  0.0501  0.0569  0.3412 -0.0361\n",
      " -0.1060  0.0797 -0.0573  0.1666  0.1814\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1200  0.0947 -0.0307 -0.2284 -0.1777\n",
      "  0.0758 -0.0426  0.0249 -0.2037  0.0998\n",
      " -0.2321  0.0203 -0.0582 -0.2084 -0.1820\n",
      " -0.2436 -0.0961 -0.2408 -0.2075 -0.2478\n",
      " -0.0113 -0.1164 -0.1620  0.1383 -0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(2).打印随机初始化的卷积核参数 (使用固定seed)\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "print( model.conv1.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.390087\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.350225\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.288934\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.279396\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.272692\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.279640\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.276944\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.233707\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.222649\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.181699\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.159016\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.055780\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.065890\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.916481\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.910268\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.704504\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.554525\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.621117\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.521305\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.533543\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.339470\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.382869\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.338094\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.271376\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.084347\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.204976\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.879644\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.776240\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.967955\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.913667\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.190108\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.847057\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.893181\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.072115\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.982275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.724942\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.857505\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.808646\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.784363\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.735870\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.584132\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.723374\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.748310\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.512329\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.768801\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.595193\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.566207\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.751794\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.805441\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.820678\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.651483\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.861828\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.450013\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.675125\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.589900\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.548274\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.504815\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.501443\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.513226\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.680256\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.761249\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.773745\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.784808\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.702843\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.620407\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.827360\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.473729\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.690846\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.614329\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.801135\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.482657\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.535080\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.534342\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.295305\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.498265\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.652372\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.444436\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.345962\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.510644\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.546157\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.368057\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.590018\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.590431\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.433518\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.524811\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.571309\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.370530\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.692114\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.516627\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.524651\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.400514\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.376999\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.313202\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.555166\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0466  0.2031  0.0893  0.1611 -0.2132\n",
      " -0.1936 -0.1076  0.1918 -0.1395 -0.1201\n",
      " -0.2006 -0.0598 -0.1203 -0.0266 -0.0498\n",
      "  0.0581 -0.0258  0.2143  0.0675  0.1800\n",
      " -0.0053 -0.0306  0.1248  0.0615 -0.0771\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0364  0.1486 -0.1940 -0.3186 -0.1161\n",
      "  0.0935  0.1672 -0.1010 -0.1421 -0.0576\n",
      "  0.0400 -0.0273  0.2717 -0.0324  0.2114\n",
      "  0.2760  0.3282  0.4855  0.4201  0.1387\n",
      " -0.0682  0.2083  0.1857  0.3354  0.2976\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1588  0.1365 -0.1396  0.1756 -0.1081\n",
      " -0.1313 -0.1584 -0.1809  0.1946  0.1662\n",
      " -0.2231 -0.0186 -0.0632  0.1343  0.3125\n",
      " -0.0924 -0.0317 -0.1161  0.1984  0.0776\n",
      " -0.0982 -0.1332  0.1467  0.2583  0.2716\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.0506 -0.1400 -0.1847  0.0781  0.0193\n",
      "  0.2604  0.0830  0.0677 -0.1320 -0.1303\n",
      "  0.0647  0.1492 -0.1345 -0.1917 -0.1927\n",
      "  0.0859  0.0290  0.1340 -0.1921 -0.1035\n",
      "  0.2538 -0.0577 -0.1743 -0.1766 -0.0167\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.2334 -0.2086  0.0280  0.1653 -0.0758\n",
      "  0.0983 -0.1439 -0.0961  0.0194  0.0723\n",
      " -0.2369 -0.0221  0.0337  0.0955 -0.0845\n",
      "  0.1606  0.0354 -0.0436  0.1656 -0.0402\n",
      " -0.1364  0.0644  0.0645  0.2998  0.1651\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1139  0.1199  0.1174 -0.1610  0.0591\n",
      "  0.1518 -0.0390  0.0978 -0.2051 -0.0853\n",
      "  0.2658  0.2762 -0.2334 -0.0760  0.0485\n",
      "  0.3203  0.0911 -0.1987 -0.1066 -0.2093\n",
      "  0.2858  0.1994 -0.0675 -0.0893 -0.0253\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0590  0.2004  0.2798 -0.0502 -0.1513\n",
      "  0.1376  0.2859  0.1823  0.0711 -0.2584\n",
      "  0.3385  0.3405  0.2042  0.0581  0.0280\n",
      "  0.1112  0.1552  0.2583 -0.0349 -0.0425\n",
      "  0.2332  0.2018  0.0644  0.2440  0.2972\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1920  0.1026  0.0852  0.1157 -0.1874\n",
      " -0.1738  0.1403  0.2622 -0.1226 -0.0631\n",
      " -0.0785  0.0763  0.2957  0.0519 -0.0504\n",
      " -0.1161  0.1627  0.2953  0.2247  0.0771\n",
      " -0.0997 -0.1714  0.1870  0.1206 -0.1041\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0796 -0.0018 -0.0129 -0.1723  0.2378\n",
      "  0.0698 -0.0696 -0.0056  0.2377  0.2448\n",
      "  0.0299 -0.0211 -0.1736  0.0291  0.3437\n",
      "  0.1022  0.0523  0.0361  0.3140 -0.0277\n",
      " -0.1726 -0.0423 -0.1340  0.1667  0.2404\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1355  0.0666 -0.0303 -0.1926 -0.1293\n",
      "  0.0835 -0.0185  0.0756 -0.1443  0.1396\n",
      " -0.1965  0.0672 -0.0126 -0.1697 -0.1512\n",
      " -0.2076 -0.0568 -0.2049 -0.1721 -0.2050\n",
      " -0.0027 -0.1051 -0.1597  0.1451 -0.0872\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#卷积层weight 的分析\n",
    "#(3).经过一轮训练之后的卷积参数\n",
    "for epoch in range(1, 1 + 1):\n",
    "    train(epoch)\n",
    "    \n",
    "print( model.conv1.weight) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积weight分析总结:\n",
    "   (1). seed可以使得每次初始化的值固定,这样在做实验的时候可以对比,大家的初始化参数都是一样的;\n",
    "\n",
    "   (2). 在实际项目中一般不需要加seed;\n",
    "   \n",
    "   (3). 这里也可以再次看到conv的参数个数为 10x1x5x5 (10为输出的卷积个数, 1为上层feature map 层数, 5x5为卷积核大小);\n",
    "    \n",
    "   (4). 可以发现每次train之后,更新的参数是conv的参数, 这里我们可以这样理解: train的过程就是为了找到合适的卷积核参数, 使得对于输入的data, 可以有效的提取特征, 进而根据所提取的特征作进一步的工作, 比如分类."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout2d (p=0.5)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#  dropout分析\n",
    "#  可见默认值为 p = 0.5 , 这里p表示随机失活神经元概率, 即0.5的神经元在单次传播过程中不进行梯度更新.\n",
    "\n",
    "print(model.conv2_drop)\n",
    "print(model.conv2_drop.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear (320 -> 50)\n",
      "320\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# 线性函数,可以等价于全连接\n",
    "# 参数包括 weight, bias 即权重和偏置(权重系数以及常数项系数)\n",
    "\n",
    "print(model.fc1)\n",
    "print(model.fc1.in_features)    #输入的神经元个数\n",
    "print(model.fc1.out_features)    #输出的神经元个数\n",
    "\n",
    "#若想打印参数信息, 如下:\n",
    "# print(model.fc1.weight)\n",
    "# print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第二, 本网络中 data 结构的相关分析 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchvision.datasets.mnist.MNIST object at 0x7fa8866cf5f8>\n",
      "60000\n",
      "(\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.0424\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1995  2.6051\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  2.3633\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5940\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1315\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1951  1.7523  2.3633\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  0.2758  1.7650  2.4524  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  1.3068  2.7960  2.7960  2.7960  2.2742\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.3860 -0.1951 -0.1951 -0.1951  1.1795  1.3068\n",
      "  0.0340  0.7722  1.5359  1.7396  2.7960  2.7960  2.7960  2.7960  2.7960\n",
      "  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7960  2.7706\n",
      "  2.7960  2.7960  2.7960  2.7960  2.7960  2.0960  1.8923  2.7197  2.6433\n",
      "  1.5614  0.9377  2.7960  2.7960  2.1851 -0.2842 -0.4242  0.1231  1.5359\n",
      " -0.2460 -0.4115  1.5359  2.7960  0.7213 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242  1.3450  2.7960  1.9942 -0.3988 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.2842  1.9942  2.7960  0.4668 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242  0.0213  2.6433  2.4396  1.6123  0.9504 -0.4115\n",
      " -0.4242 -0.4242 -0.4242 -0.4242  0.6068  2.6306  2.7960  2.7960  1.0904\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1486  1.9432  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.2206  0.7595  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.7451\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.1613  1.2305  1.9051  2.7960\n",
      " -0.4242 -0.4242 -0.4242  0.0722  1.4596  2.4906  2.7960  2.7960  2.7960\n",
      " -0.4242 -0.1187  1.0268  2.3887  2.7960  2.7960  2.7960  2.7960  2.1342\n",
      "  0.4159  2.2869  2.7960  2.7960  2.7960  2.7960  2.0960  0.6068 -0.3988\n",
      "  2.7960  2.7960  2.7960  2.7960  2.0578  0.5940 -0.3097 -0.4242 -0.4242\n",
      "  2.7960  2.7960  2.6815  1.2686 -0.2842 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.2941  1.2559 -0.2206 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.8032 -0.0933  1.6887  2.8215  2.7197  1.1923 -0.4242 -0.4242 -0.4242\n",
      "  2.4396  1.7650  2.7960  2.6560  2.0578  0.3904 -0.4242 -0.4242 -0.4242\n",
      "  0.7595  0.6195  0.6195  0.2886  0.0722 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.1060 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.4850 -0.0806 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  1.9560 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.7451  0.3904 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.2105 -0.3988 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7578  1.8923 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.5686 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.FloatTensor of size 1x28x28]\n",
      ", 5)\n",
      "<bound method _type of \n",
      "(  0  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  1  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  2  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      " ...  \n",
      "\n",
      "(59997,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59998,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59999,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "[torch.ByteTensor of size 60000x28x28]\n",
      ">\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(  0  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  1  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(  2  ,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      " ...  \n",
      "\n",
      "(59997,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59998,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "\n",
      "(59999,.,.) = \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "      ...         ⋱        ...      \n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "    0    0    0  ...     0    0    0\n",
      "[torch.ByteTensor of size 60000x28x28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对原始下载的train data 进行分析\n",
    "\n",
    "print(train_loader.dataset)    #输入网络的train data (经过了数据转换, 归一化之后的)\n",
    "print(len(train_loader.dataset))    # train data 图片张数\n",
    "print(train_loader.dataset[0])    # 该输出的为 转化后的第一张图片的数字结构\n",
    "print(train_loader.dataset.train_data[0])   #该输出的为 变回原始输入的第一张图片的数字结构\n",
    "print(train_loader.dataset.train_data[0].type) #该输出的为 变回原始输入的第一张图片的数据类型, 数据类型为torch.ByteTensor 张量\n",
    "print(train_loader.dataset.train_data[0].shape)  #图像大小为1x28x28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 可以看到对于输入到model中进行训练的数据有如下特点:\n",
    "\n",
    "(1).输入数据维度为(60000,1,28,28) 即60000张图片, 每张图片为单通道灰度图, 长和宽均为28\n",
    "\n",
    "(2).输入数据的数据类型为 归一化的float tensor (torch.FloatTensor )\n",
    "\n",
    "(3).可以通过 transforms 对数据类型进行转换\n",
    "\n",
    "(4).可以通过train_loader.dataset.train_data直接得到原始图像归一化前的张量数据结构\n",
    "\n",
    "#### 2. 我们知道这里数据类型是 Tensor 张量, 如果需要以图片形式输出可以有如下几种方式:\n",
    "    \n",
    "(1).使用 \" .numpy()\" 将 Tensor类型  转换为 numpy, 然后使用 opencv 或 matplotlib 作图;\n",
    "    \n",
    "(2).使用transforms.ToPILImage() 将 Tensor类型 转化为 PIL类型输出;\n",
    "    \n",
    "(3).使用visdom工具, 直接对 Tensor 做可视化 (尝试失败,不想研究,感兴趣可以自己研究下).\n",
    "\n",
    "(4).其他,例如使用TensorFlow的可视化工具\n",
    "\n",
    "#### 下面选取了第一种方式进行作图, 其他方式感兴趣可以自己尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlgAAAHVCAYAAAAtoIVHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa86fcc94a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 原始图像可视化输出 : 使用 opencv 或 matplotlib 作图\n",
    "\n",
    "import numpy as np    #导入numpy\n",
    "img_tensor = train_loader.dataset.train_data[0] #导入tensor 数据\n",
    "img_np = img_tensor.numpy()  #将tensor 数据 转化为numpy\n",
    "\n",
    "# #下面几行代码使用opencv画图, 不建议在jupyter notebook内使用, 因为cv2.waitKey() 会导致后面程序无法正常运行  \n",
    "# import cv2    #导入opencv\n",
    "# cv2.imshow(\"image\", img_np)\n",
    "# cv2.waitKey()  \n",
    "\n",
    "#使用matplotlib 作图, 在jupyter notebook中推荐\n",
    "import  matplotlib.pyplot as plt    #导入matplotlib画图工具\n",
    "#plt图像嵌入jupyter内\n",
    "%matplotlib inline \n",
    "\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "# 图1: 输出原图\n",
    "ax1 = fig.add_subplot(221) \n",
    "ax1.imshow(img_np)  \n",
    "\n",
    "# 图2: 输出热力图\n",
    "ax2 = fig.add_subplot(222) \n",
    "ax2.imshow(img_np,  cmap=\"hot\" )  \n",
    "\n",
    "# 图3: 输出灰度图\n",
    "ax3 = fig.add_subplot(223) \n",
    "ax3.imshow(img_np, cmap=plt.cm.gray)  \n",
    "\n",
    "# 图4: 输出反向灰度图\n",
    "ax4 = fig.add_subplot(224) \n",
    "ax4.imshow(img_np, cmap=plt.cm.gray_r)  \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsMAAAHVCAYAAAAU6/ZZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxTAMAACAZDEMAwAAIFn/H1XsRkiWN5f4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa86f4afba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#输出 前6张图\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "num =231\n",
    "for i in range(6):\n",
    "    img_tensor = train_loader.dataset.train_data[i] #导入tensor 数据\n",
    "    img_np = img_tensor.numpy()  #将tensor 数据 转化为numpy\n",
    "    ax1 = fig.add_subplot(num) \n",
    "    ax1.imshow(img_np)  \n",
    "    num += 1\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面, 我们展示一下网络的详细训练过程\n",
    "\n",
    "#### 在此, 我将展示这个网络每一步前向传播中feature map发生的变化, 以及反向传播的weight发生的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#初始化 model\n",
    "model = Net()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "#选择优化方式, 这里使用的是SGD随机梯度下降, 当然也可以选择别的\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "#导入 train数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一个batch的图片总个数:\n",
      "64\n",
      "这个batch图片的类别信息:\n",
      "[5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1, 1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5, 9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0]\n"
     ]
    }
   ],
   "source": [
    "#第一个batch的一次前向传播以及反向传播\n",
    "\n",
    "#以比较愚蠢的方式导入一个batch的数据, 这里一个batch包含64张图\n",
    "model.train()\n",
    "for batch_idx, (data_i, target_i) in enumerate(train_loader):\n",
    "    if batch_idx !=0:\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "print (\"一个batch的图片总个数:\" )\n",
    "print (len(data))\n",
    "print (\"这个batch图片的类别信息:\" )\n",
    "print ([j for j in target])\n",
    "\n",
    "#转换数据类型\n",
    "data, target = Variable(data), Variable(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整体流程\n",
    "在此, 我将展示这个前向传播以及反向传播过程中, 参数是如何变化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向传播前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1407 -0.0253  0.1999 -0.1994\n",
      " -0.1119  0.1906 -0.1263  0.0701 -0.1803\n",
      " -0.0821  0.1815  0.0779  0.1674 -0.1716\n",
      "  0.0052 -0.0165  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0292 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1960 -0.1326  0.0846 -0.1297  0.0142\n",
      "  0.1144  0.0488 -0.1724  0.1641  0.0055\n",
      " -0.0211  0.1503 -0.0945  0.1493 -0.0684\n",
      "  0.0383 -0.0214  0.0342  0.0387  0.0328\n",
      " -0.0773  0.0035 -0.0590  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1999  0.1026 -0.1491 -0.0924 -0.1258\n",
      "  0.0420  0.0497  0.1619 -0.0237  0.1122\n",
      "  0.1907  0.1579  0.0430 -0.1519  0.1576\n",
      " -0.0566  0.1748 -0.1697  0.1135 -0.1544\n",
      "  0.1580  0.1302  0.1174 -0.1951  0.0236\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1319 -0.1651  0.1064  0.1096 -0.0925\n",
      " -0.0695  0.1518 -0.1159 -0.0858  0.0017\n",
      " -0.1722 -0.1757 -0.0232  0.0240  0.1866\n",
      "  0.0097  0.0489 -0.1157  0.0142 -0.1830\n",
      " -0.0899  0.1399 -0.1802 -0.0496  0.1484\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0826 -0.0188  0.0553 -0.0241  0.1129\n",
      "  0.1193 -0.0050  0.0981  0.1629 -0.0506\n",
      "  0.1275  0.0013  0.1190  0.1001  0.1036\n",
      " -0.1814  0.1130  0.0299  0.1662  0.1661\n",
      " -0.1892  0.0051 -0.1623 -0.0477 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0287  0.1197 -0.1414  0.0240 -0.1098\n",
      " -0.1157 -0.0984 -0.0533  0.0326  0.0231\n",
      " -0.0876 -0.1740  0.1189  0.1045 -0.1423\n",
      "  0.0480 -0.0041  0.1461  0.0988  0.1371\n",
      "  0.0738  0.0284 -0.1009 -0.1743  0.1637\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0487 -0.1323  0.0369 -0.0769 -0.0445\n",
      " -0.1175  0.1258 -0.1453  0.0420 -0.1572\n",
      "  0.0530  0.1615  0.1287  0.0696 -0.1123\n",
      "  0.1034 -0.0652 -0.1831  0.0643 -0.0481\n",
      "  0.0143  0.0125  0.0592 -0.0781 -0.0212\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0751 -0.0441 -0.1314 -0.0647\n",
      "  0.0334 -0.1760 -0.1744  0.0369  0.0008\n",
      "  0.0643  0.0650 -0.1496 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1934\n",
      "  0.0604  0.0861 -0.0189 -0.1015 -0.0565\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0887 -0.0322  0.0334\n",
      " -0.1326  0.1553  0.1177  0.1299 -0.1733\n",
      " -0.0784  0.0653  0.0403 -0.1282  0.0069\n",
      "  0.0510 -0.1013  0.0019 -0.0489  0.0513\n",
      " -0.1697 -0.1396 -0.1768  0.0360 -0.1063\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1708 -0.1845  0.1475  0.1374 -0.0094\n",
      "  0.1567  0.0862 -0.0326  0.0202  0.0633\n",
      " -0.0656  0.0035 -0.1308  0.0776 -0.0355\n",
      "  0.0447 -0.0382 -0.0373 -0.0405  0.0012\n",
      "  0.0511  0.0078 -0.0660 -0.0332  0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "前向传播前, conv1的梯度大小: \n",
      "None\n",
      "输出前向传播的输出结果:\n",
      "Variable containing:\n",
      "-2.1917 -2.3389 -2.4776 -2.5055 -2.5838 -2.1226 -2.3332 -2.4761 -2.2067 -1.9665\n",
      "-2.3421 -2.4224 -2.2239 -2.4818 -2.2421 -2.1416 -2.1980 -2.3784 -2.6176 -2.0957\n",
      "-2.0686 -2.3997 -2.2660 -2.3733 -2.3183 -2.0729 -2.4807 -2.3592 -2.6409 -2.1870\n",
      "-2.2944 -2.3667 -2.4883 -2.5643 -2.3807 -2.1352 -2.2432 -2.4597 -2.1665 -2.0517\n",
      "-2.1448 -2.3865 -2.2335 -2.5471 -2.5662 -2.1384 -2.2525 -2.2114 -2.7266 -2.0348\n",
      "-2.2982 -2.2594 -2.3840 -2.3467 -2.5374 -2.4862 -2.0554 -2.5187 -2.1425 -2.1284\n",
      "-2.4886 -2.5447 -2.1866 -2.5661 -2.2188 -2.0752 -2.5031 -2.5515 -2.2258 -1.9153\n",
      "-1.9393 -2.5985 -2.1147 -2.8482 -2.4422 -2.2964 -2.1619 -2.4603 -2.3845 -2.0960\n",
      "-2.1799 -2.3747 -2.3158 -2.4328 -2.4555 -2.2211 -2.2499 -2.3455 -2.3372 -2.1603\n",
      "-2.2281 -2.5368 -2.4331 -2.4517 -2.4350 -1.9741 -2.0728 -2.5939 -2.2360 -2.2509\n",
      "-1.8192 -2.7740 -2.3935 -2.6287 -2.5342 -2.2678 -2.1573 -2.5078 -2.3442 -1.9945\n",
      "-2.4192 -2.5439 -2.1110 -2.4508 -2.4340 -2.1790 -2.3352 -2.6602 -2.1291 -1.9763\n",
      "-2.1247 -2.4333 -2.4106 -2.5872 -2.3709 -2.1526 -2.4335 -2.6375 -2.2811 -1.8529\n",
      "-2.3751 -2.7142 -2.0875 -2.5173 -2.3513 -2.0784 -2.4173 -2.0669 -2.6543 -2.0431\n",
      "-2.1912 -2.4592 -2.4106 -2.4180 -2.3067 -2.1690 -2.1832 -2.4349 -2.3272 -2.1882\n",
      "-2.2212 -2.2410 -2.3946 -2.3726 -2.4396 -2.2294 -2.1965 -2.6396 -2.3027 -2.0926\n",
      "-1.9898 -2.2431 -2.1093 -2.6554 -2.6940 -2.2502 -2.2139 -2.4399 -2.4631 -2.1987\n",
      "-2.3077 -2.4465 -2.3485 -2.5105 -2.5310 -2.4227 -2.3416 -2.7747 -1.7772 -1.9572\n",
      "-2.0951 -2.2901 -2.5304 -2.3550 -2.2505 -2.1212 -2.1675 -2.7168 -2.5046 -2.1744\n",
      "-2.3021 -2.3970 -2.5456 -2.5876 -2.3763 -2.1785 -2.4074 -2.2853 -2.2199 -1.9050\n",
      "-2.2898 -2.4483 -2.2757 -2.4523 -2.3434 -2.0998 -2.1776 -2.2350 -2.7781 -2.0995\n",
      "-2.2040 -2.2526 -2.3080 -2.5174 -2.3956 -2.1536 -2.1230 -2.2663 -2.6692 -2.2584\n",
      "-2.2036 -2.5150 -2.1456 -2.5483 -2.4155 -2.0466 -2.3890 -2.2637 -2.6616 -2.0441\n",
      "-2.2354 -2.2853 -2.2650 -2.6003 -2.3806 -2.1687 -2.4592 -2.5543 -2.4610 -1.8463\n",
      "-2.5697 -2.5652 -1.9627 -2.3670 -2.2222 -2.4777 -2.2041 -2.2341 -2.6983 -2.0008\n",
      "-2.1056 -2.7707 -2.0606 -2.7980 -2.2783 -2.0909 -2.6696 -2.3528 -2.5673 -1.8317\n",
      "-2.3632 -2.4257 -2.5368 -2.4694 -2.4007 -2.2030 -2.3762 -2.4359 -2.0958 -1.9026\n",
      "-1.9568 -2.3545 -2.4787 -2.5966 -2.2041 -2.0351 -2.5239 -2.2615 -3.1984 -1.9719\n",
      "-2.2489 -2.1556 -2.4867 -2.4040 -2.3254 -2.2293 -2.1297 -2.3336 -2.5161 -2.2713\n",
      "-2.2829 -2.4468 -2.3339 -2.5784 -2.4063 -2.2632 -2.3624 -2.4603 -2.1860 -1.8806\n",
      "-2.4679 -2.2778 -2.7220 -2.5969 -2.2259 -2.0969 -2.3808 -2.4264 -1.9929 -2.0831\n",
      "-2.4238 -2.5591 -2.2387 -2.5329 -2.1949 -2.1496 -2.2046 -2.4003 -2.4711 -2.0052\n",
      "-2.1285 -2.6520 -2.0582 -2.5639 -2.4473 -1.8961 -2.4036 -2.4409 -2.6807 -2.0946\n",
      "-2.2828 -2.5400 -2.4459 -2.2604 -2.2999 -2.0228 -2.5025 -2.4390 -2.4166 -1.9858\n",
      "-2.5354 -2.4369 -2.6835 -2.4380 -2.1282 -2.3610 -2.3397 -2.5180 -2.0610 -1.8326\n",
      "-2.3134 -2.6355 -2.2767 -2.5351 -2.2555 -2.2306 -2.4475 -2.2983 -2.2260 -1.9637\n",
      "-2.3555 -2.4203 -2.2633 -2.5761 -2.1811 -2.2187 -2.0691 -2.6659 -2.1896 -2.2376\n",
      "-2.3995 -2.3211 -2.2956 -2.4347 -2.5406 -2.5550 -2.0133 -2.4852 -1.9873 -2.1864\n",
      "-2.3185 -2.5356 -2.4706 -2.6629 -2.2481 -2.2803 -2.0107 -2.6065 -2.0153 -2.1265\n",
      "-1.9324 -2.5658 -2.1262 -2.6632 -2.5522 -2.4343 -2.1997 -2.1198 -2.7826 -2.0326\n",
      "-2.1326 -2.2145 -2.4450 -2.7521 -2.4630 -2.3214 -1.9683 -2.4741 -2.2274 -2.2378\n",
      "-2.3462 -2.3429 -2.2402 -2.4967 -2.5272 -2.1842 -2.1444 -2.5370 -2.2098 -2.1120\n",
      "-2.1757 -2.5165 -2.4680 -2.6204 -2.4094 -2.3591 -2.3112 -2.6958 -1.8663 -1.9484\n",
      "-2.1342 -2.8745 -2.0702 -2.8340 -1.9909 -2.2673 -2.4702 -2.3902 -2.7212 -1.8442\n",
      "-2.3864 -2.5237 -2.1162 -2.5365 -2.1206 -2.0354 -2.1538 -2.3193 -2.7570 -2.3045\n",
      "-2.2868 -2.2972 -2.4244 -2.2220 -2.3672 -2.0699 -2.3322 -2.6889 -2.3263 -2.1344\n",
      "-2.1557 -2.7498 -2.2694 -2.6719 -2.1903 -2.1308 -2.7246 -2.4672 -2.4089 -1.7327\n",
      "-2.3071 -2.2596 -2.4543 -2.3650 -2.4317 -2.1026 -2.1948 -2.4624 -2.3949 -2.1340\n",
      "-2.3137 -2.3152 -2.4037 -2.4310 -2.4515 -2.1451 -2.1701 -2.3205 -2.3438 -2.1850\n",
      "-2.2577 -2.3574 -2.1525 -2.7303 -2.4588 -2.3310 -1.9631 -2.8991 -2.0003 -2.2460\n",
      "-2.2999 -2.3248 -2.2850 -2.5005 -2.4065 -2.1419 -2.2773 -2.3552 -2.3842 -2.1124\n",
      "-2.1142 -2.3619 -2.8975 -2.6281 -2.2181 -1.9516 -2.2129 -2.6395 -2.2779 -2.0914\n",
      "-2.1293 -2.5448 -2.0544 -2.6568 -2.4130 -2.2041 -2.2242 -2.1898 -2.7798 -2.1021\n",
      "-2.2683 -2.3625 -2.4920 -2.5399 -2.4385 -2.2241 -2.2912 -2.1167 -2.4855 -1.9633\n",
      "-2.0891 -2.3568 -2.2857 -2.5621 -2.2980 -2.3271 -2.3080 -2.5953 -2.2897 -2.0463\n",
      "-2.2074 -2.6438 -2.5295 -2.5457 -2.2072 -2.1596 -2.2656 -2.8731 -1.9984 -1.9690\n",
      "-2.2371 -2.5495 -2.2701 -2.7344 -2.3885 -2.3036 -2.7096 -1.9611 -2.3510 -1.8796\n",
      "-2.3664 -2.3153 -2.1863 -2.4156 -2.2737 -2.2603 -2.1829 -2.4413 -2.4409 -2.1920\n",
      "-2.4292 -2.1724 -2.3422 -2.4067 -2.3432 -2.2659 -2.1561 -2.3471 -2.3404 -2.2605\n",
      "-2.3696 -2.2260 -2.4691 -2.5400 -2.1486 -2.3161 -2.3544 -2.5392 -2.3237 -1.9113\n",
      "-2.1325 -2.3291 -2.2246 -2.5680 -2.4224 -2.2070 -2.2386 -2.4278 -2.4760 -2.1067\n",
      "-2.1109 -2.2991 -2.2738 -2.6320 -2.6050 -2.1885 -2.2745 -2.4374 -2.3920 -1.9951\n",
      "-2.0481 -2.6920 -2.1901 -2.5944 -2.2659 -2.0874 -2.4800 -2.3809 -2.5863 -1.9847\n",
      "-1.8215 -2.7782 -2.2195 -2.6894 -2.4385 -2.3070 -2.2767 -2.2908 -2.7802 -1.9187\n",
      "[torch.cuda.FloatTensor of size 64x10 (GPU 0)]\n",
      "\n",
      "输出本次前向传波的loss值:\n",
      "Variable containing:\n",
      " 2.3166\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "\n",
      "前向传播后反向传播前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1407 -0.0253  0.1999 -0.1994\n",
      " -0.1119  0.1906 -0.1263  0.0701 -0.1803\n",
      " -0.0821  0.1815  0.0779  0.1674 -0.1716\n",
      "  0.0052 -0.0165  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0292 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1960 -0.1326  0.0846 -0.1297  0.0142\n",
      "  0.1144  0.0488 -0.1724  0.1641  0.0055\n",
      " -0.0211  0.1503 -0.0945  0.1493 -0.0684\n",
      "  0.0383 -0.0214  0.0342  0.0387  0.0328\n",
      " -0.0773  0.0035 -0.0590  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1999  0.1026 -0.1491 -0.0924 -0.1258\n",
      "  0.0420  0.0497  0.1619 -0.0237  0.1122\n",
      "  0.1907  0.1579  0.0430 -0.1519  0.1576\n",
      " -0.0566  0.1748 -0.1697  0.1135 -0.1544\n",
      "  0.1580  0.1302  0.1174 -0.1951  0.0236\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1319 -0.1651  0.1064  0.1096 -0.0925\n",
      " -0.0695  0.1518 -0.1159 -0.0858  0.0017\n",
      " -0.1722 -0.1757 -0.0232  0.0240  0.1866\n",
      "  0.0097  0.0489 -0.1157  0.0142 -0.1830\n",
      " -0.0899  0.1399 -0.1802 -0.0496  0.1484\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0826 -0.0188  0.0553 -0.0241  0.1129\n",
      "  0.1193 -0.0050  0.0981  0.1629 -0.0506\n",
      "  0.1275  0.0013  0.1190  0.1001  0.1036\n",
      " -0.1814  0.1130  0.0299  0.1662  0.1661\n",
      " -0.1892  0.0051 -0.1623 -0.0477 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0287  0.1197 -0.1414  0.0240 -0.1098\n",
      " -0.1157 -0.0984 -0.0533  0.0326  0.0231\n",
      " -0.0876 -0.1740  0.1189  0.1045 -0.1423\n",
      "  0.0480 -0.0041  0.1461  0.0988  0.1371\n",
      "  0.0738  0.0284 -0.1009 -0.1743  0.1637\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0487 -0.1323  0.0369 -0.0769 -0.0445\n",
      " -0.1175  0.1258 -0.1453  0.0420 -0.1572\n",
      "  0.0530  0.1615  0.1287  0.0696 -0.1123\n",
      "  0.1034 -0.0652 -0.1831  0.0643 -0.0481\n",
      "  0.0143  0.0125  0.0592 -0.0781 -0.0212\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0751 -0.0441 -0.1314 -0.0647\n",
      "  0.0334 -0.1760 -0.1744  0.0369  0.0008\n",
      "  0.0643  0.0650 -0.1496 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1934\n",
      "  0.0604  0.0861 -0.0189 -0.1015 -0.0565\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0887 -0.0322  0.0334\n",
      " -0.1326  0.1553  0.1177  0.1299 -0.1733\n",
      " -0.0784  0.0653  0.0403 -0.1282  0.0069\n",
      "  0.0510 -0.1013  0.0019 -0.0489  0.0513\n",
      " -0.1697 -0.1396 -0.1768  0.0360 -0.1063\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1708 -0.1845  0.1475  0.1374 -0.0094\n",
      "  0.1567  0.0862 -0.0326  0.0202  0.0633\n",
      " -0.0656  0.0035 -0.1308  0.0776 -0.0355\n",
      "  0.0447 -0.0382 -0.0373 -0.0405  0.0012\n",
      "  0.0511  0.0078 -0.0660 -0.0332  0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "前向传播后反向传播前, conv1的梯度大小: \n",
      "None\n",
      "反向传播后参数更新前, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1407 -0.0253  0.1999 -0.1994\n",
      " -0.1119  0.1906 -0.1263  0.0701 -0.1803\n",
      " -0.0821  0.1815  0.0779  0.1674 -0.1716\n",
      "  0.0052 -0.0165  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0292 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1960 -0.1326  0.0846 -0.1297  0.0142\n",
      "  0.1144  0.0488 -0.1724  0.1641  0.0055\n",
      " -0.0211  0.1503 -0.0945  0.1493 -0.0684\n",
      "  0.0383 -0.0214  0.0342  0.0387  0.0328\n",
      " -0.0773  0.0035 -0.0590  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1999  0.1026 -0.1491 -0.0924 -0.1258\n",
      "  0.0420  0.0497  0.1619 -0.0237  0.1122\n",
      "  0.1907  0.1579  0.0430 -0.1519  0.1576\n",
      " -0.0566  0.1748 -0.1697  0.1135 -0.1544\n",
      "  0.1580  0.1302  0.1174 -0.1951  0.0236\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1319 -0.1651  0.1064  0.1096 -0.0925\n",
      " -0.0695  0.1518 -0.1159 -0.0858  0.0017\n",
      " -0.1722 -0.1757 -0.0232  0.0240  0.1866\n",
      "  0.0097  0.0489 -0.1157  0.0142 -0.1830\n",
      " -0.0899  0.1399 -0.1802 -0.0496  0.1484\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0826 -0.0188  0.0553 -0.0241  0.1129\n",
      "  0.1193 -0.0050  0.0981  0.1629 -0.0506\n",
      "  0.1275  0.0013  0.1190  0.1001  0.1036\n",
      " -0.1814  0.1130  0.0299  0.1662  0.1661\n",
      " -0.1892  0.0051 -0.1623 -0.0477 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0287  0.1197 -0.1414  0.0240 -0.1098\n",
      " -0.1157 -0.0984 -0.0533  0.0326  0.0231\n",
      " -0.0876 -0.1740  0.1189  0.1045 -0.1423\n",
      "  0.0480 -0.0041  0.1461  0.0988  0.1371\n",
      "  0.0738  0.0284 -0.1009 -0.1743  0.1637\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0487 -0.1323  0.0369 -0.0769 -0.0445\n",
      " -0.1175  0.1258 -0.1453  0.0420 -0.1572\n",
      "  0.0530  0.1615  0.1287  0.0696 -0.1123\n",
      "  0.1034 -0.0652 -0.1831  0.0643 -0.0481\n",
      "  0.0143  0.0125  0.0592 -0.0781 -0.0212\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0751 -0.0441 -0.1314 -0.0647\n",
      "  0.0334 -0.1760 -0.1744  0.0369  0.0008\n",
      "  0.0643  0.0650 -0.1496 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1934\n",
      "  0.0604  0.0861 -0.0189 -0.1015 -0.0565\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0887 -0.0322  0.0334\n",
      " -0.1326  0.1553  0.1177  0.1299 -0.1733\n",
      " -0.0784  0.0653  0.0403 -0.1282  0.0069\n",
      "  0.0510 -0.1013  0.0019 -0.0489  0.0513\n",
      " -0.1697 -0.1396 -0.1768  0.0360 -0.1063\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1708 -0.1845  0.1475  0.1374 -0.0094\n",
      "  0.1567  0.0862 -0.0326  0.0202  0.0633\n",
      " -0.0656  0.0035 -0.1308  0.0776 -0.0355\n",
      "  0.0447 -0.0382 -0.0373 -0.0405  0.0012\n",
      "  0.0511  0.0078 -0.0660 -0.0332  0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "反向传播后参数更新前, conv1的梯度大小: \n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.3463  0.6184 -0.2758 -0.8305 -0.4801\n",
      "  -0.8215 -0.6240 -0.6429 -1.1730  0.1307\n",
      "  -1.3305 -1.0457 -0.6398 -0.8149 -0.6028\n",
      "  -1.3718 -0.5000  0.4222  0.2160 -0.4317\n",
      "  -0.8053 -0.2420  0.3499 -0.0915 -0.3746\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.8458  1.0981  1.7423  1.1609  0.2784\n",
      "   0.6371  1.1301  1.4692  1.5803  0.4052\n",
      "   1.8034  1.6476  1.7121  2.0627  0.2767\n",
      "   1.3466  1.8047  1.9710  1.0228 -0.3845\n",
      "   0.5407  1.0828  0.2695 -0.6510 -0.6353\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   1.0312  0.2995 -0.8286 -0.1971  0.6839\n",
      "   1.9231  1.3045 -0.6895 -0.3176  0.6284\n",
      "   2.4924  1.2012 -0.3450 -0.0562  0.1884\n",
      "   1.3948  1.0324 -0.0201 -0.1842 -0.0379\n",
      "   0.8227  0.7073 -0.2214 -0.6452 -0.6269\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.4975 -0.3463 -0.9919 -0.1335  0.9910\n",
      "  -0.4467 -1.0035 -0.9867  0.1089  0.9369\n",
      "  -0.4779 -0.7210 -0.5734  0.6263  1.6000\n",
      "  -0.2135 -0.4196 -0.1360  0.7248  1.2390\n",
      "   0.0434  0.1029  0.3569  1.6689  2.1162\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.5420 -0.5063  0.2818  1.1600  0.7046\n",
      "   0.8319  0.4551  0.6602  1.4149  1.1129\n",
      "   1.5212  0.9873  1.4069  1.3991  1.1776\n",
      "   0.6301  0.4116  0.1092  0.7031  0.3712\n",
      "   0.0221 -0.7653 -0.5858 -0.4976 -0.6248\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.6691 -0.2460 -0.7290 -0.5006 -0.3713\n",
      "   0.3614 -0.4556 -1.1538 -0.7508 -0.1883\n",
      "   0.6926 -0.2026 -0.5392  0.6832  0.8796\n",
      "   0.8453  0.4241  0.8854  1.5294  1.4465\n",
      "   0.1458  0.2488  0.2774  1.4359  1.3525\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.9540 -2.2914 -1.2312  0.3066  0.3058\n",
      "  -1.3925 -1.8126 -1.0787 -0.1990  0.4649\n",
      "  -1.7038 -1.4672 -0.3606  0.1169  0.8722\n",
      "  -1.2839 -0.9938  0.0822  0.6386  0.5338\n",
      "  -1.1617 -1.3410 -0.5339  0.6857  0.8773\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -0.0117 -0.8731 -0.8799 -0.6177 -0.6013\n",
      "  -0.5717 -0.9042 -0.2218 -0.5013 -0.1205\n",
      "  -0.3034  0.0373  0.4982  0.1243  0.2656\n",
      "   0.0287 -0.1075  0.3334  0.2179  1.2106\n",
      "  -0.1266  0.3910  0.5489  0.2352  1.0603\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "   0.1507  0.2236  0.4486 -0.1891 -1.2527\n",
      "   0.3117  0.6156  0.4889 -0.3017 -0.7800\n",
      "   1.1671  1.3772  0.8571  0.1191  0.3352\n",
      "   1.1114  1.4498  0.6852  0.2267  0.4673\n",
      "   0.9398  0.8102  0.0995  0.4834  1.0221\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      "1.00000e-02 *\n",
      "  -1.3299 -0.0858 -0.8089 -1.5902 -2.0066\n",
      "  -0.4566 -0.1631 -1.2764 -2.0821 -1.8971\n",
      "  -0.1947 -0.3759 -0.8444 -1.4726 -1.7862\n",
      "   0.5720  0.9287  0.2534 -0.4292 -0.4988\n",
      "   1.3671  1.2267  0.3378  0.4742  0.8270\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "参数更新后, conv1的参数大小: \n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0056  0.1406 -0.0253  0.1999 -0.1993\n",
      " -0.1118  0.1906 -0.1263  0.0702 -0.1803\n",
      " -0.0820  0.1817  0.0780  0.1675 -0.1715\n",
      "  0.0054 -0.0164  0.1201 -0.1538  0.0174\n",
      " -0.1872  0.0380 -0.0293 -0.0318 -0.1778\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      " -0.1961 -0.1327  0.0845 -0.1298  0.0142\n",
      "  0.1144  0.0487 -0.1725  0.1640  0.0054\n",
      " -0.0213  0.1502 -0.0947  0.1491 -0.0684\n",
      "  0.0382 -0.0216  0.0340  0.0386  0.0328\n",
      " -0.0773  0.0034 -0.0591  0.0112  0.1879\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.1998  0.1026 -0.1491 -0.0924 -0.1259\n",
      "  0.0418  0.0495  0.1620 -0.0236  0.1121\n",
      "  0.1905  0.1578  0.0430 -0.1519  0.1576\n",
      " -0.0568  0.1746 -0.1697  0.1135 -0.1544\n",
      "  0.1579  0.1301  0.1174 -0.1951  0.0237\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      " -0.1318 -0.1651  0.1065  0.1096 -0.0926\n",
      " -0.0695  0.1519 -0.1158 -0.0858  0.0017\n",
      " -0.1722 -0.1756 -0.0231  0.0240  0.1865\n",
      "  0.0097  0.0489 -0.1157  0.0141 -0.1831\n",
      " -0.0899  0.1399 -0.1803 -0.0497  0.1482\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0.0827 -0.0188  0.0553 -0.0242  0.1128\n",
      "  0.1192 -0.0050  0.0981  0.1627 -0.0507\n",
      "  0.1273  0.0012  0.1188  0.0999  0.1035\n",
      " -0.1814  0.1129  0.0298  0.1661  0.1660\n",
      " -0.1892  0.0052 -0.1622 -0.0476 -0.1190\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      " -0.0288  0.1197 -0.1414  0.0240 -0.1097\n",
      " -0.1158 -0.0983 -0.0532  0.0327  0.0231\n",
      " -0.0877 -0.1740  0.1190  0.1044 -0.1424\n",
      "  0.0479 -0.0042  0.1460  0.0987  0.1369\n",
      "  0.0738  0.0284 -0.1010 -0.1744  0.1636\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      "  0.0488 -0.1321  0.0370 -0.0769 -0.0446\n",
      " -0.1173  0.1259 -0.1452  0.0421 -0.1573\n",
      "  0.0532  0.1616  0.1288  0.0696 -0.1124\n",
      "  0.1035 -0.0651 -0.1831  0.0642 -0.0482\n",
      "  0.0144  0.0127  0.0592 -0.0782 -0.0213\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.1287  0.0752 -0.0440 -0.1314 -0.0647\n",
      "  0.0334 -0.1759 -0.1743  0.0370  0.0008\n",
      "  0.0643  0.0650 -0.1497 -0.0004  0.1115\n",
      " -0.0471 -0.1105  0.1863 -0.1514  0.1932\n",
      "  0.0604  0.0860 -0.0190 -0.1015 -0.0566\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.1959  0.0272  0.0886 -0.0322  0.0335\n",
      " -0.1326  0.1552  0.1177  0.1299 -0.1732\n",
      " -0.0786  0.0652  0.0402 -0.1283  0.0068\n",
      "  0.0509 -0.1014  0.0018 -0.0489  0.0513\n",
      " -0.1698 -0.1397 -0.1768  0.0360 -0.1064\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1706 -0.1845  0.1475  0.1376 -0.0092\n",
      "  0.1567  0.0862 -0.0324  0.0204  0.0635\n",
      " -0.0656  0.0035 -0.1307  0.0778 -0.0353\n",
      "  0.0447 -0.0383 -0.0373 -0.0405  0.0013\n",
      "  0.0509  0.0077 -0.0660 -0.0332  0.1091\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 预先记录前向传播前的参数大小(以conv1的weiht为例)\n",
    "print(\"前向传播前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"前向传播前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#初始化梯度\n",
    "optimizer.zero_grad()  # zero the gradient buffers，必须要置零\n",
    "\n",
    "#前向传播\n",
    "output = model(data)   #前向传播的输出结果, 是一个 64x10的结果, 其中64表示batch_size,10为类别数\n",
    "print(\"输出前向传播的输出结果:\")\n",
    "print(output)\n",
    "\n",
    "#计算loss\n",
    "loss = F.nll_loss(output, target)    # 计算本次前向传波的loss值\n",
    "print(\"输出本次前向传波的loss值:\")\n",
    "print(loss)\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"前向传播后反向传播前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"前向传播后反向传播前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#反向传播\n",
    "loss.backward()    # loss的反向传播\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"反向传播后参数更新前, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n",
    "print(\"反向传播后参数更新前, conv1的梯度大小: \")\n",
    "print(model.conv1.weight.grad)\n",
    "\n",
    "#参数更新\n",
    "optimizer.step()\n",
    "\n",
    "#再次打印前向传播后,参数大小\n",
    "print(\"参数更新后, conv1的参数大小: \")\n",
    "print(model.conv1.weight) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程详细分析\n",
    "(1). 前向传播的目的是为了通过网络计算出最终的预测结果, \n",
    "再通过损失函数计算出预测值与真实值之间的偏差,即loss, \n",
    "将loss值极小化作为优化目标, 极小化预测值与真实值之间的差别.\n",
    "\n",
    "(2). 优化的方式就是通过计算梯度, 来更新参数包括所有的weight, bias. \n",
    "进而使得对于输入的data都可以准确的预测其结果.\n",
    "\n",
    "(3).于是反向传播的过程也就是根据loss计算相关参数的grad, 计算方式采用的是连式法则.\n",
    "\n",
    "(4).在计算完各层参数的grad之后,对参数进行更新,\n",
    "这个更新后的参数,可以使得通过网络得到的预测结果与真实结果之间的差异变小,达到优化的目的.\n",
    "\n",
    "(5). 而在pytorch代码层面上, 可以发现weight的更新时发生在optimizer.step() (即参数更新指令)之后的, \n",
    "而梯度生成时发生在loss.backward() (反向传播)之后的, \n",
    "并且梯度是单次有效的,每次前向传播,反向传播后的梯度都是根据本次loss计算出来的,\n",
    "因此在进行前向传播之前,要将梯度初始化.\n",
    "\n",
    "#### 当然, 感兴趣的话还可以打印每次卷积前后data发生的变化,这对于理解卷积过程也是有所帮助的, 这部分内容我将放在后面检测过程中的单步结果中讲."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面, 我们展示一下网络的详细检测过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 1 ====. Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      "-2.3931e+01 -2.0862e+01 -1.5924e+01 -1.4905e+01 -2.4521e+01 -2.7253e+01\n",
      "\n",
      "Columns 6 to 9 \n",
      "-4.1199e+01 -1.9073e-06 -1.9175e+01 -1.4312e+01\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n",
      "==== 2 ==== Variable containing:\n",
      "\n",
      "Columns 0 to 5 \n",
      "-2.3931e+01 -2.0862e+01 -1.5924e+01 -1.4905e+01 -2.4521e+01 -2.7253e+01\n",
      "\n",
      "Columns 6 to 9 \n",
      "-4.1199e+01 -1.9073e-06 -1.9175e+01 -1.4312e+01\n",
      "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]\n",
      "\n",
      "==== 3 ==== 检测结果为: \t \n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "\n",
      "==== 4 ==== 真实结果为: \t Variable containing:\n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#以比较蠢的方式选取第一张图片数据\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "#如下两种方式得到,softmax结果:\n",
    "output = model(data)\n",
    "print(\"==== 1 ====.\", output) \n",
    "\n",
    "output = model.forward(data)\n",
    "print(\"==== 2 ====\", output) \n",
    "\n",
    "#对比检测结果与真实结果: (softmax最大的值对应的index对应的label即为检测结果)\n",
    "pred = output.data.max(1, keepdim=True)[1] \n",
    "print(\"==== 3 ==== 检测结果为: \\t {}\".format(pred))\n",
    "print(\"==== 4 ==== 真实结果为: \\t {}\".format(target))\n",
    "\n",
    "#print(output.data.max(1, keepdim=True))\n",
    "# 可见检测结果是正确的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检测过程详细分析\n",
    "\n",
    "(1). 检测就是一个前向传播的过程, 将通过卷积,全连最终接得到feature map, 通过softmax进行分类.\n",
    "\n",
    "(2). pytorch代码上, 可以发现model() 和 model,forward() 都可以得到结果.\n",
    "\n",
    "(3). 对于softmax的原理, 可以看机器学习相关资料, 这里不再展开.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)    # 卷积层, 1为输入的channel, 10为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)    # 卷积层, 10为输入的channel, 20为输出的channel, 5为卷积核大小(5*5的卷积核)\n",
    "        self.conv2_drop = nn.Dropout2d()    # Dropout层 \n",
    "        self.fc1 = nn.Linear(320, 50)  #线性函数,输入大小为320个神经元, 输出为50个神经元\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    #前向传播过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n",
    "print(target)  #打印真实label,可见这张图是7\n",
    "\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "data_ori = data\n",
    "print(\"原始输入图片的数字结构:\")\n",
    "print(data_ori)\n",
    "\n",
    "print(\"========================================\")\n",
    "data_1 = model.conv1(data_ori)\n",
    "print(\"经过conv1后的数字结构:\")\n",
    "print(model.conv1.weight)\n",
    "print(\"经过conv1后的数字结构:\")\n",
    "print(data_1)\n",
    "\n",
    "print(\"========================================\")\n",
    "data_2 = F.max_pool2d(data_1, 2)\n",
    "print(\"经过conv1的数字结构:\")\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Variable containing:\n",
      " 7\n",
      "[torch.cuda.LongTensor of size 1 (GPU 0)]\n",
      "\n",
      "原始输入图片的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.6450  1.9305  1.5996\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.4015  2.8088  2.8088\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.4286  1.0268  0.4922\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.4978  0.3395  0.0340 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  2.8088  2.6433  2.0960  2.0960  2.0960  2.0960  2.0960  2.0960\n",
      "  1.0268  1.6505  2.4651  2.8088  2.4396  2.8088  2.8088  2.8088  2.7578\n",
      " -0.4242 -0.4242 -0.2078  0.4159 -0.2460  0.4286  0.4286  0.4286  0.3268\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1442\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.2177\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.3268  2.7451\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.2686  2.8088\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.3097  2.1851  2.7324\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.1795  2.8088  1.8923\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.5304  2.7706  2.6306  0.3013\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.1824  2.3887  2.8088  1.6887 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.3860  2.1596  2.8088  2.3633  0.0213 -0.4242\n",
      " -0.4242 -0.4242 -0.4242  0.0595  2.8088  2.8088  0.5559 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.0296  2.4269  2.8088  1.0395 -0.4115 -0.4242 -0.4242\n",
      " -0.4242 -0.4242  1.2686  2.8088  2.8088  0.2377 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  0.3522  2.6560  2.8088  2.8088  0.2377 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.1159  2.8088  2.8088  2.3633  0.0849 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.1159  2.8088  2.2105 -0.1951 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.0960  2.0960  1.7396  0.2377 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.4906  2.8088  2.8088  1.3577 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.1569  2.5797  2.8088  0.9250 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.6322  2.7960  2.2360 -0.1951 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.5415  2.8215  0.6322 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  2.6051  0.1358 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.8088  0.3649 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  1.9560 -0.3606 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  0.3140 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.cuda.FloatTensor of size 1x1x28x28 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "conv1卷积核参数:\n",
      "Parameter containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0.0153  0.3259  0.1931  0.1950 -0.2066\n",
      " -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
      " -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
      "  0.0233 -0.0447  0.1835  0.1321  0.3404\n",
      "  0.1407  0.1476  0.2579  0.1959  0.0585\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0.0104 -0.0224 -0.3664 -0.4861 -0.2800\n",
      "  0.1141  0.1102 -0.2812 -0.4165 -0.3379\n",
      "  0.1372  0.1199  0.2881 -0.0999  0.1634\n",
      "  0.2905  0.3823  0.5954  0.5653  0.1324\n",
      " -0.1438  0.0064  0.0802  0.3379  0.2844\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0.0798  0.0704 -0.1030  0.2154 -0.0975\n",
      " -0.1746 -0.2341 -0.2216  0.1732  0.1978\n",
      " -0.3685 -0.1535 -0.1572  0.1322  0.3971\n",
      " -0.2559 -0.1418 -0.1172  0.3395  0.2059\n",
      " -0.1713 -0.0901  0.2505  0.3522  0.2085\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0.1241 -0.1835 -0.3326 -0.1037 -0.1134\n",
      "  0.3162  0.0332 -0.0893 -0.2823 -0.2161\n",
      "  0.1476  0.1888 -0.1981 -0.2667 -0.2407\n",
      "  0.1627  0.0904  0.0908 -0.2753 -0.1488\n",
      "  0.2771  0.0394 -0.1264 -0.2166 -0.0730\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      " -0.3005 -0.2953 -0.1426 -0.0045 -0.1248\n",
      " -0.0297 -0.2309 -0.2401 -0.1252  0.0536\n",
      " -0.3134 -0.0809 -0.0239  0.0702 -0.0181\n",
      "  0.0806 -0.0049 -0.0313  0.1956  0.0620\n",
      " -0.1641  0.0897  0.1914  0.4325  0.2904\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0.1330  0.2172  0.1960 -0.0969  0.1130\n",
      "  0.1974  0.0932  0.1406 -0.2397 -0.0977\n",
      "  0.3167  0.3363 -0.2704 -0.1850 -0.0186\n",
      "  0.3908  0.1115 -0.2426 -0.1940 -0.2603\n",
      "  0.2042  0.2092 -0.1117 -0.2013 -0.1616\n",
      "\n",
      "(6 ,0 ,.,.) = \n",
      " -0.0198  0.2373  0.2729 -0.0540 -0.1854\n",
      "  0.0802  0.3993  0.1696 -0.0028 -0.2808\n",
      "  0.2505  0.3697 -0.0181 -0.1314 -0.0736\n",
      "  0.0541  0.1614  0.2596 -0.0544 -0.0701\n",
      "  0.1702  0.2460  0.2558  0.3425  0.3234\n",
      "\n",
      "(7 ,0 ,.,.) = \n",
      " -0.0982  0.1252 -0.0135 -0.0752 -0.3418\n",
      " -0.1307  0.2256  0.3210 -0.2659 -0.3056\n",
      " -0.2033  0.0767  0.5127  0.0826 -0.2298\n",
      " -0.3450 -0.0224  0.3877  0.3312  0.0064\n",
      " -0.2941 -0.4151  0.0306  0.1472 -0.0846\n",
      "\n",
      "(8 ,0 ,.,.) = \n",
      " -0.0570 -0.0232 -0.0239 -0.1755  0.2165\n",
      "  0.0881 -0.1158 -0.0787  0.1568  0.2332\n",
      "  0.0131 -0.0756 -0.2181  0.0534  0.3254\n",
      "  0.0991  0.0501  0.0569  0.3412 -0.0361\n",
      " -0.1060  0.0797 -0.0573  0.1666  0.1814\n",
      "\n",
      "(9 ,0 ,.,.) = \n",
      " -0.1200  0.0947 -0.0307 -0.2284 -0.1777\n",
      "  0.0758 -0.0426  0.0249 -0.2037  0.0998\n",
      " -0.2321  0.0203 -0.0582 -0.2084 -0.1820\n",
      " -0.2436 -0.0961 -0.2408 -0.2075 -0.2478\n",
      " -0.0113 -0.1164 -0.1620  0.1383 -0.1092\n",
      "[torch.cuda.FloatTensor of size 10x1x5x5 (GPU 0)]\n",
      "\n",
      "conv1偏置项参数:\n",
      "Parameter containing:\n",
      " 0.0825\n",
      " 0.2392\n",
      "-0.1182\n",
      " 0.1155\n",
      " 0.0808\n",
      "-0.2142\n",
      "-0.2408\n",
      "-0.1918\n",
      "-0.1114\n",
      "-0.0601\n",
      "[torch.cuda.FloatTensor of size 10 (GPU 0)]\n",
      "\n",
      "经过conv1后的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "           ...             ⋱             ...          \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "           ...             ⋱             ...          \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "           ...             ⋱             ...          \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "           ...             ⋱             ...          \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x24x24 (GPU 0)]\n",
      "\n",
      "========================================\n",
      "经过conv1的数字结构:\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017  0.3455  1.2724  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017  1.6837  3.3778  ...   2.4993  0.7371 -0.0017\n",
      "           ...             ⋱             ...          \n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      " -0.0017 -0.0017 -0.0017  ...  -0.0017 -0.0017 -0.0017\n",
      "\n",
      "(0 ,1 ,.,.) = \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630  0.7679  1.1940  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630  2.7411  6.7602  ...   3.5579  0.1652 -0.2630\n",
      "           ...             ⋱             ...          \n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      " -0.2630 -0.2630 -0.2630  ...  -0.2630 -0.2630 -0.2630\n",
      "\n",
      "(0 ,2 ,.,.) = \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608  0.6066  1.3461  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608  3.0434  3.3505  ...  -0.1443 -0.3742 -0.2608\n",
      "           ...             ⋱             ...          \n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      " -0.2608 -0.2608 -0.2608  ...  -0.2608 -0.2608 -0.2608\n",
      "   ...\n",
      "\n",
      "(0 ,7 ,.,.) = \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.2614  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.5649  3.0527  ...  -1.5655 -0.1412  0.0535\n",
      "           ...             ⋱             ...          \n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "  0.0535  0.0535  0.0535  ...   0.0535  0.0535  0.0535\n",
      "\n",
      "(0 ,8 ,.,.) = \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756  0.0296  0.1225  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756  1.5005  1.4817  ...   0.2098 -0.5287 -0.5756\n",
      "           ...             ⋱             ...          \n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      " -0.5756 -0.5756 -0.5756  ...  -0.5756 -0.5756 -0.5756\n",
      "\n",
      "(0 ,9 ,.,.) = \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.4119 -0.6259  ...   0.5980  0.9780  0.9855\n",
      "           ...             ⋱             ...          \n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "  0.9855  0.9855  0.9855  ...   0.9855  0.9855  0.9855\n",
      "[torch.cuda.FloatTensor of size 1x10x12x12 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#导入训练好的模型\n",
    "model = torch.load('./model.pkl')\n",
    "\n",
    "#导入测试数据,这里batch_size=1, shuffle=False,保证了每次检测都是单张图片, 并且每次导入的顺序也是固定的\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "model.eval()\n",
    "print(len(test_loader)) #可见一共有10000张图片\n",
    "print(type(test_loader))  #打印测试数据的数据类型\n",
    "\n",
    "i = 1\n",
    "for data_i, target_i in test_loader:\n",
    "    if i !=1 :\n",
    "        break\n",
    "    data, target = data_i.cuda(), target_i.cuda()\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    i+=1\n",
    "\n",
    "print(target)  #打印真实label,可见这张图是7\n",
    "\n",
    "data_ori = data\n",
    "print(\"原始输入图片的数字结构:\")\n",
    "print(data_ori)\n",
    "\n",
    "print(\"========================================\")\n",
    "print(\"conv1卷积核参数:\")\n",
    "print(model.conv1.weight)\n",
    "print(\"conv1偏置项参数:\")\n",
    "print(model.conv1.bias)\n",
    "data_1 = model.conv1(data_ori)\n",
    "print(\"经过conv1后的数字结构:\")\n",
    "print(data_1)\n",
    "\n",
    "print(\"========================================\")\n",
    "data_2 = F.max_pool2d(data_1, 2)\n",
    "print(\"经过conv1的数字结构:\")\n",
    "print(data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）这里原始输入data_ori的维度是（1x1x28x28）\n",
    "\n",
    "（2）卷积核conv1.weight的维度是（10x1x5x5）\n",
    "\n",
    "（3）偏置项conv1.bias的维度是 10\n",
    "\n",
    "（4）可见输出的feature map 的维度是（1x10x24x24）\n",
    "\n",
    "#### 输出的feature map 大小计算公式 \n",
    "    输入：（N_0, C_0, H_0, W_0）\n",
    "    输出：（N_1, C_1, H_1, W_1）\n",
    "    有： W_1 = (W_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         H_1 = (H_0 + 2 × pad - kernel_size)/stride + 1\n",
    "         C_1 = 卷积核个数 （备注：卷积核大小为（C_1, C_0, kernel_size, kernel_size））\n",
    "         N_1 = N_0 = batch_size\n",
    "    其中, \n",
    "        pad为padding的大小（其就是在原始输入外侧加 pad圈 0, 那么原始为h×w的大小，加padding之后就变为了(h+2*pad)x(w+2*pad)的大小）\n",
    "        kernel_size为卷积核的大小\n",
    "        stride为卷积步长\n",
    "   ##### 需要注意的是N表示batch_size, 即当前批次共有batch_size张图, 这个是不变的, 即有N_1 = N_0\n",
    "    \n",
    "#### 本例中输出的feature map 详细计算过程\n",
    "    可以看到输出的feature map中第一个输出值: data_1(1,1,1,1) = -0.0017\n",
    "    它是由：data_ori中左上角, 大小为(N_0 x C_0 x 5 x 5), 即\n",
    "    data_ori(:,:,0:5,0:5)=\n",
    "     [-0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
    "     -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242]\n",
    "    与 第一层的 conv1.weight\n",
    "    conv1.weight(0 ,：,.,.) = \n",
    "      [0.0153  0.3259  0.1931  0.1950 -0.2066\n",
    "      -0.1773 -0.1169  0.1196 -0.3002 -0.2731\n",
    "      -0.2466 -0.1712 -0.2854 -0.2029 -0.1053\n",
    "      0.0233 -0.0447  0.1835  0.1321  0.3404\n",
    "      0.1407  0.1476  0.2579  0.1959  0.0585]\n",
    "    做点乘, 然后加上\n",
    "    conv1.bias(0) = 0.0825\n",
    "    可以使用计算器计算下结果：得到 -0.00174612\n",
    "    可见与他计算的data_1(1,1,1,1) = -0.0017是相当的\n",
    "\n",
    "#### 总结输出的feature map 详细计算公式\n",
    "    已知输入data_ori(N_0)\n",
    "    对于输出的data_1(n,c,h,w)其值为\n",
    "    data_ori(n, :, h×stride:h×stride+kernel_size, w×stride:w×stride+kernel_size)\n",
    "    与\n",
    "    conv.weight(c ,: , : ,:)做点乘\n",
    "    加上\n",
    "    conv.bias(c)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "      0.0153 + 0.3259 + 0.1931 + 0.1950 -0.2066 -0.1773 -0.1169 + 0.1196 -0.3002 -0.2731-0.2466 -0.1712 -0.2854 -0.2029 -0.1053+0.0233 -0.0447 + 0.1835 + 0.1321 + 0.3404+0.1407 + 0.1476 + 0.2579+  0.1959 + 0.0585]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0    84   185   159   151    60    36     0\n",
      "    0     0     0     0     0     0   222   254   254   254   254   241   198\n",
      "    0     0     0     0     0     0    67   114    72   114   163   227   254\n",
      "    0     0     0     0     0     0     0     0     0     0     0    17    66\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     3\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0    38\n",
      "    0     0     0     0     0     0     0     0     0     0     0    31   224\n",
      "    0     0     0     0     0     0     0     0     0     0     0   133   254\n",
      "    0     0     0     0     0     0     0     0     0     0    61   242   254\n",
      "    0     0     0     0     0     0     0     0     0     0   121   254   254\n",
      "    0     0     0     0     0     0     0     0     0     0   121   254   207\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 25 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  198   198   198   198   198   198   198   170    52     0     0     0     0\n",
      "  225   254   254   254   250   229   254   254   140     0     0     0     0\n",
      "   14    67    67    67    59    21   236   254   106     0     0     0     0\n",
      "    0     0     0     0     0    83   253   209    18     0     0     0     0\n",
      "    0     0     0     0    22   233   255    83     0     0     0     0     0\n",
      "    0     0     0     0   129   254   238    44     0     0     0     0     0\n",
      "    0     0     0    59   249   254    62     0     0     0     0     0     0\n",
      "    0     0     0   133   254   187     5     0     0     0     0     0     0\n",
      "    0     0     9   205   248    58     0     0     0     0     0     0     0\n",
      "    0     0   126   254   182     0     0     0     0     0     0     0     0\n",
      "    0    75   251   240    57     0     0     0     0     0     0     0     0\n",
      "   19   221   254   166     0     0     0     0     0     0     0     0     0\n",
      "  203   254   219    35     0     0     0     0     0     0     0     0     0\n",
      "  254   254    77     0     0     0     0     0     0     0     0     0     0\n",
      "  254   115     1     0     0     0     0     0     0     0     0     0     0\n",
      "  254    52     0     0     0     0     0     0     0     0     0     0     0\n",
      "  254    52     0     0     0     0     0     0     0     0     0     0     0\n",
      "  219    40     0     0     0     0     0     0     0     0     0     0     0\n",
      "   18     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 26 to 27 \n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.ByteTensor of size 28x28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root='../data', train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "print(test_dataset)\n",
    "print(len(test_dataset))\n",
    "print(test_dataset[0])\n",
    "\n",
    "data = test_loader.dataset.test_data[0]\n",
    "target = test_loader.dataset.test_labels[0]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform1 = transforms.Compose([  \n",
    "    transforms.ToTensor(), # range [0, 255] -> [0.0,1.0]  \n",
    "    ]  \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
